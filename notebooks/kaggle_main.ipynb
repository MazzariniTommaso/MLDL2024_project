{"cells":[{"cell_type":"markdown","metadata":{},"source":["## 0 - Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T13:11:25.589996Z","iopub.status.busy":"2024-07-09T13:11:25.589727Z","iopub.status.idle":"2024-07-09T13:11:30.309250Z","shell.execute_reply":"2024-07-09T13:11:30.308240Z","shell.execute_reply.started":"2024-07-09T13:11:25.589971Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import albumentations as A\n","from tqdm import tqdm\n","from itertools import cycle\n","from typing import Tuple, List\n","#from config import CITYSCAPES, GTA, DEEPLABV2_PATH, CITYSCAPES_PATH, GTA5_PATH\n","#from datasets import CityScapes, GTA5\n","#from models import BiSeNet, get_deeplab_v2, FCDiscriminator\n","#from utils import *\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","torch.cuda.manual_seed(42)\n"]},{"cell_type":"markdown","metadata":{},"source":["## 1 - Dataset"]},{"cell_type":"markdown","metadata":{},"source":["### CityScapes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T13:11:30.312153Z","iopub.status.busy":"2024-07-09T13:11:30.311609Z","iopub.status.idle":"2024-07-09T13:11:30.329694Z","shell.execute_reply":"2024-07-09T13:11:30.328181Z","shell.execute_reply.started":"2024-07-09T13:11:30.312116Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset\n","import torch\n","from PIL import Image\n","import numpy as np\n","import os\n","from typing import Optional, Tuple\n","from albumentations import Compose\n","\n","\n","class CityScapes(Dataset):\n","    \n","    \"\"\"\n","    A dataset class for loading and processing the CityScapes dataset.\n","    \"\"\"\n","    def __init__(self, \n","                 root_dir:str, \n","                 split:str = 'train', \n","                 transform: Optional[Compose] = None):\n","        \"\"\"\n","        Initializes the CityScapes dataset.\n","\n","        Args:\n","            root_dir (str): Root directory of the dataset.\n","            split (str, optional): Dataset split to use ('train', 'val', 'test'). Defaults to 'train'.\n","            transform (Optional[Compose], optional): Transformations to be applied on images and labels.. Defaults to None.\n","        \"\"\"\n","        super(CityScapes, self).__init__()\n","\n","        self.root_dir = root_dir\n","        self.split = split\n","        self.transform = transform\n","        \n","        # Load the data\n","        self.data = []\n","        path = os.path.join(self.root_dir, 'images', split)\n","        for city in os.listdir(path):\n","            images = os.path.join(path, city)\n","            for image in os.listdir(images):\n","                image = os.path.join(images, image)\n","                label = image.replace('images', 'gtFine').replace('_leftImg8bit','_gtFine_labelTrainIds')\n","                self.data.append((image, label))\n","\n","    def __len__(self)->int:   \n","        \"\"\"\n","        Returns the total number of samples in the dataset.\n","\n","        Returns:\n","            int: Number of samples in the dataset.\n","        \"\"\" \n","        return len(self.data)\n","\n","    def __getitem__(self, idx:int)-> Tuple[torch.Tensor, torch.Tensor]:\n","        \"\"\"\n","        Generates one sample of data.\n","\n","        Args:\n","            idx (int): Index of the sample to retrieve.\n","\n","        Returns:\n","            Tuple[torch.Tensor, torch.Tensor]: Tuple containing the image and the corresponding label.\n","        \"\"\"\n","        image_path, label_path = self.data[idx]\n","\n","        # Load image and label\n","        image = Image.open(image_path).convert('RGB')\n","        label = Image.open(label_path).convert('L')\n","        image, label = np.array(image), np.array(label)\n","        \n","        if self.transform:\n","            transformed = self.transform(image=image, mask=label)\n","            image, label = transformed['image'], transformed['mask']\n","\n","        image = torch.from_numpy(image).permute(2, 0, 1).float()/255\n","        label = torch.from_numpy(label).long()\n","        \n","        return image, label"]},{"cell_type":"markdown","metadata":{},"source":["### GTA5"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T13:11:30.331535Z","iopub.status.busy":"2024-07-09T13:11:30.331228Z","iopub.status.idle":"2024-07-09T13:11:30.353352Z","shell.execute_reply":"2024-07-09T13:11:30.352308Z","shell.execute_reply.started":"2024-07-09T13:11:30.331510Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset\n","import torch\n","from PIL import Image\n","import numpy as np\n","import os\n","from typing import Optional, Tuple\n","from albumentations import Compose\n","#from utils import get_color_to_id\n","\n","\n","class GTA5(Dataset):\n","    \n","    \"\"\"\n","    A dataset class for loading and processing the GTA5 dataset.\n","    \"\"\"\n","    \n","    def __init__(self, \n","                 root_dir:str,\n","                 compute_mask:bool=False,\n","                 transform: Optional[Compose] = None):\n","        \"\"\"\n","        Initializes the GTA5 dataset.\n","\n","        Args:\n","            root_dir (str): Root directory of the dataset.\n","            compute_mask (bool, optional): Whether to compute the mask from RGB labels. Defaults to False.\n","            transform (Optional[Compose], optional): Transformations to be applied on images and labels. Defaults to None.\n","        \"\"\"\n","        super(GTA5, self).__init__()\n","        \n","        self.root_dir = root_dir\n","        self.compute_mask = compute_mask\n","        self.transform = transform\n","        if self.compute_mask:\n","            self.color_to_id = get_color_to_id()\n","        \n","        # Load the data\n","        self.data = []\n","        image_dir = os.path.join(self.root_dir, 'images')\n","        \n","        if self.compute_mask:\n","            label_dir = os.path.join(self.root_dir, 'labels')\n","        else:\n","            label_dir = os.path.join(self.root_dir, 'masks')\n","            \n","        for filename in os.listdir(image_dir):\n","            image = os.path.join(image_dir, filename)\n","            label = os.path.join(label_dir, filename)\n","            self.data.append((image, label))\n","            \n","    def _rgb_to_label(self, image:Image.Image)->np.ndarray:\n","        \"\"\"\n","        Converts an RGB image to a label image using the color to ID mapping.\n","\n","        Args:\n","            image (Image.Image): The input RGB image.\n","\n","        Returns:\n","            np.ndarray: The label image.\n","        \"\"\"\n","        image_np = np.array(image)\n","        label = np.zeros((image_np.shape[0], image_np.shape[1]), dtype=np.uint8)\n","        \n","        for color, class_id in self.color_to_id.items():\n","            mask = np.all(image_np == color, axis=-1)\n","            label[mask] = class_id\n","    \n","        return label\n","        \n","    def __len__(self)->int: \n","        \"\"\"\n","        Returns the total number of samples in the dataset.\n","\n","        Returns:\n","            int: Number of samples in the dataset.\n","        \"\"\"\n","        return len(self.data)\n","\n","    def __getitem__(self, idx:int)-> Tuple[torch.Tensor,torch.Tensor]:\n","        \"\"\"\n","        Generates one sample of data.\n","\n","        Args:\n","            idx (int): Index of the sample to retrieve.\n","\n","        Returns:\n","            Tuple[torch.Tensor, torch.Tensor]: Tuple containing the image and the corresponding label or mask.\n","        \"\"\"\n","        image_path, label_path = self.data[idx]\n","\n","        # Load images and labels or masks\n","        image = Image.open(image_path).convert('RGB')\n","        \n","        if self.compute_mask:\n","            label = self._rgb_to_label(Image.open(label_path).convert('RGB'))\n","        else:\n","            label = Image.open(label_path).convert('L')\n","            \n","        image, label = np.array(image), np.array(label)\n","        \n","        if self.transform:\n","            transformed = self.transform(image=image, mask=label)\n","            image, label = transformed['image'], transformed['mask']\n","\n","        image = torch.from_numpy(image).permute(2, 0, 1).float()/255\n","        label = torch.from_numpy(label).long()\n","        return image, label"]},{"cell_type":"markdown","metadata":{},"source":["## 2 - Models"]},{"cell_type":"markdown","metadata":{},"source":["### DeepLabV2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T13:11:30.356509Z","iopub.status.busy":"2024-07-09T13:11:30.356133Z","iopub.status.idle":"2024-07-09T13:11:30.396604Z","shell.execute_reply":"2024-07-09T13:11:30.395585Z","shell.execute_reply.started":"2024-07-09T13:11:30.356482Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","affine_par = True\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n","        super(Bottleneck, self).__init__()\n","        # change\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes, affine=affine_par)\n","        for i in self.bn1.parameters():\n","            i.requires_grad = False\n","        padding = dilation\n","        # change\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,\n","                               padding=padding, bias=False, dilation=dilation)\n","        self.bn2 = nn.BatchNorm2d(planes, affine=affine_par)\n","        for i in self.bn2.parameters():\n","            i.requires_grad = False\n","        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4, affine=affine_par)\n","        for i in self.bn3.parameters():\n","            i.requires_grad = False\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class ClassifierModule(nn.Module):\n","    def __init__(self, inplanes, dilation_series, padding_series, num_classes):\n","        super(ClassifierModule, self).__init__()\n","        self.conv2d_list = nn.ModuleList()\n","        for dilation, padding in zip(dilation_series, padding_series):\n","            self.conv2d_list.append(\n","                nn.Conv2d(inplanes, num_classes, kernel_size=3, stride=1, padding=padding,\n","                          dilation=dilation, bias=True))\n","\n","        for m in self.conv2d_list:\n","            m.weight.data.normal_(0, 0.01)\n","\n","    def forward(self, x):\n","        out = self.conv2d_list[0](x)\n","        for i in range(len(self.conv2d_list) - 1):\n","            out += self.conv2d_list[i + 1](x)\n","        return out\n","\n","\n","class ResNetMulti(nn.Module):\n","    def __init__(self, block, layers, num_classes):\n","        self.inplanes = 64\n","        super(ResNetMulti, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n","                               bias=False)\n","        self.bn1 = nn.BatchNorm2d(64, affine=affine_par)\n","        for i in self.bn1.parameters():\n","            i.requires_grad = False\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True)  # change\n","        self.layer1 = self._make_layer(block, 64, layers[0])\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)\n","        self.layer6 = ClassifierModule(2048, [6, 12, 18, 24], [6, 12, 18, 24], num_classes)\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                m.weight.data.normal_(0, 0.01)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n","        downsample = None\n","        if (stride != 1\n","                or self.inplanes != planes * block.expansion\n","                or dilation == 2\n","                or dilation == 4):\n","            downsample = nn.Sequential(\n","                nn.Conv2d(self.inplanes, planes * block.expansion,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion, affine=affine_par))\n","        for i in downsample._modules['1'].parameters():\n","            i.requires_grad = False\n","        layers = []\n","        layers.append(\n","            block(self.inplanes, planes, stride, dilation=dilation, downsample=downsample))\n","        self.inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(self.inplanes, planes, dilation=dilation))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        _, _, H, W = x.size()\n","\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","        x = self.layer6(x)\n","\n","        x = torch.nn.functional.interpolate(x, size=(H, W), mode='bilinear')\n","\n","        #if self.training == True:\n","        #    return x, None, None\n","\n","        return x\n","\n","    def get_1x_lr_params_no_scale(self):\n","        \"\"\"\n","        This generator returns all the parameters of the net except for\n","        the last classification layer. Note that for each batchnorm layer,\n","        requires_grad is set to False in deeplab_resnet.py, therefore this function does not return\n","        any batchnorm parameter\n","        \"\"\"\n","        b = []\n","\n","        b.append(self.conv1)\n","        b.append(self.bn1)\n","        b.append(self.layer1)\n","        b.append(self.layer2)\n","        b.append(self.layer3)\n","        b.append(self.layer4)\n","\n","        for i in range(len(b)):\n","            for j in b[i].modules():\n","                jj = 0\n","                for k in j.parameters():\n","                    jj += 1\n","                    if k.requires_grad:\n","                        yield k\n","\n","    def get_10x_lr_params(self):\n","        \"\"\"\n","        This generator returns all the parameters for the last layer of the net,\n","        which does the classification of pixel into classes\n","        \"\"\"\n","        b = []\n","        if self.multi_level:\n","            b.append(self.layer5.parameters())\n","        b.append(self.layer6.parameters())\n","\n","        for j in range(len(b)):\n","            for i in b[j]:\n","                yield i\n","\n","    def optim_parameters(self, lr):\n","        return [{'params': self.get_1x_lr_params_no_scale(), 'lr': lr},\n","                {'params': self.get_10x_lr_params(), 'lr': 10 * lr}]\n","\n","\n","def get_deeplab_v2(num_classes=19, pretrain=True, pretrain_model_path='DeepLab_resnet_pretrained_imagenet.pth'):\n","    model = ResNetMulti(Bottleneck, [3, 4, 23, 3], num_classes)\n","\n","    # Pretraining loading\n","    if pretrain:\n","        print('Deeplab pretraining loading...')\n","        saved_state_dict = torch.load(pretrain_model_path)\n","\n","        new_params = model.state_dict().copy()\n","        for i in saved_state_dict:\n","            i_parts = i.split('.')\n","            new_params['.'.join(i_parts[1:])] = saved_state_dict[i]\n","        model.load_state_dict(new_params, strict=False)\n","\n","    return model\n"]},{"cell_type":"markdown","metadata":{},"source":["### BiSeNet"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T13:11:30.398666Z","iopub.status.busy":"2024-07-09T13:11:30.398336Z","iopub.status.idle":"2024-07-09T13:11:30.430748Z","shell.execute_reply":"2024-07-09T13:11:30.429784Z","shell.execute_reply.started":"2024-07-09T13:11:30.398637Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch import nn\n","#from .build_contextpath import build_contextpath\n","#import warnings\n","#warnings.filterwarnings(action='ignore')\n","\n","\n","class ConvBlock(torch.nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n","                               stride=stride, padding=padding, bias=False)\n","        self.bn = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, input):\n","        x = self.conv1(input)\n","        return self.relu(self.bn(x))\n","\n","\n","class Spatial_path(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.convblock1 = ConvBlock(in_channels=3, out_channels=64)\n","        self.convblock2 = ConvBlock(in_channels=64, out_channels=128)\n","        self.convblock3 = ConvBlock(in_channels=128, out_channels=256)\n","\n","    def forward(self, input):\n","        x = self.convblock1(input)\n","        x = self.convblock2(x)\n","        x = self.convblock3(x)\n","        return x\n","\n","\n","class AttentionRefinementModule(torch.nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","        self.bn = nn.BatchNorm2d(out_channels)\n","        self.sigmoid = nn.Sigmoid()\n","        self.in_channels = in_channels\n","        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n","\n","    def forward(self, input):\n","        # global average pooling\n","        x = self.avgpool(input)\n","        assert self.in_channels == x.size(1), 'in_channels and out_channels should all be {}'.format(x.size(1))\n","        x = self.conv(x)\n","        x = self.sigmoid(self.bn(x))\n","        # x = self.sigmoid(x)\n","        # channels of input and x should be same\n","        x = torch.mul(input, x)\n","        return x\n","\n","\n","class FeatureFusionModule(torch.nn.Module):\n","    def __init__(self, num_classes, in_channels):\n","        super().__init__()\n","        # self.in_channels = input_1.channels + input_2.channels\n","        # resnet101 3328 = 256(from spatial path) + 1024(from context path) + 2048(from context path)\n","        # resnet18  1024 = 256(from spatial path) + 256(from context path) + 512(from context path)\n","        self.in_channels = in_channels\n","\n","        self.convblock = ConvBlock(in_channels=self.in_channels, out_channels=num_classes, stride=1)\n","        self.conv1 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n","        self.relu = nn.ReLU()\n","        self.conv2 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n","        self.sigmoid = nn.Sigmoid()\n","        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n","\n","    def forward(self, input_1, input_2):\n","        x = torch.cat((input_1, input_2), dim=1)\n","        assert self.in_channels == x.size(1), 'in_channels of ConvBlock should be {}'.format(x.size(1))\n","        feature = self.convblock(x)\n","        x = self.avgpool(feature)\n","\n","        x = self.relu(self.conv1(x))\n","        x = self.sigmoid(self.conv2(x))\n","        x = torch.mul(feature, x)\n","        x = torch.add(x, feature)\n","        return x\n","\n","\n","class BiSeNet(torch.nn.Module):\n","    def __init__(self, num_classes, context_path):\n","        super().__init__()\n","        # build spatial path\n","        self.saptial_path = Spatial_path()\n","\n","        # build context path\n","        self.context_path = build_contextpath(name=context_path)\n","\n","        # build attention refinement module  for resnet 101\n","        if context_path == 'resnet101':\n","            self.attention_refinement_module1 = AttentionRefinementModule(1024, 1024)\n","            self.attention_refinement_module2 = AttentionRefinementModule(2048, 2048)\n","            # supervision block\n","            self.supervision1 = nn.Conv2d(in_channels=1024, out_channels=num_classes, kernel_size=1)\n","            self.supervision2 = nn.Conv2d(in_channels=2048, out_channels=num_classes, kernel_size=1)\n","            # build feature fusion module\n","            self.feature_fusion_module = FeatureFusionModule(num_classes, 3328)\n","\n","        elif context_path == 'resnet18':\n","            # build attention refinement module  for resnet 18\n","            self.attention_refinement_module1 = AttentionRefinementModule(256, 256)\n","            self.attention_refinement_module2 = AttentionRefinementModule(512, 512)\n","            # supervision block\n","            self.supervision1 = nn.Conv2d(in_channels=256, out_channels=num_classes, kernel_size=1)\n","            self.supervision2 = nn.Conv2d(in_channels=512, out_channels=num_classes, kernel_size=1)\n","            # build feature fusion module\n","            self.feature_fusion_module = FeatureFusionModule(num_classes, 1024)\n","        else:\n","            print('Error: unspport context_path network \\n')\n","\n","        # build final convolution\n","        self.conv = nn.Conv2d(in_channels=num_classes, out_channels=num_classes, kernel_size=1)\n","\n","        self.init_weight()\n","\n","        self.mul_lr = []\n","        self.mul_lr.append(self.saptial_path)\n","        self.mul_lr.append(self.attention_refinement_module1)\n","        self.mul_lr.append(self.attention_refinement_module2)\n","        self.mul_lr.append(self.supervision1)\n","        self.mul_lr.append(self.supervision2)\n","        self.mul_lr.append(self.feature_fusion_module)\n","        self.mul_lr.append(self.conv)\n","\n","    def init_weight(self):\n","        for name, m in self.named_modules():\n","            if 'context_path' not in name:\n","                if isinstance(m, nn.Conv2d):\n","                    nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n","                elif isinstance(m, nn.BatchNorm2d):\n","                    m.eps = 1e-5\n","                    m.momentum = 0.1\n","                    nn.init.constant_(m.weight, 1)\n","                    nn.init.constant_(m.bias, 0)\n","\n","    def forward(self, input):\n","        # output of spatial path\n","        sx = self.saptial_path(input)\n","\n","        # output of context path\n","        cx1, cx2, tail = self.context_path(input)\n","        cx1 = self.attention_refinement_module1(cx1)\n","        cx2 = self.attention_refinement_module2(cx2)\n","        cx2 = torch.mul(cx2, tail)\n","        # upsampling\n","        cx1 = torch.nn.functional.interpolate(cx1, size=sx.size()[-2:], mode='bilinear')\n","        cx2 = torch.nn.functional.interpolate(cx2, size=sx.size()[-2:], mode='bilinear')\n","        cx = torch.cat((cx1, cx2), dim=1)\n","\n","        #if self.training == True:\n","        #    cx1_sup = self.supervision1(cx1)\n","        #    cx2_sup = self.supervision2(cx2)\n","        #    cx1_sup = torch.nn.functional.interpolate(cx1_sup, size=input.size()[-2:], mode='bilinear')\n","        #    cx2_sup = torch.nn.functional.interpolate(cx2_sup, size=input.size()[-2:], mode='bilinear')\n","\n","        # output of feature fusion module\n","        result = self.feature_fusion_module(sx, cx)\n","\n","        # upsampling\n","        result = torch.nn.functional.interpolate(result, scale_factor=8, mode='bilinear')\n","        result = self.conv(result)\n","\n","        #if self.training == True:\n","        #    return result, cx1_sup, cx2_sup\n","\n","        return result"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T13:11:30.432383Z","iopub.status.busy":"2024-07-09T13:11:30.432044Z","iopub.status.idle":"2024-07-09T13:11:32.189795Z","shell.execute_reply":"2024-07-09T13:11:32.188706Z","shell.execute_reply.started":"2024-07-09T13:11:30.432359Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torchvision import models\n","\n","\n","class resnet18(torch.nn.Module):\n","    def __init__(self, pretrained=True):\n","        super().__init__()\n","        self.features = models.resnet18(pretrained=pretrained)\n","        self.conv1 = self.features.conv1\n","        self.bn1 = self.features.bn1\n","        self.relu = self.features.relu\n","        self.maxpool1 = self.features.maxpool\n","        self.layer1 = self.features.layer1\n","        self.layer2 = self.features.layer2\n","        self.layer3 = self.features.layer3\n","        self.layer4 = self.features.layer4\n","\n","    def forward(self, input):\n","        x = self.conv1(input)\n","        x = self.relu(self.bn1(x))\n","        x = self.maxpool1(x)\n","        feature1 = self.layer1(x)  # 1 / 4\n","        feature2 = self.layer2(feature1)  # 1 / 8\n","        feature3 = self.layer3(feature2)  # 1 / 16\n","        feature4 = self.layer4(feature3)  # 1 / 32\n","        # global average pooling to build tail\n","        tail = torch.mean(feature4, 3, keepdim=True)\n","        tail = torch.mean(tail, 2, keepdim=True)\n","        return feature3, feature4, tail\n","\n","\n","class resnet101(torch.nn.Module):\n","    def __init__(self, pretrained=True):\n","        super().__init__()\n","        self.features = models.resnet101(pretrained=pretrained)\n","        self.conv1 = self.features.conv1\n","        self.bn1 = self.features.bn1\n","        self.relu = self.features.relu\n","        self.maxpool1 = self.features.maxpool\n","        self.layer1 = self.features.layer1\n","        self.layer2 = self.features.layer2\n","        self.layer3 = self.features.layer3\n","        self.layer4 = self.features.layer4\n","\n","    def forward(self, input):\n","        x = self.conv1(input)\n","        x = self.relu(self.bn1(x))\n","        x = self.maxpool1(x)\n","        feature1 = self.layer1(x)  # 1 / 4\n","        feature2 = self.layer2(feature1)  # 1 / 8\n","        feature3 = self.layer3(feature2)  # 1 / 16\n","        feature4 = self.layer4(feature3)  # 1 / 32\n","        # global average pooling to build tail\n","        tail = torch.mean(feature4, 3, keepdim=True)\n","        tail = torch.mean(tail, 2, keepdim=True)\n","        return feature3, feature4, tail\n","\n","\n","def build_contextpath(name):\n","    model = {\n","        'resnet18': resnet18(pretrained=True),\n","        'resnet101': resnet101(pretrained=True)\n","    }\n","    return model[name]"]},{"cell_type":"markdown","metadata":{},"source":["### Discriminator"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T13:11:32.191584Z","iopub.status.busy":"2024-07-09T13:11:32.191068Z","iopub.status.idle":"2024-07-09T13:11:32.201239Z","shell.execute_reply":"2024-07-09T13:11:32.200146Z","shell.execute_reply.started":"2024-07-09T13:11:32.191552Z"},"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class FCDiscriminator(nn.Module):\n","\n","    def __init__(self, num_classes, ndf = 64):\n","        super(FCDiscriminator, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(num_classes, ndf, kernel_size=4, stride=2, padding=1)\n","        self.conv2 = nn.Conv2d(ndf, ndf*2, kernel_size=4, stride=2, padding=1)\n","        self.conv3 = nn.Conv2d(ndf*2, ndf*4, kernel_size=4, stride=2, padding=1)\n","        self.conv4 = nn.Conv2d(ndf*4, ndf*8, kernel_size=4, stride=2, padding=1)\n","        self.classifier = nn.Conv2d(ndf*8, 1, kernel_size=4, stride=2, padding=1)\n","\n","        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n","        #self.up_sample = nn.Upsample(scale_factor=32, mode='bilinear')\n","        #self.sigmoid = nn.Sigmoid()\n","\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.leaky_relu(x)\n","        x = self.conv2(x)\n","        x = self.leaky_relu(x)\n","        x = self.conv3(x)\n","        x = self.leaky_relu(x)\n","        x = self.conv4(x)\n","        x = self.leaky_relu(x)\n","        x = self.classifier(x)\n","        #x = self.up_sample(x)\n","        #x = self.sigmoid(x) \n","\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["## 3 - Utils"]},{"cell_type":"markdown","metadata":{},"source":["### Checkpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T13:11:32.202746Z","iopub.status.busy":"2024-07-09T13:11:32.202495Z","iopub.status.idle":"2024-07-09T13:11:32.231794Z","shell.execute_reply":"2024-07-09T13:11:32.231028Z","shell.execute_reply.started":"2024-07-09T13:11:32.202724Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","from typing import List, Dict, Tuple, Optional\n","#from utils import get_id_to_label\n","#from config import CHECKPOINT_ROOT\n","\n","def save_results(model_results: List[List[float]], \n","                 filename: str,\n","                 project_step: str,\n","                 model_params_flops: Dict[str, float],\n","                 model_latency_fps: Dict[str, float]) -> None:\n","    \"\"\"\n","    Saves the model results to a text file.\n","\n","    Args:\n","        model_results (List[List[float]]): A list containing model results.\n","            - model_results[0]: List of training losses.\n","            - model_results[1]: List of validation losses.\n","            - model_results[2]: List of training mIoU scores.\n","            - model_results[3]: List of validation mIoU scores.\n","            - model_results[4]: List of training IoU scores for each class.\n","            - model_results[5]: List of validation IoU scores for each class.\n","        filename (str): The name of the file to save the results in.\n","        project_step (str): The current project step, used for directory naming.\n","        model_params_flops (Dict[str, float]): Dictionary containing model parameters and FLOPS.\n","            - 'Parameters': Number of parameters.\n","            - 'FLOPS': Floating Point Operations per Second.\n","        model_latency_fps (Dict[str, float]): Dictionary containing model latency and FPS information.\n","            - 'mean_latency': Mean latency.\n","            - 'std_latency': Standard deviation of latency.\n","            - 'mean_fps': Mean FPS.\n","            - 'std_fps': Standard deviation of FPS.\n","    \"\"\"\n","    \n","    # Construct the checkpoint path\n","    checkpoint_path = f'{CHECKPOINT_ROOT}/{project_step}'\n","    \n","    # Create the directory if it does not exist\n","    if not os.path.exists(checkpoint_path):\n","        os.makedirs(checkpoint_path)\n","    \n","    # Open the file for writing\n","    with open(f\"{checkpoint_path}/{filename}.txt\", 'w') as file:\n","        # Write model parameters and FLOPS\n","        file.write(f\"Parameters: {model_params_flops['Parameters']}\\n\")\n","        file.write(f\"FLOPS: {model_params_flops['FLOPS']}\\n\")\n","        \n","        # Write latency information\n","        file.write(\"Latency:\\n\")\n","        file.write(f\"\\tmean: {model_latency_fps['mean_latency']}\\n\")\n","        file.write(f\"\\tstd: {model_latency_fps['std_latency']}\\n\")\n","        \n","        # Write FPS information\n","        file.write(\"FPS:\\n\")\n","        file.write(f\"\\tmean: {model_latency_fps['mean_fps']}\\n\")\n","        file.write(f\"\\tstd: {model_latency_fps['std_fps']}\\n\")\n","        \n","        # Write loss information\n","        file.write(\"Loss:\\n\")\n","        file.write(f\"\\ttrain: {model_results[0][-1]}\\n\")\n","        file.write(f\"\\tval: {model_results[1][-1]}\\n\")\n","        \n","        # Write mIoU information\n","        file.write(\"mIoU:\\n\")\n","        file.write(f\"\\ttrain: {model_results[2][-1]}\\n\")\n","        file.write(f\"\\tval: {model_results[3][-1]}\\n\")\n","        \n","        # Write training IoU for each class\n","        file.write(\"Training IoU for class:\\n\")\n","        for i, iou in enumerate(model_results[4]):\n","            file.write(f\"{get_id_to_label()[i]}: {iou}\\n\")\n","        \n","        # Write validation IoU for each class\n","        file.write(\"Validation IoU for class:\\n\")\n","        for i, iou in enumerate(model_results[5]):\n","            file.write(f\"{get_id_to_label()[i]}: {iou}\\n\")\n","            \n","def save_checkpoint(checkpoint_root: str,\n","                    project_step: str, \n","                    adversarial: bool,\n","                    model: torch.nn.Module, \n","                    model_D: torch.nn.Module, \n","                    optimizer: torch.optim.Optimizer, \n","                    optimizer_D: torch.optim.Optimizer, \n","                    epoch: int,\n","                    train_loss_list: List[float], \n","                    train_miou_list: List[float],\n","                    train_iou: List[float],\n","                    val_loss_list: List[float],\n","                    val_miou_list: List[float],\n","                    val_iou: List[float],\n","                    verbose: bool)->None:\n","    \"\"\"\n","    Saves the current state of the training process to a checkpoint file.\n","\n","    Args:\n","        checkpoint_root (str): The root directory where the checkpoint will be saved.\n","        project_step (str): The current project step or phase, used for naming the checkpoint file.\n","        adversarial (bool): Whether to use adversarial training.\n","        model (torch.nn.Module): The main model whose state is to be saved.\n","        model_D (torch.nn.Module): The auxiliary or discriminator model whose state is to be saved.\n","        optimizer (torch.optim.Optimizer): The optimizer for the main model.\n","        optimizer_D (torch.optim.Optimizer): The optimizer for the auxiliary/discriminator model.\n","        epoch (int): The current epoch number.\n","        train_loss_list (List[float]): List of training losses over epochs.\n","        train_miou_list (List[float]): List of training mean Intersection over Union (mIoU) scores over epochs.\n","        train_iou (List[float]): List of training IoU scores for each class.\n","        val_loss_list (List[float]): List of validation losses over epochs.\n","        val_miou_list (List[float]): List of validation mIoU scores over epochs.\n","        val_iou (List[float]): List of validation IoU scores for each class.\n","        verbose (bool): If True, prints a message confirming the checkpoint has been saved.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    # Construct the path for the checkpoint file\n","    checkpoint_path = f'{checkpoint_root}/{project_step}/checkpoint.pth'\n","    \n","    # Save the state of the training process, including model parameters, optimizers, and performance metrics\n","    if adversarial:\n","        torch.save({\n","            'model': model.state_dict(),\n","            'model_D': model_D.state_dict(),\n","            'optimizer': optimizer.state_dict(),\n","            'optimizer_D': optimizer_D.state_dict(),\n","            'epoch': epoch + 1,\n","            'train_loss_list': train_loss_list,\n","            'train_miou_list': train_miou_list,\n","            'train_iou': train_iou,\n","            'val_loss_list': val_loss_list,\n","            'val_miou_list': val_miou_list,\n","            'val_iou': val_iou\n","        }, checkpoint_path)\n","    else:\n","        torch.save({\n","            'model': model.state_dict(),\n","            'optimizer': optimizer.state_dict(),\n","            'epoch': epoch + 1,\n","            'train_loss_list': train_loss_list,\n","            'train_miou_list': train_miou_list,\n","            'train_iou': train_iou,\n","            'val_loss_list': val_loss_list,\n","            'val_miou_list': val_miou_list,\n","            'val_iou': val_iou\n","        }, checkpoint_path)\n","    \n","    # If verbose is True, print a confirmation message\n","    if verbose == True:\n","        print(f\"Checkpoint saved in {checkpoint_path}\")\n","    \n","def load_checkpoint(checkpoint_root: str,\n","                    project_step: str, \n","                    adversarial: bool,\n","                    model: torch.nn.Module, \n","                    model_D: torch.nn.Module,\n","                    optimizer: torch.optim.Optimizer,\n","                    optimizer_D: torch.optim.Optimizer) -> Tuple[bool, Optional[int], Optional[List[float]], Optional[List[float]], Optional[List[float]], Optional[List[float]], Optional[List[float]], Optional[List[float]]]:\n","    \"\"\"\n","    Loads the checkpoint from the specified directory and restores the model, optimizer, and training state.\n","\n","    Args:\n","        checkpoint_root (str): The root directory where the checkpoint is stored.\n","        project_step (str): The current project step or phase, used for constructing the checkpoint file path.\n","        adversarial (bool): Whether to use adversarial training.\n","        model (torch.nn.Module): The main model to load the state dictionary into.\n","        model_D (torch.nn.Module): The auxiliary or discriminator model to load the state dictionary into.\n","        optimizer (torch.optim.Optimizer): The optimizer for the main model to load the state dictionary into.\n","        optimizer_D (torch.optim.Optimizer): The optimizer for the auxiliary/discriminator model to load the state dictionary into.\n","\n","    Returns:\n","        Tuple[bool, Optional[int], Optional[List[float]], Optional[List[float]], Optional[List[float]], Optional[List[float]], Optional[List[float]], Optional[List[float]]]:\n","            - bool: Indicates whether to start training from scratch (True) or resume from a checkpoint (False).\n","            - Optional[int]: The epoch to resume from, if a checkpoint is found.\n","            - Optional[List[float]]: List of training losses over epochs, if a checkpoint is found.\n","            - Optional[List[float]]: List of training mean Intersection over Union (mIoU) scores over epochs, if a checkpoint is found.\n","            - Optional[List[float]]: List of training IoU scores for each class, if a checkpoint is found.\n","            - Optional[List[float]]: List of validation losses over epochs, if a checkpoint is found.\n","            - Optional[List[float]]: List of validation mIoU scores over epochs, if a checkpoint is found.\n","            - Optional[List[float]]: List of validation IoU scores for each class, if a checkpoint is found.\n","    \"\"\"\n","\n","    # Construct the path to the checkpoint file\n","    checkpoint_path = f'{checkpoint_root}/{project_step}/checkpoint.pth'\n","    \n","    # Check if the checkpoint file exists\n","    if os.path.exists(checkpoint_path):\n","        # Load the checkpoint\n","        checkpoint = torch.load(checkpoint_path)\n","        \n","        # Load the state dictionaries into the model, auxiliary model, and optimizers\n","        model.load_state_dict(checkpoint['model'])\n","        optimizer.load_state_dict(checkpoint['optimizer'])\n","        if adversarial:\n","            model_D.load_state_dict(checkpoint['model_D'])\n","            optimizer_D.load_state_dict(checkpoint['optimizer_D'])\n","        \n","        # Extract training state information\n","        start_epoch = checkpoint['epoch']\n","        train_loss_list = checkpoint['train_loss_list']\n","        train_miou_list = checkpoint['train_miou_list']\n","        train_iou = checkpoint['train_iou']\n","        val_loss_list = checkpoint['val_loss_list']\n","        val_miou_list = checkpoint['val_miou_list']\n","        val_iou = checkpoint['val_iou']\n","        \n","        # Print a message indicating the checkpoint was found and loaded\n","        print(f\"Checkpoint found. Resuming from epoch {start_epoch}.\")\n","        \n","        # Return the state indicating that training can resume from the checkpoint\n","        return (False, start_epoch, train_loss_list, train_miou_list, train_iou, val_loss_list, val_miou_list, val_iou)\n","    \n","    else:\n","        # Create the directory if it does not exist\n","        directory = f'{checkpoint_root}/{project_step}'\n","        if not os.path.exists(directory):\n","            os.makedirs(directory)\n","        \n","        # Print a message indicating no checkpoint was found and training will start from scratch\n","        print(f\"No checkpoint found in {directory}. Starting from scratch.\")\n","        \n","        # Return the state indicating that training should start from scratch\n","        return (True, None, None, None, None, None, None, None)\n","  "]},{"cell_type":"markdown","metadata":{},"source":["### Computations"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T13:11:32.233050Z","iopub.status.busy":"2024-07-09T13:11:32.232792Z","iopub.status.idle":"2024-07-09T13:11:50.618833Z","shell.execute_reply":"2024-07-09T13:11:50.617708Z","shell.execute_reply.started":"2024-07-09T13:11:32.233028Z"},"trusted":true},"outputs":[],"source":["! pip install -U fvcore"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T13:11:50.623521Z","iopub.status.busy":"2024-07-09T13:11:50.623186Z","iopub.status.idle":"2024-07-09T13:11:50.727240Z","shell.execute_reply":"2024-07-09T13:11:50.726348Z","shell.execute_reply.started":"2024-07-09T13:11:50.623492Z"},"trusted":true},"outputs":[],"source":["from typing import Dict, Tuple\n","import numpy as np\n","import torch\n","from fvcore.nn import FlopCountAnalysis, flop_count_table\n","import time\n","\n","def compute_flops(model: torch.nn.Module, \n","                  height: int = 512, \n","                  width: int = 1024) -> Dict[str, str]:\n","    \"\"\"\n","    Computes the number of floating point operations (FLOPs) and parameters of a model.\n","\n","    Args:\n","        model (torch.nn.Module): The neural network model to analyze.\n","        height (int, optional): The height of the input image. Defaults to 512.\n","        width (int, optional): The width of the input image. Defaults to 1024.\n","\n","    Returns:\n","        Dict[str, str]: A dictionary containing the number of parameters and FLOPs of the model.\n","    \"\"\"\n","    \n","    # Create a dummy input tensor with the specified dimensions\n","    image = torch.zeros((1, 3, height, width))\n","\n","    # Perform FLOP analysis on the model with the dummy input\n","    flops = FlopCountAnalysis(model.cpu(), image)\n","    \n","    # Generate a formatted table with the FLOP count results\n","    table = flop_count_table(flops)\n","    \n","    # Extract the number of parameters and FLOPs from the table\n","    n_param_table = table.split('\\n')[2].split('|')[2].strip()\n","    flops_table = table.split('\\n')[2].split('|')[3].strip()\n","\n","    # Return the extracted values as a dictionary\n","    return {'Parameters': n_param_table,\n","            'FLOPS': flops_table\n","            }\n","\n","\n","def compute_latency_and_fps(model: torch.nn.Module, \n","                            height: int = 512, \n","                            width: int = 1024, \n","                            iterations: int = 1000, \n","                            device: str = 'cuda') ->  Dict[str, float]:\n","    \"\"\"\n","    Computes the mean latency, standard deviation of latency, mean FPS, and standard deviation of FPS for a given model.\n","\n","    Args:\n","        model (torch.nn.Module): The neural network model to evaluate.\n","        height (int, optional): The height of the input image. Defaults to 512.\n","        width (int, optional): The width of the input image. Defaults to 1024.\n","        iterations (int, optional): Number of iterations to measure latency and FPS. Defaults to 1000.\n","        device (str, optional): Device to run inference ('cpu' or 'cuda'). Defaults to 'cuda'.\n","\n","    Returns:\n","        Dict[str, float]: Dictionary containing model latency and FPS information.\n","    \"\"\"\n","    \n","    latencies = []\n","    fps_records = []\n","    \n","    model.eval()\n","    model = model.to(device)\n","    \n","    with torch.no_grad():\n","        for _ in range(iterations):\n","            # Create a dummy input tensor with the specified dimensions and move it to the device\n","            image = torch.zeros((1, 3, height, width)).to(device)\n","            \n","            # Measure the start time of the inference\n","            start_time = time.time()\n","            \n","            # Perform inference with the model\n","            model(image)\n","            \n","            # Measure the end time of the inference\n","            end_time = time.time() \n","            \n","            # Calculate the latency in seconds and append it to the list\n","            latency = end_time - start_time\n","            latencies.append(latency)\n","            \n","            # Calculate the FPS and append it to the list\n","            fps_records.append(1 / latency)\n","    \n","    # Calculate mean and standard deviation of latency\n","    mean_latency = np.mean(latencies)\n","    std_latency = np.std(latencies)\n","    \n","    # Calculate mean and standard deviation of FPS\n","    mean_fps = np.mean(fps_records)\n","    std_fps = np.std(fps_records)\n","\n","    return {'mean_latency': mean_latency,\n","            'std_latency': std_latency,\n","            'mean_fps': mean_fps,\n","            'std_fps':std_fps}"]},{"cell_type":"markdown","metadata":{},"source":["### Data processing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T13:11:50.728696Z","iopub.status.busy":"2024-07-09T13:11:50.728415Z","iopub.status.idle":"2024-07-09T13:11:50.749292Z","shell.execute_reply":"2024-07-09T13:11:50.748465Z","shell.execute_reply.started":"2024-07-09T13:11:50.728673Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","from PIL import Image\n","import PIL\n","#from config import GTA\n","import albumentations as A\n","\n","def get_color_to_id() -> dict:\n","    \"\"\"\n","    Creates a dictionary mapping color representations to their corresponding IDs.\n","\n","    Returns:\n","        dict: A dictionary where keys are color representations (RGB tuples) and values are IDs.\n","    \"\"\"\n","    \n","    id_to_color = get_id_to_color()\n","    color_to_id = {color: id for id, color in id_to_color.items()}\n","    return color_to_id\n","\n","def get_id_to_color() -> dict:\n","    \"\"\"\n","    Returns a dictionary mapping class IDs to their corresponding RGB color representations.\n","\n","    Returns:\n","        dict: A dictionary where keys are class IDs (integers) and values are RGB tuples.\n","    \"\"\"\n","\n","    return {\n","        0: (128, 64, 128),    # road\n","        1: (244, 35, 232),    # sidewalk\n","        2: (70, 70, 70),      # building\n","        3: (102, 102, 156),   # wall\n","        4: (190, 153, 153),   # fence\n","        5: (153, 153, 153),   # pole\n","        6: (250, 170, 30),    # light\n","        7: (220, 220, 0),     # sign\n","        8: (107, 142, 35),    # vegetation\n","        9: (152, 251, 152),   # terrain\n","        10: (70, 130, 180),   # sky\n","        11: (220, 20, 60),    # person\n","        12: (255, 0, 0),      # rider\n","        13: (0, 0, 142),      # car\n","        14: (0, 0, 70),       # truck\n","        15: (0, 60, 100),     # bus\n","        16: (0, 80, 100),     # train\n","        17: (0, 0, 230),      # motorcycle\n","        18: (119, 11, 32),    # bicycle\n","    }\n","\n","def get_id_to_label() -> dict:\n","    \"\"\"\n","    Returns a dictionary mapping class IDs to their corresponding semantic labels.\n","\n","    Returns:\n","        dict: A dictionary where keys are class IDs (integers) and values are semantic labels (strings).\n","    \"\"\"\n","\n","    return {\n","        0: 'road',\n","        1: 'sidewalk',\n","        2: 'building',\n","        3: 'wall',\n","        4: 'fence',\n","        5: 'pole',\n","        6: 'light',\n","        7: 'sign',\n","        8: 'vegetation',\n","        9: 'terrain',\n","        10: 'sky',\n","        11: 'person',\n","        12: 'rider',\n","        13: 'car',\n","        14: 'truck',\n","        15: 'bus',\n","        16: 'train',\n","        17: 'motorcycle',\n","        18: 'bicycle',\n","        255: 'unlabeled'\n","    }\n","\n","def label_to_rgb(label:np.ndarray)->PIL.Image:\n","    \"\"\"\n","    Converts a 2D numpy array of class IDs (labels) into an RGB image.\n","\n","    Args:\n","        label (np.ndarray): 2D numpy array containing class IDs.\n","    Returns:\n","        PIL.Image.Image: RGB image where each pixel corresponds to a color based on class ID.\n","    \"\"\"\n","    \n","    id_to_color = get_id_to_color()\n","    color_image = np.zeros((label.shape[0], label.shape[1], 3), dtype=np.uint8)\n","    \n","    for class_id, color in id_to_color.items():\n","        color_image[label == class_id] = color\n","        \n","    # Set color to black for label 255\n","    color_image[label == 255] = (0, 0, 0)\n","    \n","    return Image.fromarray(color_image, 'RGB')\n","\n","\n","def get_augmented_data(augmentedType: str) -> A.Compose:\n","    \"\"\"\n","    Returns an augmentation pipeline based on the specified `augmentedType`.\n","\n","    Args:\n","        augmentedType (str): Type of augmentation pipeline to return.\n","            Possible values: 'transform1', 'transform2', 'transform3', 'transform4'.\n","\n","    Returns:\n","        A.Compose: Augmentation pipeline defined using Albumentations library.\n","    \"\"\"\n","    # Define different augmentation pipelines\n","    augmentations = {\n","        'transform1': A.Compose([\n","            A.Resize(GTA['height'], GTA['width']),\n","            A.HorizontalFlip(p=0.5),\n","            A.ColorJitter(p=0.5)\n","        ]),\n","        'transform2': A.Compose([\n","            A.Resize(GTA['height'], GTA['width']),\n","            A.HorizontalFlip(p=0.5),\n","            A.GaussianBlur(p=0.5)\n","        ]),\n","        'transform3': A.Compose([\n","            A.Resize(GTA['height'], GTA['width']),\n","            A.HorizontalFlip(p=0.5),\n","            A.GaussianBlur(p=0.5)\n","        ]),\n","        'transform4': A.Compose([\n","            A.Resize(GTA['height'], GTA['width']),\n","            A.HorizontalFlip(p=0.5),\n","            A.GaussianBlur(p=0.5),\n","            A.ColorJitter(p=0.5),\n","            A.RandomResizedCrop(height=GTA['height'], \n","                                width=GTA['width'], \n","                                scale=(0.5, 1.0), \n","                                ratio=(0.75, 1.333), \n","                                p=0.5)\n","        ]),\n","    }\n","    \n","    # Return the specified augmentation pipeline if it exists\n","    if augmentedType in ['transform1', 'transform2', 'transform3', 'transform4']:\n","        return augmentations[augmentedType]\n","    else:\n","        print('Transformation accepted: [transform1, transform2, transform3, transform4]')\n","        return A.Compose([\n","            A.Resize(GTA['height'], GTA['width']),\n","        ])"]},{"cell_type":"markdown","metadata":{},"source":["### Poly Lr"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T13:11:50.750711Z","iopub.status.busy":"2024-07-09T13:11:50.750335Z","iopub.status.idle":"2024-07-09T13:11:50.762039Z","shell.execute_reply":"2024-07-09T13:11:50.761023Z","shell.execute_reply.started":"2024-07-09T13:11:50.750678Z"},"trusted":true},"outputs":[],"source":["import torch\n","\n","def poly_lr_scheduler(optimizer: torch.optim.Optimizer, \n","                      init_lr: float, \n","                      iter: int, \n","                      max_iter:int = 300, \n","                      power: float = 0.9) -> float:\n","    \"\"\"\n","    Polynomial learning rate scheduler.\n","\n","    Args:\n","        optimizer (torch.optim.Optimizer): Optimizer object.\n","        init_lr (float): Initial learning rate.\n","        iter (int): Current iteration number.\n","        max_iter (int, optional): Maximum number of iterations (default is 300).\n","        power (float, optional): Power factor (default is 0.9).\n","\n","    Returns:\n","        float: Updated learning rate.\n","    \"\"\"\n","\n","    # Calculate the learning rate using the polynomial decay formula\n","    lr = init_lr * (1 - iter / max_iter) ** power\n","\n","    # Update the learning rate in the optimizer\n","    optimizer.param_groups[0]['lr'] = lr"]},{"cell_type":"markdown","metadata":{},"source":["### Statistics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T13:11:50.763784Z","iopub.status.busy":"2024-07-09T13:11:50.763242Z","iopub.status.idle":"2024-07-09T13:11:50.771761Z","shell.execute_reply":"2024-07-09T13:11:50.770923Z","shell.execute_reply.started":"2024-07-09T13:11:50.763755Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","def fast_hist(a, b, n):\n","    '''\n","    a and b are label and prediction respectively\n","    n is the number of classes\n","    '''\n","    k = (a >= 0) & (a < n)\n","    return np.bincount(n * a[k].astype(int) + b[k], minlength=n ** 2).reshape(n, n)\n","\n","def per_class_iou(hist):\n","    epsilon = 1e-5\n","    return (np.diag(hist)) / (hist.sum(1) + hist.sum(0) - np.diag(hist) + epsilon)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T13:11:50.773719Z","iopub.status.busy":"2024-07-09T13:11:50.773297Z","iopub.status.idle":"2024-07-09T13:11:50.818214Z","shell.execute_reply":"2024-07-09T13:11:50.817383Z","shell.execute_reply.started":"2024-07-09T13:11:50.773695Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import torch\n","import numpy as np\n","import os\n","import random\n","from collections import OrderedDict\n","from typing import Tuple\n","#from utils import get_id_to_label, label_to_rgb\n","#from datasets import CityScapes\n","#from models import get_deeplab_v2, BiSeNet\n","#from config import CHECKPOINT_ROOT, CITYSCAPES_PATH, DEEPLABV2_PATH\n","\n","def print_stats(epoch:int, \n","                train_loss:float,\n","                val_loss:float, \n","                train_miou:float, \n","                val_miou:float, \n","                verbose:bool)->None:\n","    \"\"\"\n","    Print training and validation statistics if verbose is True.\n","\n","    Args:\n","        epoch (int): Current epoch number.\n","        train_loss (float): Training loss value.\n","        val_loss (float): Validation loss value.\n","        train_miou (float): Training mean IoU value.\n","        val_miou (float): Validation mean IoU value.\n","        verbose (bool): Flag to control verbosity. If False, no output is printed.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    if verbose:\n","        print(f'Epoch: {epoch}')\n","        print(f'\\tTrain Loss: {train_loss}, Validation Loss: {val_loss}')\n","        print(f'\\tTrain mIoU: {train_miou}, Validation mIoU: {val_miou}')\n","    \n","def plot_loss(model_results:list, \n","              model_name:str, \n","              project_step:str, \n","              train_dataset:str, \n","              validation_dataset:str)->None:\n","    \"\"\"\n","    Plot and save the training and validation loss curves.\n","\n","    Args:\n","        model_results (list): List of model results containing training and validation losses.\n","        model_name (str): Name of the model.\n","        project_step (str): Project step or phase.\n","        train_dataset (str): Name of the training dataset.\n","        validation_dataset (str): Name of the validation dataset.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    \n","    epochs = range(len(model_results[0]))\n","    train_losses = model_results[0]\n","    validation_losses = model_results[1]\n","\n","    # Create the plot\n","    fig, ax = plt.subplots(figsize=(12, 6), dpi=300)  \n","    ax.set_title(f'Train vs. Validation Loss for {model_name}', fontsize=14, fontweight='bold')\n","    ax.set_xlabel('Epoch', fontsize=14)\n","    ax.set_ylabel('Loss', fontsize=14)\n","    ax.plot(epochs, train_losses, 'o-', color='tab:blue', label=f\"train loss ({train_dataset})\", linewidth=2, markersize=5)\n","    ax.plot(epochs, validation_losses, '^-', color='tab:red', label=f\"validation loss ({validation_dataset})\", linewidth=2, markersize=5)\n","    ax.legend(loc='upper right', fontsize=12, frameon=True, shadow=True)\n","    ax.grid(True, which='both', linewidth=0.5)\n","    ax.tick_params(axis='both', which='major', labelsize=12)\n","\n","    plt.tight_layout()\n","    plt.show()\n","    \n","    # Save the plot\n","    checkpoint_path = f'{CHECKPOINT_ROOT}/{project_step}'\n","    if os.path.exists(checkpoint_path):\n","        fig.savefig(f\"{checkpoint_path}/{model_name}_{project_step}_loss.png\", format='png')\n","    else:\n","        os.makedirs(checkpoint_path)\n","        fig.savefig(f\"{checkpoint_path}/{model_name}_{project_step}_loss.png\", format='png')\n","     \n","\n","    \n","def plot_miou(model_results:list, \n","              model_name:str, \n","              project_step:str, \n","              train_dataset:str, \n","              validation_dataset:str) -> None:\n","    \"\"\"\n","    Plot and save the training and validation mIoU curves.\n","\n","    Args:\n","        model_results (list): List of model results containing training and validation mIoU values.\n","        model_name (str): Name of the model.\n","        project_step (str): Project step or phase.\n","        train_dataset (str): Name of the training dataset.\n","        validation_dataset (str): Name of the validation dataset.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    epochs = range(len(model_results[2]))\n","    train_mIoU = model_results[2]\n","    validation_mIoU = model_results[3]\n","\n","    # Create the plot\n","    fig, ax = plt.subplots(figsize=(12, 6), dpi=300)  \n","    ax.set_title(f'Train vs. Validation mIoU for {model_name} over Epochs', fontsize=14, fontweight='bold')\n","    ax.set_xlabel('Epoch', fontsize=14)\n","    ax.set_ylabel('Mean Intersection over Union (mIoU)', fontsize=14)\n","    ax.plot(epochs, train_mIoU, 'o-', color='tab:blue', label=f\"train mIoU ({train_dataset})\", linewidth=2, markersize=5)\n","    ax.plot(epochs, validation_mIoU, '^-', color='tab:red', label=f\"validation mIoU ({validation_dataset})\", linewidth=2, markersize=5)\n","    ax.legend(loc='upper left', fontsize=12, frameon=True, shadow=True)\n","    ax.grid(True, which='both', linewidth=0.5)\n","    ax.tick_params(axis='both', which='major', labelsize=12)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Save the plot\n","    checkpoint_path = f'{CHECKPOINT_ROOT}/{project_step}'\n","    if os.path.exists(checkpoint_path):\n","        fig.savefig(f\"{checkpoint_path}/{model_name}_{project_step}_miou.png\", format='png')\n","    else:\n","        os.makedirs(checkpoint_path)\n","        fig.savefig(f\"{checkpoint_path}/{model_name}_{project_step}_miou.png\", format='png')\n","\n","def plot_iou(model_results:list, \n","              model_name:str, \n","              project_step:str, \n","              train_dataset:str, \n","              validation_dataset:str) -> None:\n","    \"\"\"\n","    Plot and save the IoU (Intersection over Union) for each class across training and validation phases.\n","\n","    Args:\n","        model_results (list): List of model results containing training and validation IoU values for each class.\n","                              It should contain two lists:\n","                              - model_results[4]: List of training IoU values for each class.\n","                              - model_results[5]: List of validation IoU values for each class.\n","        model_name (str): Name of the model.\n","        project_step (str): Project step or phase.\n","        train_dataset (str): Name of the training dataset.\n","        validation_dataset (str): Name of the validation dataset.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    num_classes = 19\n","    class_names = [get_id_to_label()[i] for i in range(num_classes)]\n","    train_iou = [model_results[4][i] for i in range(num_classes)]\n","    val_iou = [model_results[5][i] for i in range(num_classes)]\n","\n","    fig, ax = plt.subplots(figsize=(12, 6), dpi=300)  \n","    bar_width = 0.35\n","    index = np.arange(num_classes)\n","\n","    ax.bar(index, train_iou, bar_width, label=f'train IoU ({train_dataset})', color='tab:blue', alpha=0.7)\n","    ax.bar(index + bar_width, val_iou, bar_width, label=f'validation IoU ({validation_dataset})', color='tab:red', alpha=0.7)\n","\n","    ax.set_xlabel('Classes', fontsize=14)\n","    ax.set_ylabel('IoU', fontsize=14)\n","    ax.set_title(f'Training and Validation IoU for Each Class ({model_name})', fontsize=16, fontweight='bold')\n","    ax.set_xticks(index + bar_width / 2)\n","    ax.set_xticklabels(class_names, rotation=45, ha=\"right\", fontsize=12)\n","    ax.legend(loc='upper right', fontsize=12, frameon=True, shadow=True)\n","    ax.grid(True, which='both', linewidth=0.5, axis='y')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    checkpoint_path = f'{CHECKPOINT_ROOT}/{project_step}'\n","    if os.path.exists(checkpoint_path):\n","        fig.savefig(f\"{checkpoint_path}/{model_name}_{project_step}_iou.png\", format='png')\n","    else:\n","        os.makedirs(checkpoint_path)\n","        fig.savefig(f\"{checkpoint_path}/{model_name}_{project_step}_iou.png\", format='png')\n","\n","def plot_segmented_images(model_roots: list,\n","                          model_types: list[Tuple],\n","                          n_images: int = 5,\n","                          device: str = \"cpu\") -> None:\n","    \"\"\"Visualizes the segmentation results of multiple models on multiple random Cityscapes validation images.\n","\n","    Args:\n","        model_roots (list): List of paths to the model checkpoints.\n","        model_types (list): List of model types (e.g., 'DeepLabV2' or 'BiSeNet').\n","        n_images (int): Number of random images to visualize.\n","        device (str): Device to run the model on ('cpu' or 'cuda').\n","    \"\"\"\n","\n","    # Load the Cityscapes validation dataset\n","    cityscapes_dataset = CityScapes(root_dir=CITYSCAPES_PATH, split='val')\n","\n","    # Select n_images random images from the validation set\n","    selected_indices = random.sample(range(len(cityscapes_dataset)), n_images)\n","    selected_images = [cityscapes_dataset[i][0] for i in selected_indices]\n","    ground_truths = [cityscapes_dataset[i][1] for i in selected_indices]\n","\n","    # Initialize the models and load their checkpoints\n","    models = []\n","    for model_root, model_type in zip(model_roots, model_types):\n","        checkpoint = torch.load(model_root, map_location=torch.device(device))\n","        \n","        if model_type[0] == 'DeepLabV2':\n","            model = get_deeplab_v2(num_classes=19, pretrain=True, pretrain_model_path=DEEPLABV2_PATH).to(device)\n","        else:\n","            model = BiSeNet(num_classes=19, context_path=\"resnet18\").to(device)\n","        \n","        try:\n","            model.load_state_dict(checkpoint[\"model\"])\n","            print(f\"{model_type[0]} model loaded successfully.\")\n","        except RuntimeError as e:\n","            print(f\"Error: Failed to load {model_type[0]} model state dictionary with error: {e}\")\n","            print(f\"Attempting to adjust the state dictionary for {model_type[0]}...\")\n","            new_state_dict = OrderedDict()\n","            \n","            for k, v in checkpoint['model'].items():\n","                if k.startswith(\"module\"):\n","                    name = k[7:]  # remove \"module.\" prefix\n","                else:\n","                    name = k\n","                \n","                new_state_dict[name] = v\n","\n","            model.load_state_dict(new_state_dict)\n","            print(f\"Adjusted state dictionary for {model_type[0]} loaded successfully.\")\n","        model.eval()\n","        models.append(model)\n","    \n","    # Generate the segmented images for each selected image and model\n","    outputs = []\n","    with torch.no_grad():\n","        for image in selected_images:\n","            model_outputs = []\n","            for model in models:\n","                output = model(image.unsqueeze(0))\n","                output = torch.argmax(torch.softmax(output, dim=1), dim=1)\n","                output = np.squeeze(output)\n","                segmented_image = label_to_rgb(output)\n","                model_outputs.append(segmented_image)\n","            outputs.append(model_outputs)\n","\n","    # Convert the original and ground truth images to numpy arrays for plotting\n","    selected_images_np = [(image.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8) for image in selected_images]\n","    ground_truths_np = [label_to_rgb(np.squeeze(gt.unsqueeze(0))) for gt in ground_truths]\n","    \n","    # Plot the images\n","    _, axes = plt.subplots(n_images, len(models) + 2, figsize=(23, 2 * n_images))\n","    \n","    for row in range(n_images):\n","        axes[row, 0].imshow(selected_images_np[row])\n","        axes[0, 0].set_title(\"Target Image\", fontsize=16, fontweight='bold')\n","        axes[row, 0].axis(\"off\")\n","\n","        axes[row, 1].imshow(ground_truths_np[row])\n","        axes[0, 1].set_title(\"Ground Truth\", fontsize=16, fontweight='bold')\n","        axes[row, 1].axis(\"off\")\n","\n","        for col, (output, model_type) in enumerate(zip(outputs[row], model_types), start=2):\n","            axes[row, col].imshow(output)\n","            axes[0, col].set_title(model_type[1], fontsize=16, fontweight='bold')\n","            axes[row, col].axis(\"off\")\n","    \n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## 4 - Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T13:11:50.819821Z","iopub.status.busy":"2024-07-09T13:11:50.819557Z","iopub.status.idle":"2024-07-09T13:11:50.895289Z","shell.execute_reply":"2024-07-09T13:11:50.894528Z","shell.execute_reply.started":"2024-07-09T13:11:50.819799Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import albumentations as A\n","from tqdm import tqdm\n","from itertools import cycle\n","from typing import Tuple, List, Union\n","#from config import CITYSCAPES, GTA, DEEPLABV2_PATH, CITYSCAPES_PATH, GTA5_PATH\n","#from datasets import CityScapes, GTA5\n","#from models import BiSeNet, get_deeplab_v2, FCDiscriminator\n","#from utils import *\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","torch.cuda.manual_seed(42)\n","\n","\n","def get_core(model_name: str, \n","             n_classes: int,\n","             device: str,\n","             parallelize: bool,\n","             optimizer_name: str, \n","             lr: float,\n","             momentum: float,\n","             weight_decay: float,\n","             loss_fn_name: str,\n","             ignore_index: int,\n","             adversarial: bool) -> Tuple[torch.nn.Module, torch.optim.Optimizer, torch.nn.Module, torch.nn.Module, torch.optim.Optimizer, torch.nn.Module]:\n","    \"\"\"\n","    Set up components for semantic segmentation model training.\n","\n","    Args:\n","    - model_name (str): Name of the segmentation model ('DeepLabV2' or 'BiSeNet').\n","    - n_classes (int): Number of classes in the dataset.\n","    - device (str): Device to run the model on ('cpu' or 'cuda').\n","    - parallelize (bool): Whether to use DataParallel for multi-GPU training.\n","    - optimizer_name (str): Name of the optimizer ('Adam' or 'SGD').\n","    - lr (float): Learning rate for the optimizer.\n","    - momentum (float): Momentum factor for SGD optimizer.\n","    - weight_decay (float): Weight decay (L2 penalty) for the optimizer.\n","    - loss_fn_name (str): Name of the loss function ('CrossEntropyLoss').\n","    - ignore_index (int): Index to ignore in loss computation.\n","    - adversarial (bool): Whether to include adversarial training components.\n","\n","    Raises:\n","    - ValueError: If an invalid model_name, optimizer_name, or loss_fn_name is provided.\n","\n","    Returns:\n","    - Tuple containing:\n","        - model (nn.Module): Segmentation model.\n","        - optimizer (torch.optim.Optimizer): Optimizer for the segmentation model.\n","        - loss_fn (nn.Module): Loss function for the segmentation model.\n","        - model_D (nn.Module or None): Discriminator model for adversarial training (if adversarial=True).\n","        - optimizer_D (torch.optim.Optimizer or None): Optimizer for the discriminator model (if adversarial=True).\n","        - loss_D (nn.Module or None): Loss function for the discriminator model (if adversarial=True).\n","    \"\"\"\n","    \n","    model = None\n","    optimizer = None\n","    loss_fn = None\n","    model_D = None\n","    optimizer_D = None\n","    loss_D = None\n","    \n","    # Initialize segmentation model based on model_name\n","    if model_name == 'DeepLabV2':\n","        model = get_deeplab_v2(num_classes=n_classes, pretrain=True, pretrain_model_path=DEEPLABV2_PATH).to(device)\n","        if parallelize and device == 'cuda' and torch.cuda.device_count() > 1:\n","            model = torch.nn.DataParallel(model).to(device)\n","    elif model_name == 'BiSeNet':\n","        model = BiSeNet(num_classes=n_classes, context_path=\"resnet18\").to(device)\n","        if parallelize and device == 'cuda' and torch.cuda.device_count() > 1:\n","            model = torch.nn.DataParallel(model).to(device)\n","    else:\n","        raise ValueError('Model accepted: [DeepLabV2, BiSeNet]')\n","            \n","    # Initialize optimizer based on optimizer_name\n","    if optimizer_name == 'Adam':\n","        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    elif optimizer_name == 'SGD':\n","        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n","    else:\n","        raise ValueError('Optimizer accepted: [Adam, SGD]')\n","        \n","    # Initialize loss function based on loss_fn_name\n","    if loss_fn_name == 'CrossEntropyLoss':\n","        loss_fn = torch.nn.CrossEntropyLoss(ignore_index=ignore_index)\n","    else:\n","        raise ValueError('Loss function accepted: [CrossEntropyLoss]')\n","    \n","    # Initialize adversarial components if adversarial is True\n","    if adversarial:\n","        model_D = FCDiscriminator(num_classes=n_classes).to(device)\n","        if parallelize and device == 'cuda' and torch.cuda.device_count() > 1:\n","            model_D = torch.nn.DataParallel(model_D).to(device)\n","        optimizer_D = torch.optim.Adam(model_D.parameters(), lr=1e-3, betas=(0.9, 0.99))\n","        loss_D = torch.nn.BCEWithLogitsLoss()\n","        \n","    return model, optimizer, loss_fn, model_D, optimizer_D, loss_D\n","\n","def get_loaders(train_dataset_name: str, \n","                val_dataset_name: str, \n","                augmented: bool,\n","                augmentedType: str,\n","                batch_size: int,\n","                n_workers: int,\n","                adversarial: bool) -> Tuple[Union[DataLoader, Tuple[DataLoader, DataLoader]], DataLoader, int, int]:\n","    \"\"\"\n","    Set up data loaders for training and validation datasets in semantic segmentation.\n","\n","    Args:\n","    - train_dataset_name (str): Name of the training dataset ('CityScapes' or 'GTA5').\n","    - val_dataset_name (str): Name of the validation dataset ('CityScapes').\n","    - augmented (bool): Whether to use augmented data.\n","    - augmentedType (str): Type of augmentation to apply (specific to your implementation).\n","    - batch_size (int): Batch size for data loaders.\n","    - n_workers (int): Number of workers for data loading.\n","    - adversarial (bool): Whether to set up adversarial training data loaders.\n","\n","    Raises:\n","    - ValueError: If an invalid train_dataset_name or val_dataset_name is provided.\n","\n","    Returns:\n","    - Tuple containing:\n","        - train_loader (Union[DataLoader, Tuple[DataLoader, DataLoader]]): DataLoader(s) for the training dataset.\n","        - val_loader (DataLoader): DataLoader for the validation dataset.\n","        - data_height (int): Height of the dataset images.\n","        - data_width (int): Width of the dataset images.\n","    \"\"\"\n","\n","    transform_cityscapes = A.Compose([\n","        A.Resize(CITYSCAPES['height'], CITYSCAPES['width']),\n","    ])\n","    transform_gta5 = A.Compose([\n","        A.Resize(GTA['height'], GTA['width'])\n","    ])\n","\n","    train_loader = None\n","    val_loader = None\n","    data_height = None\n","    data_width = None\n","    \n","    if augmented:\n","        transform_gta5 = get_augmented_data(augmentedType)\n","    \n","    if adversarial:\n","        source_dataset = GTA5(root_dir=GTA5_PATH, transform=transform_gta5)\n","        target_dataset = CityScapes(root_dir=CITYSCAPES_PATH, split='train', transform=transform_cityscapes)\n","\n","        source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True, num_workers=n_workers)\n","        target_loader = DataLoader(target_dataset, batch_size=batch_size, shuffle=True, num_workers=n_workers)\n","\n","        train_loader = (source_loader, target_loader)\n","    else:\n","        if train_dataset_name == 'CityScapes':\n","            train_dataset = CityScapes(root_dir=CITYSCAPES_PATH, split='train', transform=transform_cityscapes)\n","            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=n_workers)\n","        elif train_dataset_name == 'GTA5':\n","            train_dataset = GTA5(root_dir=GTA5_PATH, transform=transform_gta5)\n","            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=n_workers)\n","        else:\n","            raise ValueError('Train datasets accepted: [CityScapes, GTA5]')\n","        \n","    if val_dataset_name == 'CityScapes':\n","        val_dataset = CityScapes(root_dir=CITYSCAPES_PATH, split='val', transform=transform_cityscapes)\n","        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=n_workers)\n","        data_height = CITYSCAPES['height']\n","        data_width = CITYSCAPES['width']\n","    else:\n","        raise ValueError('Val datasets accepted: [CityScapes]')\n","    \n","    return train_loader, val_loader, data_height, data_width\n","\n","def adversarial_train_step(model: torch.nn.Module, \n","                           model_D: torch.nn.Module, \n","                           loss_fn: torch.nn.Module, \n","                           loss_D: torch.nn.Module, \n","                           optimizer: torch.optim.Optimizer, \n","                           optimizer_D: torch.optim.Optimizer, \n","                           loaders: Tuple[DataLoader,DataLoader], \n","                           device: str, \n","                           n_classes: int = 19)-> Tuple[float, float, float]:\n","    \"\"\"\n","    Perform a single adversarial training step for semantic segmentation.\n","\n","    Args:\n","    - model (torch.nn.Module): Segmentation model.\n","    - model_D (torch.nn.Module): Discriminator model.\n","    - loss_fn (torch.nn.Module): Segmentation loss function.\n","    - loss_D (torch.nn.Module): Adversarial loss function for discriminator.\n","    - optimizer (torch.optim.Optimizer): Optimizer for segmentation model.\n","    - optimizer_D (torch.optim.Optimizer): Optimizer for discriminator model.\n","    - loaders (Tuple[DataLoader,DataLoader]): Source and target dataloaders for training data.\n","    - device (str): Device on which to run the models ('cuda' or 'cpu').\n","    - n_classes (int, optional): Number of classes for segmentation. Default is 19.\n","\n","    Returns:\n","    - Tuple containing:\n","        - epoch_loss (float): Average segmentation loss for the epoch.\n","        - epoch_miou (float): Mean Intersection over Union (mIoU) for the epoch.\n","        - epoch_iou (np.ndarray): Array of per-class IoU values for the epoch.\n","    \"\"\"\n","\n","    model_G = model.to(device)\n","    optimizer_G = optimizer\n","    ce_loss = loss_fn\n","    bce_loss = loss_D\n","    \n","    interp_source = nn.Upsample(size=(GTA['height'], GTA['width']), mode='bilinear')\n","    interp_target = nn.Upsample(size=(CITYSCAPES['height'], CITYSCAPES['width']), mode='bilinear')\n","    \n","    lambda_adv = 0.001\n","    total_loss = 0\n","    total_miou = 0\n","    total_iou = np.zeros(n_classes)\n","    \n","    iterations = 0\n","    \n","    model_G.train()\n","    model_D.train()\n","    \n","    source_loader, target_loader = loaders\n","    train_loader = zip(source_loader, cycle(target_loader))\n","    \n","    \n","    for (source_data, source_labels), (target_data, _) in train_loader:\n","        \n","        iterations+=1\n","\n","        source_data, source_labels = source_data.to(device), source_labels.to(device)\n","        target_data = target_data.to(device)\n","        \n","        optimizer_G.zero_grad()\n","        optimizer_D.zero_grad()\n","\n","        #TRAIN GENERATOR\n","        \n","        #Train with source\n","        for param in model_D.parameters():\n","            param.requires_grad = False\n","        \n","        output_source = model_G(source_data)\n","        output_source = interp_source(output_source) # apply upsample\n","\n","        segmentation_loss = ce_loss(output_source, source_labels)\n","        segmentation_loss.backward()\n","\n","        #Train with target\n","        output_target = model_G(target_data)\n","        output_target = interp_target(output_target) # apply upsample\n","        \n","        prediction_target = torch.nn.functional.softmax(output_target)\n","        discriminator_output_target = model_D(prediction_target)\n","        discriminator_label_source = torch.FloatTensor(discriminator_output_target.data.size()).fill_(0).cuda()\n","        \n","        adversarial_loss = bce_loss(discriminator_output_target, discriminator_label_source)\n","        discriminator_loss = lambda_adv * adversarial_loss\n","        discriminator_loss.backward()\n","        \n","        \n","        #TRAIN DISCRIMINATOR\n","        \n","        #Train with source\n","        for param in model_D.parameters():\n","            param.requires_grad = True\n","            \n","        output_source = output_source.detach()\n","        \n","        prediction_source = torch.nn.functional.softmax(output_source)\n","        discriminator_output_source = model_D(prediction_source)\n","        discriminator_label_source = torch.FloatTensor(discriminator_output_source.data.size()).fill_(0).cuda()\n","        discriminator_loss_source = bce_loss(discriminator_output_source, discriminator_label_source)\n","        discriminator_loss_source.backward()\n","\n","        #Train with target\n","        output_target = output_target.detach()\n","        \n","        prediction_target = torch.nn.functional.softmax(output_target)\n","        discriminator_output_target = model_D(prediction_target)\n","        discriminator_label_target = torch.FloatTensor(discriminator_output_target.data.size()).fill_(1).cuda()\n","        \n","        discriminator_loss_target = bce_loss(discriminator_output_target, discriminator_label_target)\n","        discriminator_loss_target.backward()\n","        \n","        optimizer_G.step()\n","        optimizer_D.step()\n","        \n","        total_loss += segmentation_loss.item()\n","        \n","        prediction_source = torch.argmax(torch.softmax(output_source, dim=1), dim=1)\n","        hist = fast_hist(source_labels.cpu().numpy(), prediction_source.cpu().numpy(), n_classes)\n","        running_iou = np.array(per_class_iou(hist)).flatten()\n","        total_miou += running_iou.sum()\n","        total_iou += running_iou\n","\n","        \n","    epoch_loss = total_loss / iterations\n","    epoch_miou = total_miou / (iterations * n_classes)\n","    epoch_iou = total_iou / iterations\n","    \n","    return epoch_loss, epoch_miou, epoch_iou\n","\n","def train_step(model: torch.nn.Module, \n","               loss_fn: torch.nn.Module, \n","               optimizer: torch.optim.Optimizer, \n","               dataloader: DataLoader, \n","               device: str, \n","               n_classes: int = 19)-> Tuple[float, float, float]:\n","    \"\"\"\n","    Perform a single training step for semantic segmentation.\n","\n","    Args:\n","    - model (torch.nn.Module): Segmentation model.\n","    - loss_fn (torch.nn.Module): Loss function for segmentation.\n","    - optimizer (torch.optim.Optimizer): Optimizer for training.\n","    - dataloader (DataLoader): DataLoader for training data.\n","    - device (str): Device on which to run the models ('cuda' or 'cpu').\n","    - n_classes (int, optional): Number of classes for segmentation. Default is 19.\n","\n","    Returns:\n","    - Tuple containing:\n","        - epoch_loss (float): Average segmentation loss for the epoch.\n","        - epoch_miou (float): Mean Intersection over Union (mIoU) for the epoch.\n","        - epoch_iou (np.ndarray): Array of per-class IoU values for the epoch.\n","    \"\"\"\n","\n","    total_loss = 0\n","    total_miou = 0\n","    total_iou = np.zeros(n_classes)\n","    \n","    model.train()\n","    \n","    for image, label in dataloader:\n","        image, label = image.to(device), label.type(torch.LongTensor).to(device)\n","    \n","        output = model(image)\n","        loss = loss_fn(output, label)\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","        \n","        prediction = torch.argmax(torch.softmax(output, dim=1), dim=1)\n","\n","        hist = fast_hist(label.cpu().numpy(), prediction.cpu().numpy(), n_classes)\n","        running_iou = np.array(per_class_iou(hist)).flatten()\n","        total_miou += running_iou.sum()\n","        total_iou += running_iou\n","    \n","    epoch_loss = total_loss / len(dataloader)\n","    epoch_miou = total_miou / (len(dataloader)* n_classes)\n","    epoch_iou = total_iou / len(dataloader)\n","    \n","    return epoch_loss, epoch_miou, epoch_iou\n","\n","def val_step(model: torch.nn.Module,  \n","             loss_fn: torch.nn.Module, \n","             dataloader: DataLoader, \n","             device: str, \n","             n_classes: int = 19) -> Tuple[float, float, float]:\n","    \"\"\"\n","    Perform a single validation step for semantic segmentation.\n","\n","    Args:\n","    - model (torch.nn.Module): Segmentation model.\n","    - loss_fn (torch.nn.Module): Loss function for segmentation.\n","    - dataloader (DataLoader): DataLoader for validation data.\n","    - device (str): Device on which to run the models ('cuda' or 'cpu').\n","    - n_classes (int, optional): Number of classes for segmentation. Default is 19.\n","\n","    Returns:\n","    - Tuple containing:\n","        - epoch_loss (float): Average segmentation loss for the epoch.\n","        - epoch_miou (float): Mean Intersection over Union (mIoU) for the epoch.\n","        - epoch_iou (np.ndarray): Array of per-class IoU values for the epoch.\n","    \"\"\"\n","    \n","    total_loss = 0\n","    total_miou = 0\n","    total_iou = np.zeros(n_classes)\n","    \n","    model.eval()\n","\n","    with torch.inference_mode(): # which is analogous to torch.no_grad\n","        for image, label in dataloader:\n","            image, label = image.to(device), label.type(torch.LongTensor).to(device)\n","            \n","            output = model(image)\n","            loss = loss_fn(output, label)\n","            total_loss += loss.item()\n","            \n","            prediction = torch.argmax(torch.softmax(output, dim=1), dim=1)\n","            \n","            hist = fast_hist(label.cpu().numpy(), prediction.cpu().numpy(), n_classes)\n","            running_iou = np.array(per_class_iou(hist)).flatten()\n","            total_miou += running_iou.sum()\n","            total_iou += running_iou\n","    \n","    epoch_loss = total_loss / len(dataloader)\n","    epoch_miou = total_miou / (len(dataloader)* n_classes)\n","    epoch_iou = total_iou / len(dataloader)\n","    \n","    return epoch_loss, epoch_miou, epoch_iou\n","    \n","def train(model: torch.nn.Module, \n","          model_D: torch.nn.Module, \n","          optimizer: torch.optim.Optimizer, \n","          optimizer_D: torch.optim.Optimizer, \n","          loss_fn: torch.nn.Module, \n","          loss_D: torch.nn.Module, \n","          train_loader: Union[DataLoader, Tuple[DataLoader,DataLoader]],  \n","          val_loader: DataLoader, \n","          epochs: int, \n","          device: str, \n","          checkpoint_root: str,\n","          project_step: str,\n","          verbose: bool,\n","          n_classes: int = 19,\n","          power: float = 0.9,\n","          adversarial: bool = False) -> Tuple[List[float], List[float], List[float], List[float], List[float], List[float]]:\n","    \"\"\"\n","    Train a semantic segmentation model with optional adversarial training.\n","\n","    Args:\n","        model (torch.nn.Module): Semantic segmentation model.\n","        model_D (torch.nn.Module): Discriminator model for adversarial training.\n","        optimizer (torch.optim.Optimizer): Optimizer for the segmentation model.\n","        optimizer_D (torch.optim.Optimizer): Optimizer for the discriminator.\n","        loss_fn (torch.nn.Module): Loss function for segmentation.\n","        loss_D (torch.nn.Module): Loss function for adversarial training.\n","        train_loader (Union[DataLoader, Tuple[DataLoader,DataLoader]]): DataLoader(s) for the training dataset.\n","        val_loader (DataLoader): DataLoader for validation data.\n","        epochs (int): Number of epochs to train.\n","        device (str): Device on which to run computations ('cuda' or 'cpu').\n","        checkpoint_root (str): Root directory to save checkpoints.\n","        project_step (str): Name/id of the project or step.\n","        verbose (bool): Whether to print verbose training statistics.\n","        n_classes (int, optional): Number of classes for segmentation. Defaults to 19.\n","        power (float, optional): Power parameter for learning rate scheduler. Defaults to 0.9.\n","        adversarial (bool, optional): Whether to use adversarial training. Defaults to False.\n","\n","    Returns:\n","        Tuple containing lists of:\n","        - train_loss_list (List[float]): List of training losses per epoch.\n","        - val_loss_list (List[float]): List of validation losses per epoch.\n","        - train_miou_list (List[float]): List of training mIoU per epoch.\n","        - val_miou_list (List[float]): List of validation mIoU per epoch.\n","        - train_iou (List[float]): List of per-class IoU for training per epoch.\n","        - val_iou (List[float]): List of per-class IoU for validation per epoch.\n","    \"\"\"\n","    \n","    # Load or initialize checkpoint\n","    no_checkpoint, start_epoch, train_loss_list, train_miou_list, train_iou, val_loss_list, val_miou_list, val_iou = load_checkpoint(checkpoint_root=checkpoint_root, project_step=project_step, adversarial=adversarial, model=model, model_D=model_D, optimizer=optimizer, optimizer_D=optimizer_D)\n","        \n","    if no_checkpoint:\n","        train_loss_list, train_miou_list = [], []\n","        val_loss_list, val_miou_list = [], []\n","        start_epoch = 0\n","    \n","    for epoch in tqdm(range(start_epoch, epochs)):\n","        \n","        # Perform training step\n","        if adversarial:\n","            train_loss, train_miou, train_iou = adversarial_train_step(model=model,\n","                                                                       model_D=model_D,\n","                                                                       loss_fn=loss_fn, \n","                                                                       loss_D=loss_D, \n","                                                                       optimizer=optimizer, \n","                                                                       optimizer_D=optimizer_D, \n","                                                                       loaders=train_loader, \n","                                                                       device=device, \n","                                                                       n_classes=n_classes)\n","        else:\n","            train_loss, train_miou, train_iou = train_step(model=model, \n","                                                           loss_fn=loss_fn, \n","                                                           optimizer=optimizer, \n","                                                           train_loader=train_loader, \n","                                                           device=device, \n","                                                           n_classes=n_classes)\n","        \n","        # Perform validation step\n","        val_loss, val_miou, val_iou = val_step(model=model, \n","                                               loss_fn=loss_fn, \n","                                               val_loader=val_loader,\n","                                               device=device, \n","                                               n_classes=n_classes)\n","        \n","        # Append metrics to lists\n","        train_loss_list.append(train_loss) \n","        train_miou_list.append(train_miou) \n","        val_loss_list.append(val_loss)\n","        val_miou_list.append(val_miou)\n","\n","        # Print statistics if verbose\n","        print_stats(epoch=epoch, \n","                    train_loss=train_loss,\n","                    val_loss=val_loss, \n","                    train_miou=train_miou, \n","                    val_miou=val_miou, \n","                    verbose=verbose)\n","\n","        # Adjust learning rate\n","        poly_lr_scheduler(optimizer=optimizer,\n","                          init_lr=optimizer.param_groups[0]['lr'],\n","                          iter=epoch, \n","                          max_iter=epochs,\n","                          power=power)\n","        if adversarial:\n","            poly_lr_scheduler(optimizer=optimizer_D,\n","                              init_lr=optimizer_D.param_groups[0]['lr'],\n","                              iter=epoch, \n","                              max_iter=epochs,\n","                              power=power)\n","        \n","        # Save checkpoint after each epoch\n","        save_checkpoint(checkpoint_root=checkpoint_root, \n","                        project_step=project_step,\n","                        adversarial=adversarial,\n","                        model=model, \n","                        model_D=model_D,\n","                        optimizer=optimizer, \n","                        optimizer_D=optimizer_D, \n","                        epoch=epoch,\n","                        train_loss_list=train_loss_list, \n","                        train_miou_list=train_miou_list,\n","                        train_iou=train_iou,\n","                        val_loss_list=val_loss_list,\n","                        val_miou_list=val_miou_list,\n","                        val_iou=val_iou,\n","                        verbose=verbose)\n","        \n","    return train_loss_list, val_loss_list, train_miou_list, val_miou_list, train_iou, val_iou\n","    \n","def pipeline (model_name: str, \n","              train_dataset_name: str, \n","              val_dataset_name: str,\n","              n_classes:int,\n","              epochs: int,\n","              augmented: bool,\n","              augmentedType:str,\n","              optimizer_name: str,\n","              lr:float,\n","              momentum:float,\n","              weight_decay:float,\n","              loss_fn_name: str,\n","              ignore_index:int,\n","              batch_size: int,\n","              n_workers: int,\n","              device:str,\n","              parallelize:bool,\n","              project_step:str,\n","              verbose: bool,\n","              checkpoint_root:str,\n","              power:float,\n","              evalIterations:int,\n","              adversarial:bool\n","              )->None:\n","    \"\"\"\n","    Main pipeline function to orchestrate the training and evaluation of a deep learning model.\n","\n","    Args:\n","        model_name (str): Name of the deep learning model architecture.\n","        train_dataset_name (str): Name of the training dataset.\n","        val_dataset_name (str): Name of the validation dataset.\n","        n_classes (int): Number of classes in the dataset.\n","        epochs (int): Number of epochs for training.\n","        augmented (bool): Whether to use data augmentation during training.\n","        augmentedType (str): Type of data augmentation to apply.\n","        optimizer_name (str): Name of the optimizer to use.\n","        lr (float): Learning rate for the optimizer.\n","        momentum (float): Momentum factor for optimizers like SGD.\n","        weight_decay (float): Weight decay (L2 penalty) for the optimizer.\n","        loss_fn_name (str): Name of the loss function.\n","        ignore_index (int): Index to ignore in the loss function (e.g., for padding).\n","        batch_size (int): Batch size for training and validation data loaders.\n","        n_workers (int): Number of workers for data loading.\n","        device (str): Device to run the model on ('cuda' or 'cpu').\n","        parallelize (bool): Whether to use GPU parallelization.\n","        project_step (str): Name or identifier of the current project step or experiment.\n","        verbose (bool): Whether to print detailed logs during training.\n","        checkpoint_root (str): Root directory to save checkpoints and results.\n","        power (float): Power parameter for polynomial learning rate scheduler.\n","        evalIterations (int): Number of iterations for evaluating model latency and FPS.\n","        adversarial (bool): Whether to use adversarial training.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    \n","    \n","    # get model\n","    model, optimizer, loss_fn, model_D, optimizer_D, loss_D = get_core(model_name, \n","                                                                       n_classes,\n","                                                                       device,\n","                                                                       parallelize,\n","                                                                       optimizer_name, \n","                                                                       lr,\n","                                                                       momentum,\n","                                                                       weight_decay,\n","                                                                       loss_fn_name,\n","                                                                       ignore_index,\n","                                                                       adversarial)\n","    # get loader\n","    train_loader, val_loader, data_height, data_width = get_loaders(train_dataset_name, \n","                                                                    val_dataset_name, \n","                                                                    augmented,\n","                                                                    augmentedType,\n","                                                                    batch_size,\n","                                                                    n_workers,\n","                                                                    adversarial)\n","    # train\n","    model_results = train(model=model,\n","                          model_D = model_D,\n","                          optimizer=optimizer, \n","                          optimizer_D = optimizer_D,\n","                          loss_fn = loss_fn, \n","                          loss_D = loss_D,\n","                          train_loader=train_loader, \n","                          val_loader=val_loader, \n","                          epochs=epochs, \n","                          device=device, \n","                          checkpoint_root=checkpoint_root,\n","                          project_step=project_step,\n","                          verbose=verbose,\n","                          n_classes=n_classes,\n","                          power=power,\n","                          adversarial=adversarial)\n","    \n","    # evaluation\n","    model_params_flops = compute_flops(model=model, \n","                                       height=data_height, \n","                                       width=data_width)\n","    \n","    model_latency_fps = compute_latency_and_fps(model=model,\n","                                                height=data_height, \n","                                                width=data_width, \n","                                                iterations=evalIterations, \n","                                                device=device)\n","    \n","    # visualization\n","    plot_loss(model_results, \n","              model_name, \n","              project_step, \n","              train_dataset_name, \n","              val_dataset_name)\n","    \n","    plot_miou(model_results, \n","              model_name, \n","              project_step, \n","              train_dataset_name, \n","              val_dataset_name)\n","    \n","    plot_iou(model_results, \n","             model_name, \n","             project_step, \n","             train_dataset_name, \n","             val_dataset_name)\n","    \n","    # save results\n","    save_results(model_results, \n","                 filename=f\"{model_name}_metrics_{project_step}\", \n","                 project_step=project_step,\n","                 model_params_flops=model_params_flops,\n","                 model_latency_fps=model_latency_fps)"]},{"cell_type":"markdown","metadata":{},"source":["## 5 - Steps"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T13:11:50.896644Z","iopub.status.busy":"2024-07-09T13:11:50.896332Z","iopub.status.idle":"2024-07-09T13:11:50.906110Z","shell.execute_reply":"2024-07-09T13:11:50.905393Z","shell.execute_reply.started":"2024-07-09T13:11:50.896615Z"},"trusted":true},"outputs":[],"source":["CITYSCAPES_PATH = '/kaggle/input/cityscapes/Cityscapes/Cityspaces'\n","GTA5_PATH = '/kaggle/input/gta5-with-mask/GTA5_with_mask'# GTA5 with mask\n","DEEPLABV2_PATH = '/kaggle/input/deeplab_v2_model/pytorch/model_weight/1/deeplab_resnet_pretrained_imagenet.pth'#'models/deeplab_resnet_pretrained_imagenet.pth'\n","CHECKPOINT_ROOT = '/kaggle/working'\n","\n","CITYSCAPES = {\n","    'width': 1024, \n","    'height': 512\n","}\n","GTA = {\n","    'width': 1280, \n","    'height': 720\n","}"]},{"cell_type":"markdown","metadata":{},"source":["### Step 2.1"]},{"cell_type":"markdown","metadata":{},"source":["#### SGD"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T07:46:20.653478Z","iopub.status.busy":"2024-07-09T07:46:20.653134Z","iopub.status.idle":"2024-07-09T07:46:20.660567Z","shell.execute_reply":"2024-07-09T07:46:20.659666Z","shell.execute_reply.started":"2024-07-09T07:46:20.653447Z"},"trusted":true},"outputs":[],"source":["# for kaggle:\n","config = {\n","    'model_name': 'DeepLabV2', # [DeepLabV2, BiSeNet]\n","    'train_dataset_name': 'CityScapes', # [CityScapes, GTA5]\n","    'val_dataset_name': 'CityScapes', # [CityScapes]\n","    'n_classes': 19,\n","    'epochs': 50,\n","    'augmented': False,\n","    'augmentedType': 'transform1', # [transform1,transform2,transform3,transform4]\n","    'optimizer_name': 'SGD', # [SGD, Adam] # TRY BOTH\n","    'lr': 1e-3, \n","    'momentum': 0.9,\n","    'weight_decay': 5e-4,\n","    'loss_fn_name': 'CrossEntropyLoss', # [CrossEntropyLoss]\n","    'ignore_index': 255,\n","    'batch_size': 4, # [2,4,8]\n","    'n_workers': 4, # [0,2,4]\n","    'device': 'cuda',\n","    'parallelize': True,\n","    'project_step': 'Step2_1_SGD_1e_3', # [Step2_1,Step2_2,Step3_1,Step3_2,Step4]\n","    'verbose': False,\n","    'checkpoint_root': CHECKPOINT_ROOT,\n","    'power': 0.9,\n","    'evalIterations': 100,\n","    'adversarial':False\n","}"]},{"cell_type":"markdown","metadata":{},"source":["#### Adam"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T07:46:20.661898Z","iopub.status.busy":"2024-07-09T07:46:20.661579Z","iopub.status.idle":"2024-07-09T07:46:20.670726Z","shell.execute_reply":"2024-07-09T07:46:20.669819Z","shell.execute_reply.started":"2024-07-09T07:46:20.661867Z"},"trusted":true},"outputs":[],"source":["# for kaggle:\n","config = {\n","    'model_name': 'DeepLabV2', # [DeepLabV2, BiSeNet]\n","    'train_dataset_name': 'CityScapes', # [CityScapes, GTA5]\n","    'val_dataset_name': 'CityScapes', # [CityScapes]\n","    'n_classes': 19,\n","    'epochs': 50,\n","    'augmented': False,\n","    'augmentedType': 'transform1', # [transform1,transform2,transform3,transform4]\n","    'optimizer_name': 'Adam', # [SGD, Adam] # TRY BOTH\n","    'lr': 2.5e-4, \n","    'momentum': 0.9,\n","    'weight_decay': 5e-4,\n","    'loss_fn_name': 'CrossEntropyLoss', # [CrossEntropyLoss]\n","    'ignore_index': 255,\n","    'batch_size': 4, # [2,4,8]\n","    'n_workers': 4, # [0,2,4]\n","    'device': 'cuda',\n","    'parallelize': True,\n","    'project_step': 'Step2_1_Adam', # [Step2_1,Step2_2,Step3_1,Step3_2,Step4]\n","    'verbose': False,\n","    'checkpoint_root': CHECKPOINT_ROOT,\n","    'power': 0.9,\n","    'evalIterations': 100,\n","    'adversarial':False\n","}"]},{"cell_type":"markdown","metadata":{},"source":["### Step 2.2"]},{"cell_type":"markdown","metadata":{},"source":["#### SGD"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# for kaggle:\n","config = {\n","    'model_name': 'BiSeNet', # [DeepLabV2, BiSeNet]\n","    'train_dataset_name': 'CityScapes', # [CityScapes, GTA5]\n","    'val_dataset_name': 'CityScapes', # [CityScapes]\n","    'n_classes': 19,\n","    'epochs': 50,\n","    'augmented': False,\n","    'augmentedType': 'transform1', # [transform1,transform2,transform3,transform4]\n","    'optimizer_name': 'SGD', # [SGD, Adam]\n","    'lr': 2.5e-4,\n","    'momentum': 0.9,\n","    'weight_decay': 5e-4,\n","    'loss_fn_name': 'CrossEntropyLoss', # [CrossEntropyLoss]\n","    'ignore_index': 255,\n","    'batch_size': 8, # [2,4,8]\n","    'n_workers': 4, # [0,2,4]\n","    'device': 'cuda',\n","    'parallelize': True,\n","    'project_step': 'Step2_2_SGD', # [Step2_1,Step2_2,Step3_1,Step3_2,Step4]\n","    'verbose': False,\n","    'checkpoint_root': CHECKPOINT_ROOT,\n","    'power': 0.9,\n","    'evalIterations': 100,\n","    'adversarial':False     \n","}"]},{"cell_type":"markdown","metadata":{},"source":["#### Adam"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# for kaggle:\n","config = {\n","    'model_name': 'BiSeNet', # [DeepLabV2, BiSeNet]\n","    'train_dataset_name': 'CityScapes', # [CityScapes, GTA5]\n","    'val_dataset_name': 'CityScapes', # [CityScapes]\n","    'n_classes': 19,\n","    'epochs': 50,\n","    'augmented': False,\n","    'augmentedType': 'transform1', # [transform1,transform2,transform3,transform4]\n","    'optimizer_name': 'Adam', # [SGD, Adam]\n","    'lr': 2.5e-4,\n","    'momentum': 0.9,\n","    'weight_decay': 5e-4,\n","    'loss_fn_name': 'CrossEntropyLoss', # [CrossEntropyLoss]\n","    'ignore_index': 255,\n","    'batch_size': 8, # [2,4,8]\n","    'n_workers': 4, # [0,2,4]\n","    'device': 'cuda',\n","    'parallelize': True,\n","    'project_step': 'Step2_2_Adam', # [Step2_1,Step2_2,Step3_1,Step3_2,Step4]\n","    'verbose': False,\n","    'checkpoint_root': CHECKPOINT_ROOT,\n","    'power': 0.9,\n","    'evalIterations': 100,\n","    'adversarial':False\n","}"]},{"cell_type":"markdown","metadata":{},"source":["### Step 3.1"]},{"cell_type":"markdown","metadata":{},"source":["#### SGD"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# for kaggle:\n","config = {\n","    'model_name': 'BiSeNet', # [DeepLabV2, BiSeNet]\n","    'train_dataset_name': 'GTA5', # [CityScapes, GTA5]\n","    'val_dataset_name': 'CityScapes', # [CityScapes]\n","    'n_classes': 19,\n","    'epochs': 50,\n","    'augmented': False,\n","    'augmentedType': 'transform1', # [transform1,transform2,transform3,transform4]\n","    'optimizer_name': 'SGD', # [SGD, Adam]\n","    'lr': 2.5e-4,\n","    'momentum': 0.9,\n","    'weight_decay': 5e-4,\n","    'loss_fn_name': 'CrossEntropyLoss', # [CrossEntropyLoss]\n","    'ignore_index': 255,\n","    'batch_size': 8, # [2,4,8]\n","    'n_workers': 4, # [0,2,4]\n","    'device': 'cuda',\n","    'parallelize': True,\n","    'project_step': 'Step3_1_SGD', # [Step2_1,Step2_2,Step3_1,Step3_2,Step4]\n","    'verbose': True,\n","    'checkpoint_root': CHECKPOINT_ROOT,\n","    'power': 0.9,\n","    'evalIterations': 100,\n","    'adversarial':False\n","}"]},{"cell_type":"markdown","metadata":{},"source":["#### Adam"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# for kaggle:\n","config = {\n","    'model_name': 'BiSeNet', # [DeepLabV2, BiSeNet]\n","    'train_dataset_name': 'GTA5', # [CityScapes, GTA5]\n","    'val_dataset_name': 'CityScapes', # [CityScapes]\n","    'n_classes': 19,\n","    'epochs': 50,\n","    'augmented': False,\n","    'augmentedType': 'transform1', # [transform1,transform2,transform3,transform4]\n","    'optimizer_name': 'Adam', # [SGD, Adam]\n","    'lr': 2.5e-4,\n","    'momentum': 0.9,\n","    'weight_decay': 5e-4,\n","    'loss_fn_name': 'CrossEntropyLoss', # [CrossEntropyLoss]\n","    'ignore_index': 255,\n","    'batch_size': 8, # [2,4,8]\n","    'n_workers': 4, # [0,2,4]\n","    'device': 'cuda',\n","    'parallelize': True,\n","    'project_step': 'Step3_1_Adam', # [Step2_1,Step2_2,Step3_1,Step3_2,Step4]\n","    'verbose': True,\n","    'checkpoint_root': CHECKPOINT_ROOT,\n","    'power': 0.9,\n","    'evalIterations': 100,\n","    'adversarial':False\n","}"]},{"cell_type":"markdown","metadata":{},"source":["### Step 3.2"]},{"cell_type":"markdown","metadata":{},"source":["#### L'optimizer va scelto sulla base del risultato ottenuto dallo step 3.1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T13:11:50.911893Z","iopub.status.busy":"2024-07-09T13:11:50.907203Z","iopub.status.idle":"2024-07-09T13:11:50.918363Z","shell.execute_reply":"2024-07-09T13:11:50.917418Z","shell.execute_reply.started":"2024-07-09T13:11:50.911857Z"},"trusted":true},"outputs":[],"source":["# for kaggle:\n","config = {\n","    'model_name': 'BiSeNet', # [DeepLabV2, BiSeNet]\n","    'train_dataset_name': 'GTA5', # [CityScapes, GTA5]\n","    'val_dataset_name': 'CityScapes', # [CityScapes]\n","    'n_classes': 19,\n","    'epochs': 50,\n","    'augmented': True,\n","    'augmentedType': 'transform1', # [transform1,transform2,transform3,transform4]\n","    'optimizer_name': 'SGD', # [SGD, Adam]\n","    'lr': 2.5e-4,\n","    'momentum': 0.9,\n","    'weight_decay': 5e-4,\n","    'loss_fn_name': 'CrossEntropyLoss', # [CrossEntropyLoss]\n","    'ignore_index': 255,\n","    'batch_size': 8, # [2,4,8]\n","    'n_workers': 4, # [0,2,4]\n","    'device': 'cuda',\n","    'parallelize': True,\n","    'project_step': 'Step3_2_SGD_transform1', # [Step2_1,Step2_2,Step3_1,Step3_2,Step4]\n","    'verbose': False,\n","    'checkpoint_root': CHECKPOINT_ROOT,\n","    'power': 0.9,\n","    'evalIterations': 100,\n","    'adversarial':False\n","}"]},{"cell_type":"markdown","metadata":{},"source":["### Step 4"]},{"cell_type":"markdown","metadata":{},"source":["#### TODO"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-08T14:00:52.937250Z","iopub.status.busy":"2024-07-08T14:00:52.936852Z","iopub.status.idle":"2024-07-08T14:00:52.949580Z","shell.execute_reply":"2024-07-08T14:00:52.948588Z","shell.execute_reply.started":"2024-07-08T14:00:52.937218Z"},"trusted":true},"outputs":[],"source":["# for kaggle:\n","config = {\n","    'model_name': 'BiSeNet', # [DeepLabV2, BiSeNet]\n","    'train_dataset_name': 'CityScapes', # [CityScapes, GTA5]\n","    'val_dataset_name': 'CityScapes', # [CityScapes]\n","    'n_classes': 19,\n","    'epochs': 5,\n","    'augmented': True,\n","    'augmentedType': 'transform3', # [transform1,transform2,transform3,transform4]\n","    'optimizer_name': 'Adam', # [SGD, Adam]\n","    'lr': 2.5e-4,\n","    'momentum': 0.9,\n","    'weight_decay': 5e-4,\n","    'loss_fn_name': 'CrossEntropyLoss', # [CrossEntropyLoss]\n","    'ignore_index': 255,\n","    'batch_size': 8, # [2,4,8]\n","    'n_workers': 0, # [0,2,4]\n","    'device': 'cuda',\n","    'parallelize': True,\n","    'project_step': 'Step4_Adam', # [Step2_1,Step2_2,Step3_1,Step3_2,Step4]\n","    'verbose': False,\n","    'checkpoint_root': CHECKPOINT_ROOT,\n","    'power': 0.9,\n","    'evalIterations': 100,\n","    'adversarial': True\n","}"]},{"cell_type":"markdown","metadata":{},"source":["## Main"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T13:11:50.919776Z","iopub.status.busy":"2024-07-09T13:11:50.919466Z","iopub.status.idle":"2024-07-09T13:11:51.069491Z","shell.execute_reply":"2024-07-09T13:11:51.068555Z","shell.execute_reply.started":"2024-07-09T13:11:50.919754Z"},"trusted":true},"outputs":[],"source":["# Then clean the cache\n","torch.cuda.empty_cache()\n","# then collect the garbage\n","import gc\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T13:11:51.070809Z","iopub.status.busy":"2024-07-09T13:11:51.070573Z"},"trusted":true},"outputs":[],"source":["if __name__ == '__main__':\n","\n","    # for kaggle:\n","    pipeline(\n","        model_name=config['model_name'], \n","        train_dataset_name=config['train_dataset_name'], \n","        val_dataset_name=config['val_dataset_name'],\n","        n_classes=config['n_classes'],\n","        epochs=config['epochs'],\n","        augmented=config['augmented'],\n","        augmentedType=config['augmentedType'],\n","        optimizer_name=config['optimizer_name'],\n","        lr=config['lr'],\n","        momentum=config['momentum'],\n","        weight_decay=config['weight_decay'],\n","        loss_fn_name=config['loss_fn_name'],\n","        ignore_index=config['ignore_index'],\n","        batch_size=config['batch_size'],\n","        n_workers=config['n_workers'],\n","        device=config['device'],\n","        parallelize=config['parallelize'],\n","        project_step=config['project_step'],\n","        verbose=config['verbose'],\n","        checkpoint_root=config['checkpoint_root'],\n","        power=config['power'],\n","        evalIterations=config['evalIterations'],\n","        adversarial=config['adversarial']\n","    )"]},{"cell_type":"markdown","metadata":{},"source":["# Kaggle directory setup"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-09T12:11:19.446595Z","iopub.status.busy":"2024-07-09T12:11:19.445761Z","iopub.status.idle":"2024-07-09T12:11:22.512453Z","shell.execute_reply":"2024-07-09T12:11:22.511461Z","shell.execute_reply.started":"2024-07-09T12:11:19.446557Z"},"trusted":true},"outputs":[],"source":["# Only for reset\n","#! rm /kaggle/working/dir_name/checkpoint.pth\n","#! rm -r /kaggle/working/dir_name/images\n","#! rm -r /kaggle/working/dir_name/logs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-09T11:43:43.594530Z","iopub.status.idle":"2024-07-09T11:43:43.594885Z","shell.execute_reply":"2024-07-09T11:43:43.594725Z","shell.execute_reply.started":"2024-07-09T11:43:43.594711Z"},"trusted":true},"outputs":[],"source":["! zip -r dir_name.zip /kaggle/working/dir_name"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-09T11:43:43.596539Z","iopub.status.idle":"2024-07-09T11:43:43.596924Z","shell.execute_reply":"2024-07-09T11:43:43.596757Z","shell.execute_reply.started":"2024-07-09T11:43:43.596742Z"},"trusted":true},"outputs":[],"source":["from IPython.display import FileLink\n","FileLink(r'dir_name.zip')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4930442,"sourceId":8299437,"sourceType":"datasetVersion"},{"datasetId":5037104,"sourceId":8452058,"sourceType":"datasetVersion"},{"datasetId":5292696,"sourceId":8801225,"sourceType":"datasetVersion"},{"modelInstanceId":39736,"sourceId":47461,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
