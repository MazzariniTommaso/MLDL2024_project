{"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1XNh4ReVvaCnFcvxsLVHahkWxy2OqKrm4","authorship_tag":"ABX9TyOekpKEeKXtTbQhIZpLmV9P"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8403691,"sourceType":"datasetVersion","datasetId":5000492},{"sourceId":8452058,"sourceType":"datasetVersion","datasetId":5037104},{"sourceId":8495419,"sourceType":"datasetVersion","datasetId":5069095},{"sourceId":47461,"sourceType":"modelInstanceVersion","modelInstanceId":39736}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Evaluating the domain shift problem in Semantic Segmentation \nIn semantic segmentation collecting manually annotated images is expensive. A popular solution consists in adopting synthetic datasets (i.e. artificial images generated in a simulation environment).\nSpecifically, in this step we employ the synthetic images from GTA5 [5] (source domain) to train our real-time segmentation network, which is then evaluated on the real images from Cityscapes [5] (target domain).\n- Dataset: GTA5 [5]\n- Training Set:  GTA5 \n- Validation Set: Cityscapes [5] validation split \n- Training epochs: 50\n- Training resolution (GTA5): 1280x720\n- Test resolution (Cityscapes): 1024x512\n- Backbone: ResNet18 (pre-trained on ImageNet) [2]\n- Semantic Classes: 19\n- Metrics: mIoU\n\n","metadata":{"id":"OAZEiffSoALO"}},{"cell_type":"code","source":"!pip install -U fvcore","metadata":{"execution":{"iopub.status.busy":"2024-05-31T11:24:42.769458Z","iopub.execute_input":"2024-05-31T11:24:42.769781Z","iopub.status.idle":"2024-05-31T11:25:02.597917Z","shell.execute_reply.started":"2024-05-31T11:24:42.769753Z","shell.execute_reply":"2024-05-31T11:25:02.596751Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting fvcore\n  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m790.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fvcore) (1.26.4)\nCollecting yacs>=0.1.6 (from fvcore)\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from fvcore) (6.0.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from fvcore) (4.66.1)\nRequirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.10/site-packages (from fvcore) (2.4.0)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from fvcore) (9.5.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from fvcore) (0.9.0)\nCollecting iopath>=0.1.7 (from fvcore)\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from iopath>=0.1.7->fvcore) (4.9.0)\nCollecting portalocker (from iopath>=0.1.7->fvcore)\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nBuilding wheels for collected packages: fvcore, iopath\n  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=c65c7800901cc4339f77b3b845c4de8054c95a9c4fbcc328fc9d584a4984fe98\n  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=5f03ab7e120f92b49a4e697fd8b5c7f2d2cc84ddca523a4ba38dc641df0769fc\n  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\nSuccessfully built fvcore iopath\nInstalling collected packages: yacs, portalocker, iopath, fvcore\nSuccessfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-2.8.2 yacs-0.1.8\n","output_type":"stream"}]},{"cell_type":"code","source":"# WANDB\n!pip install -q wandb","metadata":{"execution":{"iopub.status.busy":"2024-05-31T11:25:02.600130Z","iopub.execute_input":"2024-05-31T11:25:02.600434Z","iopub.status.idle":"2024-05-31T11:25:15.426426Z","shell.execute_reply.started":"2024-05-31T11:25:02.600406Z","shell.execute_reply":"2024-05-31T11:25:15.425057Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# If you run the model for the first time remove all the previus checkpoints\n! rm -r checkpoints/","metadata":{"execution":{"iopub.status.busy":"2024-05-31T11:25:15.428007Z","iopub.execute_input":"2024-05-31T11:25:15.428367Z","iopub.status.idle":"2024-05-31T11:25:16.428932Z","shell.execute_reply.started":"2024-05-31T11:25:15.428338Z","shell.execute_reply":"2024-05-31T11:25:16.427986Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"rm: cannot remove 'checkpoints/': No such file or directory\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 0 - Import libraries","metadata":{"id":"cNa1Un4uoZ6M"}},{"cell_type":"code","source":"import wandb\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nimport torch.optim as optim\n\nimport os\nimport zipfile\nimport numpy as np\nimport time\nfrom PIL import Image\n\nfrom fvcore.nn import FlopCountAnalysis, flop_count_table\n\nimport warnings\nwarnings.filterwarnings(action='ignore')","metadata":{"id":"mYTc9GfSouYh","executionInfo":{"status":"ok","timestamp":1715695760300,"user_tz":-120,"elapsed":13275,"user":{"displayName":"Tommaso Mazzarini","userId":"16046308562458308219"}},"execution":{"iopub.status.busy":"2024-05-31T11:25:16.431383Z","iopub.execute_input":"2024-05-31T11:25:16.431716Z","iopub.status.idle":"2024-05-31T11:25:16.538369Z","shell.execute_reply.started":"2024-05-31T11:25:16.431686Z","shell.execute_reply":"2024-05-31T11:25:16.537564Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# 1 - Start WanDB","metadata":{}},{"cell_type":"code","source":"wandb.login()","metadata":{"execution":{"iopub.status.busy":"2024-05-31T11:25:16.539502Z","iopub.execute_input":"2024-05-31T11:25:16.539779Z","iopub.status.idle":"2024-05-31T11:26:04.703875Z","shell.execute_reply.started":"2024-05-31T11:25:16.539755Z","shell.execute_reply":"2024-05-31T11:26:04.702923Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"# 2 - Model Pipeline","metadata":{}},{"cell_type":"code","source":"def model_pipeline(hyperparameters=None):\n\n    # tell wandb to get started\n    with wandb.init(project=\"MLDL-step3a\", config=hyperparameters):\n        # access all HPs through wandb.config, so logging matches execution!\n        config = wandb.config\n\n        # make the model, data, and optimization problem\n        model, train_loader, val_loader, criterion, optimizer, start_epoch = make(config)\n        \n        # and use them to train the model\n        train(model, train_loader, criterion, optimizer, config, start_epoch)\n\n        # and test its final performance\n        val(model, val_loader)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-05-31T11:26:04.705294Z","iopub.execute_input":"2024-05-31T11:26:04.706136Z","iopub.status.idle":"2024-05-31T11:26:04.713101Z","shell.execute_reply.started":"2024-05-31T11:26:04.706098Z","shell.execute_reply":"2024-05-31T11:26:04.712045Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# 3 - Datasets","metadata":{"id":"sKK0P3Aloi9U"}},{"cell_type":"code","source":"def make(config):\n    # Make the data\n    train, test = get_data(train=True), get_data(train=False)\n    train_loader = make_loader(train, batch_size=config.batch_size,train=True)\n    test_loader = make_loader(test, batch_size=config.batch_size,train=False)\n\n    # Make the model (BiSeNet with ResNet-18 backbone)\n    model = build_model(model_type='BiSeNet').cuda()\n\n    # Make the loss and optimizer\n    optimizer = optim.SGD(model.parameters(), \n                          lr=config.learning_rate, \n                          momentum=config.momentum, \n                          weight_decay=config.weight_decay)\n    \n    criterion = torch.nn.CrossEntropyLoss(ignore_index=255)\n    \n    # Load the last checkpoint\n    start_epoch = 0\n    #start_epoch = load_checkpoint(config, model, optimizer)\n    \n    return model, train_loader, test_loader, criterion, optimizer, start_epoch","metadata":{"execution":{"iopub.status.busy":"2024-05-31T11:26:04.714180Z","iopub.execute_input":"2024-05-31T11:26:04.714441Z","iopub.status.idle":"2024-05-31T11:26:04.726165Z","shell.execute_reply.started":"2024-05-31T11:26:04.714419Z","shell.execute_reply":"2024-05-31T11:26:04.725242Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n# Augumentation\nimage_transform = {\n    'cityscapes': transforms.Compose([\n        transforms.Resize((512,1024)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        transforms.RandomApply([\n            transforms.RandomHorizontalFlip(p=0.5), # Horizontal flip with probability 0.5 \n            transforms.ColorJitter(brightness=0.2,contrast=0.2, saturation=0.2, hue=0.1)# Color jittering\n        ], p=0.5)\n        \n    ]),\n    'gta': transforms.Compose([\n        transforms.Resize((720,1280)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        transforms.RandomApply([\n            transforms.RandomHorizontalFlip(p=0.5), # Horizontal flip with probability 0.5 \n            transforms.ColorJitter(brightness=0.2,contrast=0.2, saturation=0.2, hue=0.1)# Color jittering\n        ], p=0.5)\n    ])\n}\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-05-31T11:26:04.727467Z","iopub.execute_input":"2024-05-31T11:26:04.727804Z","iopub.status.idle":"2024-05-31T11:26:04.746208Z","shell.execute_reply.started":"2024-05-31T11:26:04.727754Z","shell.execute_reply":"2024-05-31T11:26:04.745170Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"\"\\n# Augumentation\\nimage_transform = {\\n    'cityscapes': transforms.Compose([\\n        transforms.Resize((512,1024)),\\n        transforms.ToTensor(),\\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\\n        transforms.RandomApply([\\n            transforms.RandomHorizontalFlip(p=0.5), # Horizontal flip with probability 0.5 \\n            transforms.ColorJitter(brightness=0.2,contrast=0.2, saturation=0.2, hue=0.1)# Color jittering\\n        ], p=0.5)\\n        \\n    ]),\\n    'gta': transforms.Compose([\\n        transforms.Resize((720,1280)),\\n        transforms.ToTensor(),\\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\\n        transforms.RandomApply([\\n            transforms.RandomHorizontalFlip(p=0.5), # Horizontal flip with probability 0.5 \\n            transforms.ColorJitter(brightness=0.2,contrast=0.2, saturation=0.2, hue=0.1)# Color jittering\\n        ], p=0.5)\\n    ])\\n}\\n\""},"metadata":{}}]},{"cell_type":"code","source":"# Define transforms for preprocessing\n\nimage_transform = {\n    'cityscapes': transforms.Compose([\n        transforms.Resize((512,1024)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ]),\n    'gta': transforms.Compose([\n        transforms.Resize((720,1280)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n}\n\nlabel_transform = {\n    'cityscapes': transforms.Compose([\n        transforms.Resize((512,1024))\n    ]),\n    'gta': transforms.Compose([\n        transforms.Resize((720,1280))\n    ])\n}\n\n# GTA5 for train and CityScapes for test\ncitiyscapes_dir ='/kaggle/input/cityscapes/Cityscapes/Cityspaces'\n#gta_dir = '/kaggle/input/gta5-dataset/GTA5'\ngta_dir = '/kaggle/input/gta5-dataset-with-masks'\n\ndef get_data(train=True):\n    if train == True:\n        # train dataset\n        dataset = GTA5(root_dir=gta_dir, image_transform=image_transform['gta'], label_transform=label_transform['gta'])\n    else:\n        # test dataset\n        dataset = CityScapes(root_dir=citiyscapes_dir, split='val', image_transform=image_transform['cityscapes'], label_transform=label_transform['cityscapes'])\n    \n    return dataset\n\n\ndef make_loader(dataset, batch_size = 8, train=True):\n    if train == True:\n        # train dataloader\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n    else:\n        # test dataloader\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False,drop_last=True)\n    \n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2024-05-31T11:26:04.747388Z","iopub.execute_input":"2024-05-31T11:26:04.747804Z","iopub.status.idle":"2024-05-31T11:26:04.760643Z","shell.execute_reply.started":"2024-05-31T11:26:04.747772Z","shell.execute_reply":"2024-05-31T11:26:04.759909Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# TO IMPLEMENT\ndef build_model(model_type):\n    if model_type == 'BiSeNet':\n        return BiSeNet(num_classes=19, context_path=\"resnet18\")\n    elif model_type == 'DeepLabV2':\n        pretrain_model_path = '/kaggle/input/deeplab_v2_model/pytorch/model_weight/1/deeplab_resnet_pretrained_imagenet.pth'\n        return get_deeplab_v2(num_classes=19, pretrain=True, pretrain_model_path=pretrain_model_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T11:26:04.763721Z","iopub.execute_input":"2024-05-31T11:26:04.764001Z","iopub.status.idle":"2024-05-31T11:26:04.773873Z","shell.execute_reply.started":"2024-05-31T11:26:04.763969Z","shell.execute_reply":"2024-05-31T11:26:04.773048Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## 3.1 - CityScapes","metadata":{}},{"cell_type":"code","source":"class CityScapes(Dataset):\n    def __init__(self, root_dir, split='train', image_transform=None, label_transform=None):\n        super(CityScapes, self).__init__()\n        \"\"\"\n        Args:\n            root_dir (string): Directory with all the images and annotations.\n            split (string): 'train' or 'val'.\n            transform (callable, optional): Optional transform to be applied on a sample.\n        \"\"\"\n\n        self.root_dir = root_dir\n        self.split = split\n        self.image_transform = image_transform\n        self.label_transform = label_transform\n\n        # Get the image and label directories\n        self.image_dir = os.path.join(root_dir, 'images', split)\n        self.label_dir = os.path.join(root_dir, 'gtFine', split)\n\n        # Get a list of all image files\n        self.image_files = []\n        for city_dir in os.listdir(self.image_dir):\n            city_image_dir = os.path.join(self.image_dir, city_dir)\n            self.image_files.extend([os.path.join(city_image_dir, f) for f in os.listdir(city_image_dir) if f.endswith('.png')])\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_name = self.image_files[idx]\n\n        # Get the corresponding label image path\n        label_name = img_name.replace('images', 'gtFine').replace('_leftImg8bit', '_gtFine_labelTrainIds')\n\n        # Load image and label\n        image = Image.open(img_name).convert('RGB')\n        label = Image.open(label_name).convert('L')\n\n        if self.image_transform:\n            image = self.image_transform(image)\n        if self.label_transform:\n            label = self.label_transform(label)\n\n        label = torch.Tensor(np.array(label))\n\n        return image, label","metadata":{"id":"TE5jHHpwaduG","executionInfo":{"status":"ok","timestamp":1715695833931,"user_tz":-120,"elapsed":3,"user":{"displayName":"Tommaso Mazzarini","userId":"16046308562458308219"}},"execution":{"iopub.status.busy":"2024-05-31T11:26:04.775170Z","iopub.execute_input":"2024-05-31T11:26:04.775796Z","iopub.status.idle":"2024-05-31T11:26:04.786713Z","shell.execute_reply.started":"2024-05-31T11:26:04.775760Z","shell.execute_reply":"2024-05-31T11:26:04.785769Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## GTA5","metadata":{}},{"cell_type":"code","source":"import matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as T\n\nclass GTA5(Dataset):\n    def __init__(self, root_dir, image_transform=None, label_transform=None):\n        super(GTA5, self).__init__()\n        \"\"\"\n        Args:\n            root_dir (string): Directory with all the images and annotations.\n            transform (callable, optional): Optional transform to be applied on a sample.\n        \"\"\"\n\n        self.root_dir = root_dir\n        self.image_transform = image_transform\n        self.label_transform = label_transform\n\n        # Get the image and label directories\n        self.image_dir = os.path.join(root_dir, 'images')\n        self.label_dir = os.path.join(root_dir, 'masks')\n\n        # Get a list of all image files\n        self.image_files = []\n        # Get a list of all files in the images directory\n        for file_name in os.listdir(self.image_dir):\n            file_path = os.path.join(self.image_dir, file_name)\n            if os.path.isfile(file_path):\n                self.image_files.append(file_name)\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_name, label_name = self.image_files[idx], self.image_files[idx]\n        \n        img_path = os.path.join(self.image_dir, img_name)\n        label_path = os.path.join(self.label_dir, label_name)\n\n        # Load image and label\n        image = Image.open(img_path).convert('RGB')\n        label = Image.open(label_path).convert('L')\n        \n        if self.image_transform:\n            image = self.image_transform(image)\n        if self.label_transform:\n            label = self.label_transform(label)\n\n        label = torch.Tensor(np.array(label)) \n\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2024-05-31T11:26:04.787731Z","iopub.execute_input":"2024-05-31T11:26:04.787981Z","iopub.status.idle":"2024-05-31T11:26:04.803271Z","shell.execute_reply.started":"2024-05-31T11:26:04.787960Z","shell.execute_reply":"2024-05-31T11:26:04.802245Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# 4 - BiSeNet","metadata":{"id":"breeFfksou_h"}},{"cell_type":"code","source":"class ConvBlock(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n                               stride=stride, padding=padding, bias=False)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU()\n\n    def forward(self, input):\n        x = self.conv1(input)\n        return self.relu(self.bn(x))\n\n\nclass Spatial_path(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.convblock1 = ConvBlock(in_channels=3, out_channels=64)\n        self.convblock2 = ConvBlock(in_channels=64, out_channels=128)\n        self.convblock3 = ConvBlock(in_channels=128, out_channels=256)\n\n    def forward(self, input):\n        x = self.convblock1(input)\n        x = self.convblock2(x)\n        x = self.convblock3(x)\n        return x\n\n\nclass AttentionRefinementModule(torch.nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.sigmoid = nn.Sigmoid()\n        self.in_channels = in_channels\n        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n\n    def forward(self, input):\n        # global average pooling\n        x = self.avgpool(input)\n        assert self.in_channels == x.size(1), 'in_channels and out_channels should all be {}'.format(x.size(1))\n        x = self.conv(x)\n        x = self.sigmoid(self.bn(x))\n        # x = self.sigmoid(x)\n        # channels of input and x should be same\n        x = torch.mul(input, x)\n        return x\n\n\nclass FeatureFusionModule(torch.nn.Module):\n    def __init__(self, num_classes, in_channels):\n        super().__init__()\n        # self.in_channels = input_1.channels + input_2.channels\n        # resnet101 3328 = 256(from spatial path) + 1024(from context path) + 2048(from context path)\n        # resnet18  1024 = 256(from spatial path) + 256(from context path) + 512(from context path)\n        self.in_channels = in_channels\n\n        self.convblock = ConvBlock(in_channels=self.in_channels, out_channels=num_classes, stride=1)\n        self.conv1 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n        self.sigmoid = nn.Sigmoid()\n        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n\n    def forward(self, input_1, input_2):\n        x = torch.cat((input_1, input_2), dim=1)\n        assert self.in_channels == x.size(1), 'in_channels of ConvBlock should be {}'.format(x.size(1))\n        feature = self.convblock(x)\n        x = self.avgpool(feature)\n\n        x = self.relu(self.conv1(x))\n        x = self.sigmoid(self.conv2(x))\n        x = torch.mul(feature, x)\n        x = torch.add(x, feature)\n        return x\n\n\nclass BiSeNet(torch.nn.Module):\n    def __init__(self, num_classes, context_path):\n        super().__init__()\n        # build spatial path\n        self.saptial_path = Spatial_path()\n\n        # build context path\n        self.context_path = build_contextpath(name=context_path)\n\n        # build attention refinement module  for resnet 101\n        if context_path == 'resnet101':\n            self.attention_refinement_module1 = AttentionRefinementModule(1024, 1024)\n            self.attention_refinement_module2 = AttentionRefinementModule(2048, 2048)\n            # supervision block\n            self.supervision1 = nn.Conv2d(in_channels=1024, out_channels=num_classes, kernel_size=1)\n            self.supervision2 = nn.Conv2d(in_channels=2048, out_channels=num_classes, kernel_size=1)\n            # build feature fusion module\n            self.feature_fusion_module = FeatureFusionModule(num_classes, 3328)\n\n        elif context_path == 'resnet18':\n            # build attention refinement module  for resnet 18\n            self.attention_refinement_module1 = AttentionRefinementModule(256, 256)\n            self.attention_refinement_module2 = AttentionRefinementModule(512, 512)\n            # supervision block\n            self.supervision1 = nn.Conv2d(in_channels=256, out_channels=num_classes, kernel_size=1)\n            self.supervision2 = nn.Conv2d(in_channels=512, out_channels=num_classes, kernel_size=1)\n            # build feature fusion module\n            self.feature_fusion_module = FeatureFusionModule(num_classes, 1024)\n        else:\n            print('Error: unspport context_path network \\n')\n\n        # build final convolution\n        self.conv = nn.Conv2d(in_channels=num_classes, out_channels=num_classes, kernel_size=1)\n\n        self.init_weight()\n\n        self.mul_lr = []\n        self.mul_lr.append(self.saptial_path)\n        self.mul_lr.append(self.attention_refinement_module1)\n        self.mul_lr.append(self.attention_refinement_module2)\n        self.mul_lr.append(self.supervision1)\n        self.mul_lr.append(self.supervision2)\n        self.mul_lr.append(self.feature_fusion_module)\n        self.mul_lr.append(self.conv)\n\n    def init_weight(self):\n        for name, m in self.named_modules():\n            if 'context_path' not in name:\n                if isinstance(m, nn.Conv2d):\n                    nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n                elif isinstance(m, nn.BatchNorm2d):\n                    m.eps = 1e-5\n                    m.momentum = 0.1\n                    nn.init.constant_(m.weight, 1)\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, input):\n        # output of spatial path\n        sx = self.saptial_path(input)\n\n        # output of context path\n        cx1, cx2, tail = self.context_path(input)\n        cx1 = self.attention_refinement_module1(cx1)\n        cx2 = self.attention_refinement_module2(cx2)\n        cx2 = torch.mul(cx2, tail)\n        # upsampling\n        cx1 = torch.nn.functional.interpolate(cx1, size=sx.size()[-2:], mode='bilinear')\n        cx2 = torch.nn.functional.interpolate(cx2, size=sx.size()[-2:], mode='bilinear')\n        cx = torch.cat((cx1, cx2), dim=1)\n\n        if self.training == True:\n            cx1_sup = self.supervision1(cx1)\n            cx2_sup = self.supervision2(cx2)\n            cx1_sup = torch.nn.functional.interpolate(cx1_sup, size=input.size()[-2:], mode='bilinear')\n            cx2_sup = torch.nn.functional.interpolate(cx2_sup, size=input.size()[-2:], mode='bilinear')\n\n        # output of feature fusion module\n        result = self.feature_fusion_module(sx, cx)\n\n        # upsampling\n        result = torch.nn.functional.interpolate(result, scale_factor=8, mode='bilinear')\n        result = self.conv(result)\n\n        if self.training == True:\n            return result, cx1_sup, cx2_sup\n\n        return result","metadata":{"id":"sOluxCUSo6TN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715695905826,"user_tz":-120,"elapsed":12513,"user":{"displayName":"Tommaso Mazzarini","userId":"16046308562458308219"}},"outputId":"18e9c583-19cc-4b94-f206-b30ba08384cb","execution":{"iopub.status.busy":"2024-05-31T11:26:04.804849Z","iopub.execute_input":"2024-05-31T11:26:04.805193Z","iopub.status.idle":"2024-05-31T11:26:04.838142Z","shell.execute_reply.started":"2024-05-31T11:26:04.805163Z","shell.execute_reply":"2024-05-31T11:26:04.837338Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class resnet18(torch.nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = models.resnet18(pretrained=pretrained)\n        self.conv1 = self.features.conv1\n        self.bn1 = self.features.bn1\n        self.relu = self.features.relu\n        self.maxpool1 = self.features.maxpool\n        self.layer1 = self.features.layer1\n        self.layer2 = self.features.layer2\n        self.layer3 = self.features.layer3\n        self.layer4 = self.features.layer4\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.relu(self.bn1(x))\n        x = self.maxpool1(x)\n        feature1 = self.layer1(x)  # 1 / 4\n        feature2 = self.layer2(feature1)  # 1 / 8\n        feature3 = self.layer3(feature2)  # 1 / 16\n        feature4 = self.layer4(feature3)  # 1 / 32\n        # global average pooling to build tail\n        tail = torch.mean(feature4, 3, keepdim=True)\n        tail = torch.mean(tail, 2, keepdim=True)\n        return feature3, feature4, tail\n\n\nclass resnet101(torch.nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = models.resnet101(pretrained=pretrained)\n        self.conv1 = self.features.conv1\n        self.bn1 = self.features.bn1\n        self.relu = self.features.relu\n        self.maxpool1 = self.features.maxpool\n        self.layer1 = self.features.layer1\n        self.layer2 = self.features.layer2\n        self.layer3 = self.features.layer3\n        self.layer4 = self.features.layer4\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.relu(self.bn1(x))\n        x = self.maxpool1(x)\n        feature1 = self.layer1(x)  # 1 / 4\n        feature2 = self.layer2(feature1)  # 1 / 8\n        feature3 = self.layer3(feature2)  # 1 / 16\n        feature4 = self.layer4(feature3)  # 1 / 32\n        # global average pooling to build tail\n        tail = torch.mean(feature4, 3, keepdim=True)\n        tail = torch.mean(tail, 2, keepdim=True)\n        return feature3, feature4, tail\n\n\ndef build_contextpath(name):\n    model = {\n        'resnet18': resnet18(pretrained=True),\n        'resnet101': resnet101(pretrained=True)\n    }\n    return model[name]","metadata":{"execution":{"iopub.status.busy":"2024-05-31T11:26:04.839421Z","iopub.execute_input":"2024-05-31T11:26:04.839871Z","iopub.status.idle":"2024-05-31T11:26:04.854661Z","shell.execute_reply.started":"2024-05-31T11:26:04.839841Z","shell.execute_reply":"2024-05-31T11:26:04.853848Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# 5 - Training","metadata":{}},{"cell_type":"code","source":"def train(model, dataloader, criterion, optimizer, config, start_epoch):\n    \n    for epoch in range(start_epoch, config.epochs):\n        running_loss = 0.0\n        total_mIOU = 0\n        total_images = 0\n        \n        for _, (inputs, targets) in enumerate(dataloader):\n\n            inputs, targets = inputs.cuda(), id_processing(targets).cuda()\n\n            outputs = model(inputs)\n\n            loss = criterion(outputs[0], targets)\n\n            # Backprpagation\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n            _, predicted = outputs[0].max(1)\n\n            running_mIOU = mean_iou(outputs[0].size()[1], predicted, targets)\n            total_mIOU += running_mIOU.sum().item()\n            total_images += len(predicted)\n                \n        train_loss = running_loss / len(dataloader)\n        mIOU = total_mIOU/total_images\n        \n        # Save the train metrics by using wandb\n        train_log(train_loss, mIOU, epoch)\n        \n        # Save checkpoint (overwrite)\n        # save_checkpoint(config, model, optimizer, train_loss, mIOU, epoch)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T11:26:04.856010Z","iopub.execute_input":"2024-05-31T11:26:04.856355Z","iopub.status.idle":"2024-05-31T11:26:04.871813Z","shell.execute_reply.started":"2024-05-31T11:26:04.856332Z","shell.execute_reply":"2024-05-31T11:26:04.871054Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def id_processing(targets):\n    targets = targets.cuda()\n    \n    # Define valid indices\n    valid_indices = torch.tensor(list(range(19)) + [255]).to(targets.device)\n\n    # Replace all IDs not in valid_indices with 255\n    processed_targets = torch.where(torch.isin(targets, valid_indices), targets, torch.tensor(255, device=targets.device))\n    \n    # Release GPU memory IMPORTANT\n    del targets\n    torch.cuda.empty_cache()\n\n    return processed_targets.long()","metadata":{"execution":{"iopub.status.busy":"2024-05-31T11:26:04.872993Z","iopub.execute_input":"2024-05-31T11:26:04.873298Z","iopub.status.idle":"2024-05-31T11:26:04.884983Z","shell.execute_reply.started":"2024-05-31T11:26:04.873270Z","shell.execute_reply":"2024-05-31T11:26:04.884166Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def mean_iou(num_classes, pred, target):\n    mIOU = 0\n    for i in range(len(pred)):\n        hist = fast_hist(target[i].cpu().numpy(),pred[i].cpu().numpy(), num_classes)\n        IOU = per_class_iou(hist)\n        mIOU = mIOU + sum(IOU)/num_classes\n    return mIOU\n\ndef fast_hist(a, b, n):\n    \"\"\"\n    a and b are predict and mask respectively\n    n is the number of classes\n    \"\"\"\n    k = (a >= 0) & (a < n) #assign True if the value is in the range between 0 and 18 (class labels)\n    return np.bincount(n * a[k].astype(int) + b[k], minlength=n ** 2).reshape((n, n))\n\ndef per_class_iou(hist):\n    epsilon = 1e-5\n    return (np.diag(hist)) / (hist.sum(1) + hist.sum(0) - np.diag(hist) + epsilon)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T11:26:04.886358Z","iopub.execute_input":"2024-05-31T11:26:04.887122Z","iopub.status.idle":"2024-05-31T11:26:04.895779Z","shell.execute_reply.started":"2024-05-31T11:26:04.887095Z","shell.execute_reply":"2024-05-31T11:26:04.894842Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def train_log(loss, mIOU, epoch):\n    #wandb.log({\"epoch\": epoch, \"loss\": loss, \"mIOU\":mIOU})\n    wandb.log({\"loss\": loss, \"mIOU\":mIOU}, step= epoch)\n    print(f\"----------------------------------\")\n    print(f\"Loss after {epoch} epochs: {loss:.3f}\")\n    print(f\"mIOU after {epoch} epochs: {mIOU:.3f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-05-31T11:26:04.896992Z","iopub.execute_input":"2024-05-31T11:26:04.897346Z","iopub.status.idle":"2024-05-31T11:26:04.910609Z","shell.execute_reply.started":"2024-05-31T11:26:04.897317Z","shell.execute_reply":"2024-05-31T11:26:04.909696Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(config, model, optimizer, train_loss, mIOU, epoch):\n    checkpoint_path = os.path.join(config.checkpoint_dir, \"checkpoint.pth\")\n    torch.save({\n        'epoch': epoch + 1,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': train_loss,\n        'mIOU': mIOU\n    }, checkpoint_path)\n    print(f\"Checkpoint saved in {checkpoint_path} | Epoch: {epoch}\")\n    \n    \ndef load_checkpoint(config, model, optimizer):\n    if os.path.exists(config.checkpoint_dir):\n        checkpoint = torch.load(config.checkpoint_dir + \"/checkpoint.pth\")\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        start_epoch = checkpoint['epoch']\n        print(f\"Checkpoint found. Resuming from epoch {start_epoch}.\")\n        return start_epoch\n    else:\n        os.mkdir(config.checkpoint_dir) # divide the directory wrt the model (eg. checkpoints/DeepLabV2, checkpoints/BiSeNet)\n        print(\"No checkpoint found. Starting from scratch.\")\n        return 0","metadata":{"execution":{"iopub.status.busy":"2024-05-31T11:26:04.911679Z","iopub.execute_input":"2024-05-31T11:26:04.912518Z","iopub.status.idle":"2024-05-31T11:26:04.922219Z","shell.execute_reply.started":"2024-05-31T11:26:04.912494Z","shell.execute_reply":"2024-05-31T11:26:04.921370Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# 6 - Validation","metadata":{"id":"vcIkDP8ho8wt"}},{"cell_type":"code","source":"# Validation method\ndef val(model, dataloader):\n    model.eval()\n    total_mIOU = 0\n    total_images = 0\n    latency_list = []\n    FPS_list = []\n    \n    with torch.no_grad():\n        for _, (inputs, targets) in enumerate(dataloader):\n            inputs, targets = inputs.cuda(), id_processing(targets).cuda()\n            \n            start = time.time() # Record start time\n            outputs = model(inputs)\n            end = time.time() # Record end time\n\n            # Calculate latency for this iteration\n            latency_i = end - start\n            latency_list.append(latency_i)\n\n            # Calculate FPS for this iteration\n            FPS_i = 1 / latency_i\n            FPS_list.append(FPS_i)\n\n            _, predicted = outputs.max(1)\n            \n            running_mIOU = mean_iou(outputs.size()[1], predicted, targets)\n            total_mIOU += running_mIOU.sum().item()\n            total_images += len(predicted)\n        \n    mIOU = total_mIOU/total_images\n    latency = np.sum(latency_list) / len(latency_list)\n    test_FPS = np.sum(FPS_list) / len(FPS_list)\n    \n    # compute flops and #param\n    image, _ = next(iter(dataloader))\n    height, width = image.shape[2], image.shape[3]\n    zero_image = torch.zeros((1, 3, height, width))\n    flops = FlopCountAnalysis(model, zero_image.cuda())\n    print(flop_count_table(flops))\n\n    print(f'\\n\\nmIoU: {(mIOU*100):.3f}%, Latency: {latency:.3f}, FPS: {test_FPS:.3f}')\n\n    wandb.log({\"mIOU\":mIOU,\"latency\":latency,\"FPS\":test_FPS})","metadata":{"execution":{"iopub.status.busy":"2024-05-31T11:26:04.923515Z","iopub.execute_input":"2024-05-31T11:26:04.923853Z","iopub.status.idle":"2024-05-31T11:26:04.937065Z","shell.execute_reply.started":"2024-05-31T11:26:04.923824Z","shell.execute_reply":"2024-05-31T11:26:04.936183Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# 7 - Hyperparameter Sweeps using WanDB","metadata":{}},{"cell_type":"code","source":"sweep_config= {\n    'name': 'DomainAdaptation-sweep',\n    'metric': {'name': 'loss', 'goal': 'minimize'}, # the goal is maximize the accuracy\n    'method': 'random', # test all possible combinations of the hyperparameters\n    'parameters': {\n        'epochs': {'values': [5]},        \n        'learning_rate': {'values': [0.1, 0.001, 0.0001]}, # 2 parameters to optimize during the sweep\n        'batch_size': {'values': [2, 4, 8]},\n        'momentum': {'values': [0.9]},\n        'weight_decay': {'values': [5e-4]}\n    }\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-31T11:26:04.938142Z","iopub.execute_input":"2024-05-31T11:26:04.938396Z","iopub.status.idle":"2024-05-31T11:26:04.951068Z","shell.execute_reply.started":"2024-05-31T11:26:04.938375Z","shell.execute_reply":"2024-05-31T11:26:04.950285Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"sweep_id = wandb.sweep(sweep=sweep_config, project=\"MLDL-step3a\")","metadata":{"execution":{"iopub.status.busy":"2024-05-31T11:26:04.952041Z","iopub.execute_input":"2024-05-31T11:26:04.952318Z","iopub.status.idle":"2024-05-31T11:26:05.171709Z","shell.execute_reply.started":"2024-05-31T11:26:04.952297Z","shell.execute_reply":"2024-05-31T11:26:05.170747Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Create sweep with ID: g9ioen8f\nSweep URL: https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f\n","output_type":"stream"}]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    print(\"Start hyperparameter sweeps\\n\")\n    wandb.agent(sweep_id, function=model_pipeline, count=10)\nelse:\n    print(\"CUDA is Not available\")","metadata":{"execution":{"iopub.status.busy":"2024-05-31T11:26:05.172912Z","iopub.execute_input":"2024-05-31T11:26:05.173286Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Start hyperparameter sweeps\n\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pfk8sgsh with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.9\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtommasomazzarini2001\u001b[0m (\u001b[33mpolito-tmazzarini\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240531_112607-pfk8sgsh</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/pfk8sgsh' target=\"_blank\">vivid-sweep-1</a></strong> to <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/pfk8sgsh' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/pfk8sgsh</a>"},"metadata":{}},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 128MB/s] \nDownloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n100%|██████████| 171M/171M [00:01<00:00, 154MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"----------------------------------\nLoss after 0 epochs: 1.098\nmIOU after 0 epochs: 0.152%\n----------------------------------\nLoss after 1 epochs: 0.717\nmIOU after 1 epochs: 0.183%\n----------------------------------\nLoss after 2 epochs: 0.635\nmIOU after 2 epochs: 0.193%\n----------------------------------\nLoss after 3 epochs: 0.588\nmIOU after 3 epochs: 0.201%\n----------------------------------\nLoss after 4 epochs: 0.557\nmIOU after 4 epochs: 0.206%\n| module                                      | #parameters or shape   | #flops     |\n|:--------------------------------------------|:-----------------------|:-----------|\n| model                                       | 12.582M                | 25.78G     |\n|  saptial_path                               |  0.371M                |  5.088G    |\n|   saptial_path.convblock1                   |   1.856K               |   0.243G   |\n|    saptial_path.convblock1.conv1            |    1.728K              |    0.226G  |\n|    saptial_path.convblock1.bn               |    0.128K              |    16.777M |\n|   saptial_path.convblock2                   |   73.984K              |   2.424G   |\n|    saptial_path.convblock2.conv1            |    73.728K             |    2.416G  |\n|    saptial_path.convblock2.bn               |    0.256K              |    8.389M  |\n|   saptial_path.convblock3                   |   0.295M               |   2.42G    |\n|    saptial_path.convblock3.conv1            |    0.295M              |    2.416G  |\n|    saptial_path.convblock3.bn               |    0.512K              |    4.194M  |\n|  context_path.features                      |  11.69M                |  19.002G   |\n|   context_path.features.conv1               |   9.408K               |   1.233G   |\n|    context_path.features.conv1.weight       |    (64, 3, 7, 7)       |            |\n|   context_path.features.bn1                 |   0.128K               |   16.777M  |\n|    context_path.features.bn1.weight         |    (64,)               |            |\n|    context_path.features.bn1.bias           |    (64,)               |            |\n|   context_path.features.layer1              |   0.148M               |   4.849G   |\n|    context_path.features.layer1.0           |    73.984K             |    2.424G  |\n|    context_path.features.layer1.1           |    73.984K             |    2.424G  |\n|   context_path.features.layer2              |   0.526M               |   4.305G   |\n|    context_path.features.layer2.0           |    0.23M               |    1.885G  |\n|    context_path.features.layer2.1           |    0.295M              |    2.42G   |\n|   context_path.features.layer3              |   2.1M                 |   4.3G     |\n|    context_path.features.layer3.0           |    0.919M              |    1.882G  |\n|    context_path.features.layer3.1           |    1.181M              |    2.418G  |\n|   context_path.features.layer4              |   8.394M               |   4.298G   |\n|    context_path.features.layer4.0           |    3.673M              |    1.881G  |\n|    context_path.features.layer4.1           |    4.721M              |    2.417G  |\n|   context_path.features.fc                  |   0.513M               |            |\n|    context_path.features.fc.weight          |    (1000, 512)         |            |\n|    context_path.features.fc.bias            |    (1000,)             |            |\n|  attention_refinement_module1               |  66.304K               |  0.59M     |\n|   attention_refinement_module1.conv         |   65.792K              |   65.536K  |\n|    attention_refinement_module1.conv.weight |    (256, 256, 1, 1)    |            |\n|    attention_refinement_module1.conv.bias   |    (256,)              |            |\n|   attention_refinement_module1.bn           |   0.512K               |   0.512K   |\n|    attention_refinement_module1.bn.weight   |    (256,)              |            |\n|    attention_refinement_module1.bn.bias     |    (256,)              |            |\n|   attention_refinement_module1.avgpool      |                        |   0.524M   |\n|  attention_refinement_module2               |  0.264M                |  0.525M    |\n|   attention_refinement_module2.conv         |   0.263M               |   0.262M   |\n|    attention_refinement_module2.conv.weight |    (512, 512, 1, 1)    |            |\n|    attention_refinement_module2.conv.bias   |    (512,)              |            |\n|   attention_refinement_module2.bn           |   1.024K               |   1.024K   |\n|    attention_refinement_module2.bn.weight   |    (512,)              |            |\n|    attention_refinement_module2.bn.bias     |    (512,)              |            |\n|   attention_refinement_module2.avgpool      |                        |   0.262M   |\n|  supervision1                               |  4.883K                |            |\n|   supervision1.weight                       |   (19, 256, 1, 1)      |            |\n|   supervision1.bias                         |   (19,)                |            |\n|  supervision2                               |  9.747K                |            |\n|   supervision2.weight                       |   (19, 512, 1, 1)      |            |\n|   supervision2.bias                         |   (19,)                |            |\n|  feature_fusion_module                      |  0.176M                |  1.435G    |\n|   feature_fusion_module.convblock           |   0.175M               |   1.435G   |\n|    feature_fusion_module.convblock.conv1    |    0.175M              |    1.434G  |\n|    feature_fusion_module.convblock.bn       |    38                  |    0.311M  |\n|   feature_fusion_module.conv1               |   0.38K                |   0.361K   |\n|    feature_fusion_module.conv1.weight       |    (19, 19, 1, 1)      |            |\n|    feature_fusion_module.conv1.bias         |    (19,)               |            |\n|   feature_fusion_module.conv2               |   0.38K                |   0.361K   |\n|    feature_fusion_module.conv2.weight       |    (19, 19, 1, 1)      |            |\n|    feature_fusion_module.conv2.bias         |    (19,)               |            |\n|   feature_fusion_module.avgpool             |                        |   0.156M   |\n|  conv                                       |  0.38K                 |  0.189G    |\n|   conv.weight                               |   (19, 19, 1, 1)       |            |\n|   conv.bias                                 |   (19,)                |            |\n\n\nmIoU: 11.726%, Latency: 0.006, FPS: 188.091\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>FPS</td><td>▁</td></tr><tr><td>latency</td><td>▁</td></tr><tr><td>loss</td><td>█▃▂▁▁</td></tr><tr><td>mIOU</td><td>▄▆▇█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>FPS</td><td>188.09098</td></tr><tr><td>latency</td><td>0.00554</td></tr><tr><td>loss</td><td>0.55703</td></tr><tr><td>mIOU</td><td>0.11726</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">vivid-sweep-1</strong> at: <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/pfk8sgsh' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/pfk8sgsh</a><br/> View project at: <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240531_112607-pfk8sgsh/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8spx81u6 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.9\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240531_122207-8spx81u6</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/8spx81u6' target=\"_blank\">apricot-sweep-2</a></strong> to <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/8spx81u6' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/8spx81u6</a>"},"metadata":{}},{"name":"stdout","text":"----------------------------------\nLoss after 0 epochs: 0.623\nmIOU after 0 epochs: 0.186%\n----------------------------------\nLoss after 1 epochs: 0.452\nmIOU after 1 epochs: 0.226%\n----------------------------------\nLoss after 2 epochs: 0.399\nmIOU after 2 epochs: 0.245%\n----------------------------------\nLoss after 3 epochs: 0.389\nmIOU after 3 epochs: 0.249%\n----------------------------------\nLoss after 4 epochs: 0.380\nmIOU after 4 epochs: 0.253%\n| module                                      | #parameters or shape   | #flops     |\n|:--------------------------------------------|:-----------------------|:-----------|\n| model                                       | 12.582M                | 25.78G     |\n|  saptial_path                               |  0.371M                |  5.088G    |\n|   saptial_path.convblock1                   |   1.856K               |   0.243G   |\n|    saptial_path.convblock1.conv1            |    1.728K              |    0.226G  |\n|    saptial_path.convblock1.bn               |    0.128K              |    16.777M |\n|   saptial_path.convblock2                   |   73.984K              |   2.424G   |\n|    saptial_path.convblock2.conv1            |    73.728K             |    2.416G  |\n|    saptial_path.convblock2.bn               |    0.256K              |    8.389M  |\n|   saptial_path.convblock3                   |   0.295M               |   2.42G    |\n|    saptial_path.convblock3.conv1            |    0.295M              |    2.416G  |\n|    saptial_path.convblock3.bn               |    0.512K              |    4.194M  |\n|  context_path.features                      |  11.69M                |  19.002G   |\n|   context_path.features.conv1               |   9.408K               |   1.233G   |\n|    context_path.features.conv1.weight       |    (64, 3, 7, 7)       |            |\n|   context_path.features.bn1                 |   0.128K               |   16.777M  |\n|    context_path.features.bn1.weight         |    (64,)               |            |\n|    context_path.features.bn1.bias           |    (64,)               |            |\n|   context_path.features.layer1              |   0.148M               |   4.849G   |\n|    context_path.features.layer1.0           |    73.984K             |    2.424G  |\n|    context_path.features.layer1.1           |    73.984K             |    2.424G  |\n|   context_path.features.layer2              |   0.526M               |   4.305G   |\n|    context_path.features.layer2.0           |    0.23M               |    1.885G  |\n|    context_path.features.layer2.1           |    0.295M              |    2.42G   |\n|   context_path.features.layer3              |   2.1M                 |   4.3G     |\n|    context_path.features.layer3.0           |    0.919M              |    1.882G  |\n|    context_path.features.layer3.1           |    1.181M              |    2.418G  |\n|   context_path.features.layer4              |   8.394M               |   4.298G   |\n|    context_path.features.layer4.0           |    3.673M              |    1.881G  |\n|    context_path.features.layer4.1           |    4.721M              |    2.417G  |\n|   context_path.features.fc                  |   0.513M               |            |\n|    context_path.features.fc.weight          |    (1000, 512)         |            |\n|    context_path.features.fc.bias            |    (1000,)             |            |\n|  attention_refinement_module1               |  66.304K               |  0.59M     |\n|   attention_refinement_module1.conv         |   65.792K              |   65.536K  |\n|    attention_refinement_module1.conv.weight |    (256, 256, 1, 1)    |            |\n|    attention_refinement_module1.conv.bias   |    (256,)              |            |\n|   attention_refinement_module1.bn           |   0.512K               |   0.512K   |\n|    attention_refinement_module1.bn.weight   |    (256,)              |            |\n|    attention_refinement_module1.bn.bias     |    (256,)              |            |\n|   attention_refinement_module1.avgpool      |                        |   0.524M   |\n|  attention_refinement_module2               |  0.264M                |  0.525M    |\n|   attention_refinement_module2.conv         |   0.263M               |   0.262M   |\n|    attention_refinement_module2.conv.weight |    (512, 512, 1, 1)    |            |\n|    attention_refinement_module2.conv.bias   |    (512,)              |            |\n|   attention_refinement_module2.bn           |   1.024K               |   1.024K   |\n|    attention_refinement_module2.bn.weight   |    (512,)              |            |\n|    attention_refinement_module2.bn.bias     |    (512,)              |            |\n|   attention_refinement_module2.avgpool      |                        |   0.262M   |\n|  supervision1                               |  4.883K                |            |\n|   supervision1.weight                       |   (19, 256, 1, 1)      |            |\n|   supervision1.bias                         |   (19,)                |            |\n|  supervision2                               |  9.747K                |            |\n|   supervision2.weight                       |   (19, 512, 1, 1)      |            |\n|   supervision2.bias                         |   (19,)                |            |\n|  feature_fusion_module                      |  0.176M                |  1.435G    |\n|   feature_fusion_module.convblock           |   0.175M               |   1.435G   |\n|    feature_fusion_module.convblock.conv1    |    0.175M              |    1.434G  |\n|    feature_fusion_module.convblock.bn       |    38                  |    0.311M  |\n|   feature_fusion_module.conv1               |   0.38K                |   0.361K   |\n|    feature_fusion_module.conv1.weight       |    (19, 19, 1, 1)      |            |\n|    feature_fusion_module.conv1.bias         |    (19,)               |            |\n|   feature_fusion_module.conv2               |   0.38K                |   0.361K   |\n|    feature_fusion_module.conv2.weight       |    (19, 19, 1, 1)      |            |\n|    feature_fusion_module.conv2.bias         |    (19,)               |            |\n|   feature_fusion_module.avgpool             |                        |   0.156M   |\n|  conv                                       |  0.38K                 |  0.189G    |\n|   conv.weight                               |   (19, 19, 1, 1)       |            |\n|   conv.bias                                 |   (19,)                |            |\n\n\nmIoU: 12.399%, Latency: 0.006, FPS: 188.827\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>FPS</td><td>▁</td></tr><tr><td>latency</td><td>▁</td></tr><tr><td>loss</td><td>█▃▂▁▁</td></tr><tr><td>mIOU</td><td>▄▇██▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>FPS</td><td>188.82686</td></tr><tr><td>latency</td><td>0.00578</td></tr><tr><td>loss</td><td>0.38035</td></tr><tr><td>mIOU</td><td>0.12399</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">apricot-sweep-2</strong> at: <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/8spx81u6' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/8spx81u6</a><br/> View project at: <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240531_122207-8spx81u6/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: q3m1lmgz with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.9\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240531_131350-q3m1lmgz</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/q3m1lmgz' target=\"_blank\">decent-sweep-3</a></strong> to <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/q3m1lmgz' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/q3m1lmgz</a>"},"metadata":{}},{"name":"stdout","text":"----------------------------------\nLoss after 0 epochs: 1.631\nmIOU after 0 epochs: 0.102%\n----------------------------------\nLoss after 1 epochs: 1.073\nmIOU after 1 epochs: 0.146%\n----------------------------------\nLoss after 2 epochs: 0.901\nmIOU after 2 epochs: 0.160%\n----------------------------------\nLoss after 3 epochs: 0.809\nmIOU after 3 epochs: 0.167%\n----------------------------------\nLoss after 4 epochs: 0.747\nmIOU after 4 epochs: 0.173%\n| module                                      | #parameters or shape   | #flops     |\n|:--------------------------------------------|:-----------------------|:-----------|\n| model                                       | 12.582M                | 25.78G     |\n|  saptial_path                               |  0.371M                |  5.088G    |\n|   saptial_path.convblock1                   |   1.856K               |   0.243G   |\n|    saptial_path.convblock1.conv1            |    1.728K              |    0.226G  |\n|    saptial_path.convblock1.bn               |    0.128K              |    16.777M |\n|   saptial_path.convblock2                   |   73.984K              |   2.424G   |\n|    saptial_path.convblock2.conv1            |    73.728K             |    2.416G  |\n|    saptial_path.convblock2.bn               |    0.256K              |    8.389M  |\n|   saptial_path.convblock3                   |   0.295M               |   2.42G    |\n|    saptial_path.convblock3.conv1            |    0.295M              |    2.416G  |\n|    saptial_path.convblock3.bn               |    0.512K              |    4.194M  |\n|  context_path.features                      |  11.69M                |  19.002G   |\n|   context_path.features.conv1               |   9.408K               |   1.233G   |\n|    context_path.features.conv1.weight       |    (64, 3, 7, 7)       |            |\n|   context_path.features.bn1                 |   0.128K               |   16.777M  |\n|    context_path.features.bn1.weight         |    (64,)               |            |\n|    context_path.features.bn1.bias           |    (64,)               |            |\n|   context_path.features.layer1              |   0.148M               |   4.849G   |\n|    context_path.features.layer1.0           |    73.984K             |    2.424G  |\n|    context_path.features.layer1.1           |    73.984K             |    2.424G  |\n|   context_path.features.layer2              |   0.526M               |   4.305G   |\n|    context_path.features.layer2.0           |    0.23M               |    1.885G  |\n|    context_path.features.layer2.1           |    0.295M              |    2.42G   |\n|   context_path.features.layer3              |   2.1M                 |   4.3G     |\n|    context_path.features.layer3.0           |    0.919M              |    1.882G  |\n|    context_path.features.layer3.1           |    1.181M              |    2.418G  |\n|   context_path.features.layer4              |   8.394M               |   4.298G   |\n|    context_path.features.layer4.0           |    3.673M              |    1.881G  |\n|    context_path.features.layer4.1           |    4.721M              |    2.417G  |\n|   context_path.features.fc                  |   0.513M               |            |\n|    context_path.features.fc.weight          |    (1000, 512)         |            |\n|    context_path.features.fc.bias            |    (1000,)             |            |\n|  attention_refinement_module1               |  66.304K               |  0.59M     |\n|   attention_refinement_module1.conv         |   65.792K              |   65.536K  |\n|    attention_refinement_module1.conv.weight |    (256, 256, 1, 1)    |            |\n|    attention_refinement_module1.conv.bias   |    (256,)              |            |\n|   attention_refinement_module1.bn           |   0.512K               |   0.512K   |\n|    attention_refinement_module1.bn.weight   |    (256,)              |            |\n|    attention_refinement_module1.bn.bias     |    (256,)              |            |\n|   attention_refinement_module1.avgpool      |                        |   0.524M   |\n|  attention_refinement_module2               |  0.264M                |  0.525M    |\n|   attention_refinement_module2.conv         |   0.263M               |   0.262M   |\n|    attention_refinement_module2.conv.weight |    (512, 512, 1, 1)    |            |\n|    attention_refinement_module2.conv.bias   |    (512,)              |            |\n|   attention_refinement_module2.bn           |   1.024K               |   1.024K   |\n|    attention_refinement_module2.bn.weight   |    (512,)              |            |\n|    attention_refinement_module2.bn.bias     |    (512,)              |            |\n|   attention_refinement_module2.avgpool      |                        |   0.262M   |\n|  supervision1                               |  4.883K                |            |\n|   supervision1.weight                       |   (19, 256, 1, 1)      |            |\n|   supervision1.bias                         |   (19,)                |            |\n|  supervision2                               |  9.747K                |            |\n|   supervision2.weight                       |   (19, 512, 1, 1)      |            |\n|   supervision2.bias                         |   (19,)                |            |\n|  feature_fusion_module                      |  0.176M                |  1.435G    |\n|   feature_fusion_module.convblock           |   0.175M               |   1.435G   |\n|    feature_fusion_module.convblock.conv1    |    0.175M              |    1.434G  |\n|    feature_fusion_module.convblock.bn       |    38                  |    0.311M  |\n|   feature_fusion_module.conv1               |   0.38K                |   0.361K   |\n|    feature_fusion_module.conv1.weight       |    (19, 19, 1, 1)      |            |\n|    feature_fusion_module.conv1.bias         |    (19,)               |            |\n|   feature_fusion_module.conv2               |   0.38K                |   0.361K   |\n|    feature_fusion_module.conv2.weight       |    (19, 19, 1, 1)      |            |\n|    feature_fusion_module.conv2.bias         |    (19,)               |            |\n|   feature_fusion_module.avgpool             |                        |   0.156M   |\n|  conv                                       |  0.38K                 |  0.189G    |\n|   conv.weight                               |   (19, 19, 1, 1)       |            |\n|   conv.bias                                 |   (19,)                |            |\n\n\nmIoU: 12.197%, Latency: 0.006, FPS: 178.893\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>FPS</td><td>▁</td></tr><tr><td>latency</td><td>▁</td></tr><tr><td>loss</td><td>█▄▂▁▁</td></tr><tr><td>mIOU</td><td>▁▆▇█▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>FPS</td><td>178.89297</td></tr><tr><td>latency</td><td>0.00607</td></tr><tr><td>loss</td><td>0.74686</td></tr><tr><td>mIOU</td><td>0.12197</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">decent-sweep-3</strong> at: <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/q3m1lmgz' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/q3m1lmgz</a><br/> View project at: <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240531_131350-q3m1lmgz/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: o2vwnzsp with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.9\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240531_140530-o2vwnzsp</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/o2vwnzsp' target=\"_blank\">legendary-sweep-4</a></strong> to <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/o2vwnzsp' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/o2vwnzsp</a>"},"metadata":{}},{"name":"stdout","text":"----------------------------------\nLoss after 0 epochs: 0.666\nmIOU after 0 epochs: 0.177%\n----------------------------------\nLoss after 1 epochs: 0.463\nmIOU after 1 epochs: 0.224%\n----------------------------------\nLoss after 2 epochs: 0.424\nmIOU after 2 epochs: 0.238%\n----------------------------------\nLoss after 3 epochs: 0.383\nmIOU after 3 epochs: 0.253%\n----------------------------------\nLoss after 4 epochs: 0.375\nmIOU after 4 epochs: 0.256%\n| module                                      | #parameters or shape   | #flops     |\n|:--------------------------------------------|:-----------------------|:-----------|\n| model                                       | 12.582M                | 25.78G     |\n|  saptial_path                               |  0.371M                |  5.088G    |\n|   saptial_path.convblock1                   |   1.856K               |   0.243G   |\n|    saptial_path.convblock1.conv1            |    1.728K              |    0.226G  |\n|    saptial_path.convblock1.bn               |    0.128K              |    16.777M |\n|   saptial_path.convblock2                   |   73.984K              |   2.424G   |\n|    saptial_path.convblock2.conv1            |    73.728K             |    2.416G  |\n|    saptial_path.convblock2.bn               |    0.256K              |    8.389M  |\n|   saptial_path.convblock3                   |   0.295M               |   2.42G    |\n|    saptial_path.convblock3.conv1            |    0.295M              |    2.416G  |\n|    saptial_path.convblock3.bn               |    0.512K              |    4.194M  |\n|  context_path.features                      |  11.69M                |  19.002G   |\n|   context_path.features.conv1               |   9.408K               |   1.233G   |\n|    context_path.features.conv1.weight       |    (64, 3, 7, 7)       |            |\n|   context_path.features.bn1                 |   0.128K               |   16.777M  |\n|    context_path.features.bn1.weight         |    (64,)               |            |\n|    context_path.features.bn1.bias           |    (64,)               |            |\n|   context_path.features.layer1              |   0.148M               |   4.849G   |\n|    context_path.features.layer1.0           |    73.984K             |    2.424G  |\n|    context_path.features.layer1.1           |    73.984K             |    2.424G  |\n|   context_path.features.layer2              |   0.526M               |   4.305G   |\n|    context_path.features.layer2.0           |    0.23M               |    1.885G  |\n|    context_path.features.layer2.1           |    0.295M              |    2.42G   |\n|   context_path.features.layer3              |   2.1M                 |   4.3G     |\n|    context_path.features.layer3.0           |    0.919M              |    1.882G  |\n|    context_path.features.layer3.1           |    1.181M              |    2.418G  |\n|   context_path.features.layer4              |   8.394M               |   4.298G   |\n|    context_path.features.layer4.0           |    3.673M              |    1.881G  |\n|    context_path.features.layer4.1           |    4.721M              |    2.417G  |\n|   context_path.features.fc                  |   0.513M               |            |\n|    context_path.features.fc.weight          |    (1000, 512)         |            |\n|    context_path.features.fc.bias            |    (1000,)             |            |\n|  attention_refinement_module1               |  66.304K               |  0.59M     |\n|   attention_refinement_module1.conv         |   65.792K              |   65.536K  |\n|    attention_refinement_module1.conv.weight |    (256, 256, 1, 1)    |            |\n|    attention_refinement_module1.conv.bias   |    (256,)              |            |\n|   attention_refinement_module1.bn           |   0.512K               |   0.512K   |\n|    attention_refinement_module1.bn.weight   |    (256,)              |            |\n|    attention_refinement_module1.bn.bias     |    (256,)              |            |\n|   attention_refinement_module1.avgpool      |                        |   0.524M   |\n|  attention_refinement_module2               |  0.264M                |  0.525M    |\n|   attention_refinement_module2.conv         |   0.263M               |   0.262M   |\n|    attention_refinement_module2.conv.weight |    (512, 512, 1, 1)    |            |\n|    attention_refinement_module2.conv.bias   |    (512,)              |            |\n|   attention_refinement_module2.bn           |   1.024K               |   1.024K   |\n|    attention_refinement_module2.bn.weight   |    (512,)              |            |\n|    attention_refinement_module2.bn.bias     |    (512,)              |            |\n|   attention_refinement_module2.avgpool      |                        |   0.262M   |\n|  supervision1                               |  4.883K                |            |\n|   supervision1.weight                       |   (19, 256, 1, 1)      |            |\n|   supervision1.bias                         |   (19,)                |            |\n|  supervision2                               |  9.747K                |            |\n|   supervision2.weight                       |   (19, 512, 1, 1)      |            |\n|   supervision2.bias                         |   (19,)                |            |\n|  feature_fusion_module                      |  0.176M                |  1.435G    |\n|   feature_fusion_module.convblock           |   0.175M               |   1.435G   |\n|    feature_fusion_module.convblock.conv1    |    0.175M              |    1.434G  |\n|    feature_fusion_module.convblock.bn       |    38                  |    0.311M  |\n|   feature_fusion_module.conv1               |   0.38K                |   0.361K   |\n|    feature_fusion_module.conv1.weight       |    (19, 19, 1, 1)      |            |\n|    feature_fusion_module.conv1.bias         |    (19,)               |            |\n|   feature_fusion_module.conv2               |   0.38K                |   0.361K   |\n|    feature_fusion_module.conv2.weight       |    (19, 19, 1, 1)      |            |\n|    feature_fusion_module.conv2.bias         |    (19,)               |            |\n|   feature_fusion_module.avgpool             |                        |   0.156M   |\n|  conv                                       |  0.38K                 |  0.189G    |\n|   conv.weight                               |   (19, 19, 1, 1)       |            |\n|   conv.bias                                 |   (19,)                |            |\n\n\nmIoU: 11.994%, Latency: 0.006, FPS: 190.586\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>FPS</td><td>▁</td></tr><tr><td>latency</td><td>▁</td></tr><tr><td>loss</td><td>█▃▂▁▁</td></tr><tr><td>mIOU</td><td>▄▇▇█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>FPS</td><td>190.58558</td></tr><tr><td>latency</td><td>0.0057</td></tr><tr><td>loss</td><td>0.37534</td></tr><tr><td>mIOU</td><td>0.11994</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">legendary-sweep-4</strong> at: <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/o2vwnzsp' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/o2vwnzsp</a><br/> View project at: <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240531_140530-o2vwnzsp/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: n5x1b7qm with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.9\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240531_145708-n5x1b7qm</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/n5x1b7qm' target=\"_blank\">dainty-sweep-5</a></strong> to <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/n5x1b7qm' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/n5x1b7qm</a>"},"metadata":{}},{"name":"stdout","text":"----------------------------------\nLoss after 0 epochs: 0.770\nmIOU after 0 epochs: 0.180%\n----------------------------------\nLoss after 1 epochs: 0.500\nmIOU after 1 epochs: 0.213%\n----------------------------------\nLoss after 2 epochs: 0.443\nmIOU after 2 epochs: 0.228%\n----------------------------------\nLoss after 3 epochs: 0.409\nmIOU after 3 epochs: 0.239%\n----------------------------------\nLoss after 4 epochs: 0.384\nmIOU after 4 epochs: 0.247%\n| module                                      | #parameters or shape   | #flops     |\n|:--------------------------------------------|:-----------------------|:-----------|\n| model                                       | 12.582M                | 25.78G     |\n|  saptial_path                               |  0.371M                |  5.088G    |\n|   saptial_path.convblock1                   |   1.856K               |   0.243G   |\n|    saptial_path.convblock1.conv1            |    1.728K              |    0.226G  |\n|    saptial_path.convblock1.bn               |    0.128K              |    16.777M |\n|   saptial_path.convblock2                   |   73.984K              |   2.424G   |\n|    saptial_path.convblock2.conv1            |    73.728K             |    2.416G  |\n|    saptial_path.convblock2.bn               |    0.256K              |    8.389M  |\n|   saptial_path.convblock3                   |   0.295M               |   2.42G    |\n|    saptial_path.convblock3.conv1            |    0.295M              |    2.416G  |\n|    saptial_path.convblock3.bn               |    0.512K              |    4.194M  |\n|  context_path.features                      |  11.69M                |  19.002G   |\n|   context_path.features.conv1               |   9.408K               |   1.233G   |\n|    context_path.features.conv1.weight       |    (64, 3, 7, 7)       |            |\n|   context_path.features.bn1                 |   0.128K               |   16.777M  |\n|    context_path.features.bn1.weight         |    (64,)               |            |\n|    context_path.features.bn1.bias           |    (64,)               |            |\n|   context_path.features.layer1              |   0.148M               |   4.849G   |\n|    context_path.features.layer1.0           |    73.984K             |    2.424G  |\n|    context_path.features.layer1.1           |    73.984K             |    2.424G  |\n|   context_path.features.layer2              |   0.526M               |   4.305G   |\n|    context_path.features.layer2.0           |    0.23M               |    1.885G  |\n|    context_path.features.layer2.1           |    0.295M              |    2.42G   |\n|   context_path.features.layer3              |   2.1M                 |   4.3G     |\n|    context_path.features.layer3.0           |    0.919M              |    1.882G  |\n|    context_path.features.layer3.1           |    1.181M              |    2.418G  |\n|   context_path.features.layer4              |   8.394M               |   4.298G   |\n|    context_path.features.layer4.0           |    3.673M              |    1.881G  |\n|    context_path.features.layer4.1           |    4.721M              |    2.417G  |\n|   context_path.features.fc                  |   0.513M               |            |\n|    context_path.features.fc.weight          |    (1000, 512)         |            |\n|    context_path.features.fc.bias            |    (1000,)             |            |\n|  attention_refinement_module1               |  66.304K               |  0.59M     |\n|   attention_refinement_module1.conv         |   65.792K              |   65.536K  |\n|    attention_refinement_module1.conv.weight |    (256, 256, 1, 1)    |            |\n|    attention_refinement_module1.conv.bias   |    (256,)              |            |\n|   attention_refinement_module1.bn           |   0.512K               |   0.512K   |\n|    attention_refinement_module1.bn.weight   |    (256,)              |            |\n|    attention_refinement_module1.bn.bias     |    (256,)              |            |\n|   attention_refinement_module1.avgpool      |                        |   0.524M   |\n|  attention_refinement_module2               |  0.264M                |  0.525M    |\n|   attention_refinement_module2.conv         |   0.263M               |   0.262M   |\n|    attention_refinement_module2.conv.weight |    (512, 512, 1, 1)    |            |\n|    attention_refinement_module2.conv.bias   |    (512,)              |            |\n|   attention_refinement_module2.bn           |   1.024K               |   1.024K   |\n|    attention_refinement_module2.bn.weight   |    (512,)              |            |\n|    attention_refinement_module2.bn.bias     |    (512,)              |            |\n|   attention_refinement_module2.avgpool      |                        |   0.262M   |\n|  supervision1                               |  4.883K                |            |\n|   supervision1.weight                       |   (19, 256, 1, 1)      |            |\n|   supervision1.bias                         |   (19,)                |            |\n|  supervision2                               |  9.747K                |            |\n|   supervision2.weight                       |   (19, 512, 1, 1)      |            |\n|   supervision2.bias                         |   (19,)                |            |\n|  feature_fusion_module                      |  0.176M                |  1.435G    |\n|   feature_fusion_module.convblock           |   0.175M               |   1.435G   |\n|    feature_fusion_module.convblock.conv1    |    0.175M              |    1.434G  |\n|    feature_fusion_module.convblock.bn       |    38                  |    0.311M  |\n|   feature_fusion_module.conv1               |   0.38K                |   0.361K   |\n|    feature_fusion_module.conv1.weight       |    (19, 19, 1, 1)      |            |\n|    feature_fusion_module.conv1.bias         |    (19,)               |            |\n|   feature_fusion_module.conv2               |   0.38K                |   0.361K   |\n|    feature_fusion_module.conv2.weight       |    (19, 19, 1, 1)      |            |\n|    feature_fusion_module.conv2.bias         |    (19,)               |            |\n|   feature_fusion_module.avgpool             |                        |   0.156M   |\n|  conv                                       |  0.38K                 |  0.189G    |\n|   conv.weight                               |   (19, 19, 1, 1)       |            |\n|   conv.bias                                 |   (19,)                |            |\n\n\nmIoU: 12.415%, Latency: 0.006, FPS: 189.313\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.024 MB of 0.024 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>FPS</td><td>▁</td></tr><tr><td>latency</td><td>▁</td></tr><tr><td>loss</td><td>█▃▂▁▁</td></tr><tr><td>mIOU</td><td>▄▆▇█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>FPS</td><td>189.31284</td></tr><tr><td>latency</td><td>0.0055</td></tr><tr><td>loss</td><td>0.3841</td></tr><tr><td>mIOU</td><td>0.12415</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">dainty-sweep-5</strong> at: <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/n5x1b7qm' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/n5x1b7qm</a><br/> View project at: <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240531_145708-n5x1b7qm/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1n78nh02 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.9\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240531_154928-1n78nh02</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/1n78nh02' target=\"_blank\">dashing-sweep-6</a></strong> to <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/1n78nh02' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/1n78nh02</a>"},"metadata":{}},{"name":"stdout","text":"----------------------------------\nLoss after 0 epochs: 1.264\nmIOU after 0 epochs: 0.131%\n----------------------------------\nLoss after 1 epochs: 0.809\nmIOU after 1 epochs: 0.166%\n----------------------------------\nLoss after 2 epochs: 0.703\nmIOU after 2 epochs: 0.179%\n----------------------------------\nLoss after 3 epochs: 0.647\nmIOU after 3 epochs: 0.186%\n----------------------------------\nLoss after 4 epochs: 0.609\nmIOU after 4 epochs: 0.192%\n| module                                      | #parameters or shape   | #flops     |\n|:--------------------------------------------|:-----------------------|:-----------|\n| model                                       | 12.582M                | 25.78G     |\n|  saptial_path                               |  0.371M                |  5.088G    |\n|   saptial_path.convblock1                   |   1.856K               |   0.243G   |\n|    saptial_path.convblock1.conv1            |    1.728K              |    0.226G  |\n|    saptial_path.convblock1.bn               |    0.128K              |    16.777M |\n|   saptial_path.convblock2                   |   73.984K              |   2.424G   |\n|    saptial_path.convblock2.conv1            |    73.728K             |    2.416G  |\n|    saptial_path.convblock2.bn               |    0.256K              |    8.389M  |\n|   saptial_path.convblock3                   |   0.295M               |   2.42G    |\n|    saptial_path.convblock3.conv1            |    0.295M              |    2.416G  |\n|    saptial_path.convblock3.bn               |    0.512K              |    4.194M  |\n|  context_path.features                      |  11.69M                |  19.002G   |\n|   context_path.features.conv1               |   9.408K               |   1.233G   |\n|    context_path.features.conv1.weight       |    (64, 3, 7, 7)       |            |\n|   context_path.features.bn1                 |   0.128K               |   16.777M  |\n|    context_path.features.bn1.weight         |    (64,)               |            |\n|    context_path.features.bn1.bias           |    (64,)               |            |\n|   context_path.features.layer1              |   0.148M               |   4.849G   |\n|    context_path.features.layer1.0           |    73.984K             |    2.424G  |\n|    context_path.features.layer1.1           |    73.984K             |    2.424G  |\n|   context_path.features.layer2              |   0.526M               |   4.305G   |\n|    context_path.features.layer2.0           |    0.23M               |    1.885G  |\n|    context_path.features.layer2.1           |    0.295M              |    2.42G   |\n|   context_path.features.layer3              |   2.1M                 |   4.3G     |\n|    context_path.features.layer3.0           |    0.919M              |    1.882G  |\n|    context_path.features.layer3.1           |    1.181M              |    2.418G  |\n|   context_path.features.layer4              |   8.394M               |   4.298G   |\n|    context_path.features.layer4.0           |    3.673M              |    1.881G  |\n|    context_path.features.layer4.1           |    4.721M              |    2.417G  |\n|   context_path.features.fc                  |   0.513M               |            |\n|    context_path.features.fc.weight          |    (1000, 512)         |            |\n|    context_path.features.fc.bias            |    (1000,)             |            |\n|  attention_refinement_module1               |  66.304K               |  0.59M     |\n|   attention_refinement_module1.conv         |   65.792K              |   65.536K  |\n|    attention_refinement_module1.conv.weight |    (256, 256, 1, 1)    |            |\n|    attention_refinement_module1.conv.bias   |    (256,)              |            |\n|   attention_refinement_module1.bn           |   0.512K               |   0.512K   |\n|    attention_refinement_module1.bn.weight   |    (256,)              |            |\n|    attention_refinement_module1.bn.bias     |    (256,)              |            |\n|   attention_refinement_module1.avgpool      |                        |   0.524M   |\n|  attention_refinement_module2               |  0.264M                |  0.525M    |\n|   attention_refinement_module2.conv         |   0.263M               |   0.262M   |\n|    attention_refinement_module2.conv.weight |    (512, 512, 1, 1)    |            |\n|    attention_refinement_module2.conv.bias   |    (512,)              |            |\n|   attention_refinement_module2.bn           |   1.024K               |   1.024K   |\n|    attention_refinement_module2.bn.weight   |    (512,)              |            |\n|    attention_refinement_module2.bn.bias     |    (512,)              |            |\n|   attention_refinement_module2.avgpool      |                        |   0.262M   |\n|  supervision1                               |  4.883K                |            |\n|   supervision1.weight                       |   (19, 256, 1, 1)      |            |\n|   supervision1.bias                         |   (19,)                |            |\n|  supervision2                               |  9.747K                |            |\n|   supervision2.weight                       |   (19, 512, 1, 1)      |            |\n|   supervision2.bias                         |   (19,)                |            |\n|  feature_fusion_module                      |  0.176M                |  1.435G    |\n|   feature_fusion_module.convblock           |   0.175M               |   1.435G   |\n|    feature_fusion_module.convblock.conv1    |    0.175M              |    1.434G  |\n|    feature_fusion_module.convblock.bn       |    38                  |    0.311M  |\n|   feature_fusion_module.conv1               |   0.38K                |   0.361K   |\n|    feature_fusion_module.conv1.weight       |    (19, 19, 1, 1)      |            |\n|    feature_fusion_module.conv1.bias         |    (19,)               |            |\n|   feature_fusion_module.conv2               |   0.38K                |   0.361K   |\n|    feature_fusion_module.conv2.weight       |    (19, 19, 1, 1)      |            |\n|    feature_fusion_module.conv2.bias         |    (19,)               |            |\n|   feature_fusion_module.avgpool             |                        |   0.156M   |\n|  conv                                       |  0.38K                 |  0.189G    |\n|   conv.weight                               |   (19, 19, 1, 1)       |            |\n|   conv.bias                                 |   (19,)                |            |\n\n\nmIoU: 12.242%, Latency: 0.006, FPS: 177.949\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>FPS</td><td>▁</td></tr><tr><td>latency</td><td>▁</td></tr><tr><td>loss</td><td>█▃▂▁▁</td></tr><tr><td>mIOU</td><td>▂▆▇█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>FPS</td><td>177.94932</td></tr><tr><td>latency</td><td>0.00584</td></tr><tr><td>loss</td><td>0.60886</td></tr><tr><td>mIOU</td><td>0.12242</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">dashing-sweep-6</strong> at: <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/1n78nh02' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/1n78nh02</a><br/> View project at: <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240531_154928-1n78nh02/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0zda6wle with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.9\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240531_164156-0zda6wle</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/0zda6wle' target=\"_blank\">major-sweep-7</a></strong> to <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/sweeps/g9ioen8f</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/0zda6wle' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/0zda6wle</a>"},"metadata":{}},{"name":"stdout","text":"----------------------------------\nLoss after 0 epochs: 0.689\nmIOU after 0 epochs: 0.186%\n----------------------------------\nLoss after 1 epochs: 0.496\nmIOU after 1 epochs: 0.223%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 8 - Build final model","metadata":{}},{"cell_type":"code","source":"# best configuration (TO CONFIGURE)\nconfig = dict(\n    epochs=50,\n    batch_size=2,\n    learning_rate=0.001,\n    momentum=0.9,\n    weight_decay=5e-4,\n    architecture=\"BiSeNet\",\n    checkpoint_dir=\"/kaggle/working/checkpoints\")","metadata":{"execution":{"iopub.status.busy":"2024-05-31T10:12:00.041724Z","iopub.execute_input":"2024-05-31T10:12:00.042084Z","iopub.status.idle":"2024-05-31T10:12:00.046695Z","shell.execute_reply.started":"2024-05-31T10:12:00.042057Z","shell.execute_reply":"2024-05-31T10:12:00.045779Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    print(\"Building the model with the best configuration\")\n    # Build, train and analyze the model with the pipeline\n    model = model_pipeline(config)\nelse:\n    print(\"CUDA is Not available\")","metadata":{"execution":{"iopub.status.busy":"2024-05-31T10:31:17.793087Z","iopub.execute_input":"2024-05-31T10:31:17.793465Z","iopub.status.idle":"2024-05-31T10:58:54.505367Z","shell.execute_reply.started":"2024-05-31T10:31:17.793433Z","shell.execute_reply":"2024-05-31T10:58:54.504036Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Building the model with the best configuration\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240531_103117-zvq27kgw</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/zvq27kgw' target=\"_blank\">silvery-disco-9</a></strong> to <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/zvq27kgw' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/zvq27kgw</a>"},"metadata":{}},{"name":"stdout","text":"Checkpoint found. Resuming from epoch 1.\n----------------------------------\nLoss after 1 epochs: 0.836\nmIOU after 1 epochs: 0.157%\nCheckpoint saved in /kaggle/working/checkpoints/checkpoint.pth | Epoch: 1\n----------------------------------\nLoss after 2 epochs: 0.761\nmIOU after 2 epochs: 0.166%\nCheckpoint saved in /kaggle/working/checkpoints/checkpoint.pth | Epoch: 2\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_33/2884409676.py\", line 12, in model_pipeline\n    train(model, train_loader, criterion, optimizer, config, start_epoch)\n  File \"/tmp/ipykernel_33/1525349719.py\", line 8, in train\n    for _, (inputs, targets) in enumerate(dataloader):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n    data = self._next_data()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 674, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_33/3070032509.py\", line 40, in __getitem__\n    image = Image.open(img_path).convert('RGB')\n  File \"/opt/conda/lib/python3.10/site-packages/PIL/Image.py\", line 933, in convert\n    self.load()\n  File \"/opt/conda/lib/python3.10/site-packages/PIL/ImageFile.py\", line 269, in load\n    n, err_code = decoder.decode(b)\nKeyboardInterrupt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▁</td></tr><tr><td>mIOU</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.7613</td></tr><tr><td>mIOU</td><td>0.16598</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">silvery-disco-9</strong> at: <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/zvq27kgw' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/zvq27kgw</a><br/> View project at: <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240531_103117-zvq27kgw/logs</code>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilding the model with the best configuration\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Build, train and analyze the model with the pipeline\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA is Not available\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[7], line 12\u001b[0m, in \u001b[0;36mmodel_pipeline\u001b[0;34m(hyperparameters)\u001b[0m\n\u001b[1;32m      9\u001b[0m model, train_loader, val_loader, criterion, optimizer, start_epoch \u001b[38;5;241m=\u001b[39m make(config)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# and use them to train the model\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# and test its final performance\u001b[39;00m\n\u001b[1;32m     15\u001b[0m val(model, val_loader)\n","Cell \u001b[0;32mIn[15], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, criterion, optimizer, config, start_epoch)\u001b[0m\n\u001b[1;32m      5\u001b[0m total_mIOU \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m total_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, (inputs, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     10\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mcuda(), id_processing(targets)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     12\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[12], line 40\u001b[0m, in \u001b[0;36mGTA5.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     37\u001b[0m label_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_dir, label_name)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Load image and label\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m label \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(label_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_transform:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:933\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[1;32m    887\u001b[0m ):\n\u001b[1;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-05-21T18:43:13.282783Z","iopub.status.idle":"2024-05-21T18:43:13.283135Z","shell.execute_reply.started":"2024-05-21T18:43:13.282947Z","shell.execute_reply":"2024-05-21T18:43:13.282961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9 - Save model weights","metadata":{}},{"cell_type":"code","source":"torch.save(model.state_dict(), 'model_weights.pth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10 - Model Application","metadata":{}},{"cell_type":"code","source":"# Lad the model \nmodel =  pass # we do not specify ``weights``, i.e. create untrained model\nmodel.load_state_dict(torch.load('model_weights.pth'))\nmodel.eval()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pick une image\n# use model and predict the mask\n# map each class to a colored value\n# plot the segmented image","metadata":{"execution":{"iopub.status.busy":"2024-05-22T09:28:20.040095Z","iopub.execute_input":"2024-05-22T09:28:20.040795Z","iopub.status.idle":"2024-05-22T09:28:20.045272Z","shell.execute_reply.started":"2024-05-22T09:28:20.040762Z","shell.execute_reply":"2024-05-22T09:28:20.044240Z"},"trusted":true},"execution_count":1,"outputs":[]}]}