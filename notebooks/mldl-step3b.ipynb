{"cells":[{"cell_type":"markdown","metadata":{"id":"OAZEiffSoALO"},"source":["# Data augmentations to reduce the domain shift \n","A naive solution to improve the generalization capability of the segmentation network trained on the synthetic domain consists in the usage of data augmentations during training. Through them, we i) virtually expand the dataset size and ii) modify the visual appearance of source (synthetic) images in order to make them more similar to the target (real) ones. \n","Specifically, we repeat the previous experiment, introducing data augmentations at training time (e.g. horizontal flip, Gaussian Blur, Multiply, ecc.). The decision of what kind of algorithm is left to the student. Set the probability to perform augmentation to 0.5.\n","\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T14:16:59.586088Z","iopub.status.busy":"2024-05-29T14:16:59.585793Z","iopub.status.idle":"2024-05-29T14:17:18.305366Z","shell.execute_reply":"2024-05-29T14:17:18.304454Z","shell.execute_reply.started":"2024-05-29T14:16:59.586061Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting fvcore\n","  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m791.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fvcore) (1.26.4)\n","Collecting yacs>=0.1.6 (from fvcore)\n","  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from fvcore) (6.0.1)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from fvcore) (4.66.1)\n","Requirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.10/site-packages (from fvcore) (2.4.0)\n","Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from fvcore) (9.5.0)\n","Requirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from fvcore) (0.9.0)\n","Collecting iopath>=0.1.7 (from fvcore)\n","  Downloading iopath-0.1.10.tar.gz (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from iopath>=0.1.7->fvcore) (4.9.0)\n","Collecting portalocker (from iopath>=0.1.7->fvcore)\n","  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n","Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n","Building wheels for collected packages: fvcore, iopath\n","  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=15caf3df2f3581e8c11eb7cc4f5c3a40a6c9f4025492c53bd2308efcee751ab2\n","  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n","  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=a183fed79cd00524a9e24941eef7f049c0f88991990695b3cf1bff9cd9dafb6f\n","  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n","Successfully built fvcore iopath\n","Installing collected packages: yacs, portalocker, iopath, fvcore\n","Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-2.8.2 yacs-0.1.8\n"]}],"source":["!pip install -U fvcore"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T14:17:18.307346Z","iopub.status.busy":"2024-05-29T14:17:18.307081Z","iopub.status.idle":"2024-05-29T14:17:30.621023Z","shell.execute_reply":"2024-05-29T14:17:30.619968Z","shell.execute_reply.started":"2024-05-29T14:17:18.307323Z"},"trusted":true},"outputs":[],"source":["# WANDB\n","!pip install -q wandb"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T14:17:30.622977Z","iopub.status.busy":"2024-05-29T14:17:30.622695Z","iopub.status.idle":"2024-05-29T14:17:31.579530Z","shell.execute_reply":"2024-05-29T14:17:31.578558Z","shell.execute_reply.started":"2024-05-29T14:17:30.622952Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["rm: cannot remove 'checkpoints/': No such file or directory\n"]}],"source":["# If you run the model for the first time remove all the previus checkpoints\n","! rm -r checkpoints/"]},{"cell_type":"markdown","metadata":{"id":"cNa1Un4uoZ6M"},"source":["# 0 - Import libraries"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T14:17:31.582618Z","iopub.status.busy":"2024-05-29T14:17:31.582160Z","iopub.status.idle":"2024-05-29T14:17:37.995137Z","shell.execute_reply":"2024-05-29T14:17:37.994166Z","shell.execute_reply.started":"2024-05-29T14:17:31.582582Z"},"executionInfo":{"elapsed":13275,"status":"ok","timestamp":1715695760300,"user":{"displayName":"Tommaso Mazzarini","userId":"16046308562458308219"},"user_tz":-120},"id":"mYTc9GfSouYh","trusted":true},"outputs":[],"source":["import wandb\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","import torch.optim as optim\n","\n","import os\n","import zipfile\n","import numpy as np\n","import time\n","from PIL import Image\n","\n","from fvcore.nn import FlopCountAnalysis, flop_count_table\n","\n","import warnings\n","warnings.filterwarnings(action='ignore')"]},{"cell_type":"markdown","metadata":{},"source":["# 1 - Start WanDB"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T14:17:37.996697Z","iopub.status.busy":"2024-05-29T14:17:37.996280Z","iopub.status.idle":"2024-05-29T14:17:57.360837Z","shell.execute_reply":"2024-05-29T14:17:57.359967Z","shell.execute_reply.started":"2024-05-29T14:17:37.996656Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["wandb.login()"]},{"cell_type":"markdown","metadata":{},"source":["# 2 - Model Pipeline"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T14:17:59.939235Z","iopub.status.busy":"2024-05-29T14:17:59.938101Z","iopub.status.idle":"2024-05-29T14:17:59.944837Z","shell.execute_reply":"2024-05-29T14:17:59.943877Z","shell.execute_reply.started":"2024-05-29T14:17:59.939201Z"},"trusted":true},"outputs":[],"source":["def model_pipeline(hyperparameters=None):\n","\n","    # tell wandb to get started\n","    with wandb.init(project=\"MLDL-step3a\", config=hyperparameters):\n","        # access all HPs through wandb.config, so logging matches execution!\n","        config = wandb.config\n","\n","        # make the model, data, and optimization problem\n","        model, train_loader, val_loader, criterion, optimizer, start_epoch = make(config)\n","        \n","        # and use them to train the model\n","        train(model, train_loader, criterion, optimizer, config, start_epoch)\n","\n","        # and test its final performance\n","        val(model, val_loader)\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"sKK0P3Aloi9U"},"source":["# 3 - Datasets"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T14:18:04.018217Z","iopub.status.busy":"2024-05-29T14:18:04.017350Z","iopub.status.idle":"2024-05-29T14:18:04.025051Z","shell.execute_reply":"2024-05-29T14:18:04.023992Z","shell.execute_reply.started":"2024-05-29T14:18:04.018182Z"},"trusted":true},"outputs":[],"source":["def make(config):\n","    # Make the data\n","    train, test = get_data(train=True), get_data(train=False)\n","    train_loader = make_loader(train, batch_size=config.batch_size,train=True)\n","    test_loader = make_loader(test, batch_size=config.batch_size,train=False)\n","\n","    # Make the model (BiSeNet with ResNet-18 backbone)\n","    model = build_model(model_type='BiSeNet').cuda()\n","\n","    # Make the loss and optimizer\n","    optimizer = optim.SGD(model.parameters(), \n","                          lr=config.learning_rate, \n","                          momentum=config.momentum, \n","                          weight_decay=config.weight_decay)\n","    \n","    criterion = torch.nn.CrossEntropyLoss(ignore_index=255)\n","    \n","    # Load the last checkpoint\n","    start_epoch = load_checkpoint(config, model, optimizer)\n","    \n","    return model, train_loader, test_loader, criterion, optimizer, start_epoch"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T14:18:20.411064Z","iopub.status.busy":"2024-05-29T14:18:20.410708Z","iopub.status.idle":"2024-05-29T14:18:20.426183Z","shell.execute_reply":"2024-05-29T14:18:20.425209Z","shell.execute_reply.started":"2024-05-29T14:18:20.411035Z"},"trusted":true},"outputs":[],"source":["import torchvision.transforms as transforms\n","\n","# Define augmentation transformations for training\n","augmentation_transforms_train = [\n","    transforms.RandomHorizontalFlip(p=0.5),  # Horizontal flip with probability 0.5\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color jittering\n","    transforms.RandomResizedCrop(size=(512, 1024), scale=(0.5, 1.0), ratio=(0.75, 1.333))  # Random crop and resize\n","]\n","\n","# Define base transformations for validation/test\n","base_transforms_city = [\n","    transforms.Resize((512, 1024)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","]\n","\n","base_transforms_gta = [\n","    transforms.Resize((720, 1280)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","]\n","\n","# Define transforms for preprocessing\n","image_transform_cityescapes = {\n","    'train': transforms.Compose([\n","        transforms.Resize((512, 1024)),\n","        transforms.RandomApply(augmentation_transforms_train, p=0.5),  # Apply augmentation with probability 0.5\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose(base_transforms_city)\n","}\n","\n","image_transform_gta = {\n","    'train': transforms.Compose(base_transforms_gta),  # Use base transforms for training GTA5\n","    'val': transforms.Compose(base_transforms_gta)\n","}\n","\n","label_transform_cityescapes = transforms.Compose([\n","    transforms.Resize((512, 1024))\n","])\n","\n","label_transform_gta = transforms.Compose([\n","    transforms.Resize((720, 1280))\n","])\n","\n","# GTA5 for train and CityScapes for test\n","cityscapes_dir = '/kaggle/input/cityscapes/Cityscapes/Cityspaces'\n","gta_dir = '/kaggle/input/gta5-dataset-with-masks'\n","\n","def get_data(train=True):\n","    if train:\n","        # train dataset\n","        dataset = GTA5(root_dir=gta_dir, image_transform=image_transform_gta['train'], label_transform=label_transform_gta)\n","    else:\n","        # test dataset\n","        dataset = CityScapes(root_dir=cityscapes_dir, split='val', image_transform=image_transform_cityescapes['val'], label_transform=label_transform_cityescapes)\n","    \n","    return dataset\n","\n","def make_loader(dataset, batch_size = 8, train=True):\n","    if train == True:\n","        # train dataloader\n","        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n","    else:\n","        # test dataloader\n","        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False,drop_last=True)\n","    \n","    return dataloader\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T14:18:20.917358Z","iopub.status.busy":"2024-05-29T14:18:20.916707Z","iopub.status.idle":"2024-05-29T14:18:20.922607Z","shell.execute_reply":"2024-05-29T14:18:20.921747Z","shell.execute_reply.started":"2024-05-29T14:18:20.917327Z"},"trusted":true},"outputs":[],"source":["# TO IMPLEMENT\n","def build_model(model_type):\n","    if model_type == 'BiSeNet':\n","        return BiSeNet(num_classes=19, context_path=\"resnet18\")\n","    elif model_type == 'DeepLabV2':\n","        pretrain_model_path = '/kaggle/input/deeplab_v2_model/pytorch/model_weight/1/deeplab_resnet_pretrained_imagenet.pth'\n","        return get_deeplab_v2(num_classes=19, pretrain=True, pretrain_model_path=pretrain_model_path)"]},{"cell_type":"markdown","metadata":{},"source":["## 3.1 - CityScapes"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T14:18:21.532523Z","iopub.status.busy":"2024-05-29T14:18:21.531498Z","iopub.status.idle":"2024-05-29T14:18:21.543770Z","shell.execute_reply":"2024-05-29T14:18:21.542754Z","shell.execute_reply.started":"2024-05-29T14:18:21.532486Z"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1715695833931,"user":{"displayName":"Tommaso Mazzarini","userId":"16046308562458308219"},"user_tz":-120},"id":"TE5jHHpwaduG","trusted":true},"outputs":[],"source":["class CityScapes(Dataset):\n","    def __init__(self, root_dir, split='train', image_transform=None, label_transform=None):\n","        super(CityScapes, self).__init__()\n","        \"\"\"\n","        Args:\n","            root_dir (string): Directory with all the images and annotations.\n","            split (string): 'train' or 'val'.\n","            transform (callable, optional): Optional transform to be applied on a sample.\n","        \"\"\"\n","\n","        self.root_dir = root_dir\n","        self.split = split\n","        self.image_transform = image_transform\n","        self.label_transform = label_transform\n","\n","        # Get the image and label directories\n","        self.image_dir = os.path.join(root_dir, 'images', split)\n","        self.label_dir = os.path.join(root_dir, 'gtFine', split)\n","\n","        # Get a list of all image files\n","        self.image_files = []\n","        for city_dir in os.listdir(self.image_dir):\n","            city_image_dir = os.path.join(self.image_dir, city_dir)\n","            self.image_files.extend([os.path.join(city_image_dir, f) for f in os.listdir(city_image_dir) if f.endswith('.png')])\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.image_files[idx]\n","\n","        # Get the corresponding label image path\n","        label_name = img_name.replace('images', 'gtFine').replace('_leftImg8bit', '_gtFine_labelTrainIds')\n","\n","        # Load image and label\n","        image = Image.open(img_name).convert('RGB')\n","        label = Image.open(label_name).convert('L')\n","\n","        if self.image_transform:\n","            image = self.image_transform(image)\n","        if self.label_transform:\n","            label = self.label_transform(label)\n","\n","        label = torch.Tensor(np.array(label))\n","\n","        return image, label"]},{"cell_type":"markdown","metadata":{},"source":["## GTA5"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T14:18:21.898427Z","iopub.status.busy":"2024-05-29T14:18:21.897562Z","iopub.status.idle":"2024-05-29T14:18:21.908829Z","shell.execute_reply":"2024-05-29T14:18:21.907874Z","shell.execute_reply.started":"2024-05-29T14:18:21.898392Z"},"trusted":true},"outputs":[],"source":["import matplotlib.image as mpimg\n","import matplotlib.pyplot as plt\n","import torchvision.transforms as T\n","\n","class GTA5(Dataset):\n","    def __init__(self, root_dir, image_transform=None, label_transform=None):\n","        super(GTA5, self).__init__()\n","        \"\"\"\n","        Args:\n","            root_dir (string): Directory with all the images and annotations.\n","            transform (callable, optional): Optional transform to be applied on a sample.\n","        \"\"\"\n","\n","        self.root_dir = root_dir\n","        self.image_transform = image_transform\n","        self.label_transform = label_transform\n","\n","        # Get the image and label directories\n","        self.image_dir = os.path.join(root_dir, 'images')\n","        self.label_dir = os.path.join(root_dir, 'masks')\n","\n","        # Get a list of all image files\n","        self.image_files = []\n","        # Get a list of all files in the images directory\n","        for file_name in os.listdir(self.image_dir):\n","            file_path = os.path.join(self.image_dir, file_name)\n","            if os.path.isfile(file_path):\n","                self.image_files.append(file_name)\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        img_name, label_name = self.image_files[idx], self.image_files[idx]\n","        \n","        img_path = os.path.join(self.image_dir, img_name)\n","        label_path = os.path.join(self.label_dir, label_name)\n","\n","        # Load image and label\n","        image = Image.open(img_path).convert('RGB')\n","        label = Image.open(label_path).convert('L')\n","        \n","        if self.image_transform:\n","            image = self.image_transform(image)\n","        if self.label_transform:\n","            label = self.label_transform(label)\n","\n","        label = torch.Tensor(np.array(label)) \n","\n","        return image, label"]},{"cell_type":"markdown","metadata":{"id":"breeFfksou_h"},"source":["# 4 - BiSeNet"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-29T14:18:22.239151Z","iopub.status.busy":"2024-05-29T14:18:22.238787Z","iopub.status.idle":"2024-05-29T14:18:22.272693Z","shell.execute_reply":"2024-05-29T14:18:22.271735Z","shell.execute_reply.started":"2024-05-29T14:18:22.239122Z"},"executionInfo":{"elapsed":12513,"status":"ok","timestamp":1715695905826,"user":{"displayName":"Tommaso Mazzarini","userId":"16046308562458308219"},"user_tz":-120},"id":"sOluxCUSo6TN","outputId":"18e9c583-19cc-4b94-f206-b30ba08384cb","trusted":true},"outputs":[],"source":["class ConvBlock(torch.nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n","                               stride=stride, padding=padding, bias=False)\n","        self.bn = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, input):\n","        x = self.conv1(input)\n","        return self.relu(self.bn(x))\n","\n","\n","class Spatial_path(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.convblock1 = ConvBlock(in_channels=3, out_channels=64)\n","        self.convblock2 = ConvBlock(in_channels=64, out_channels=128)\n","        self.convblock3 = ConvBlock(in_channels=128, out_channels=256)\n","\n","    def forward(self, input):\n","        x = self.convblock1(input)\n","        x = self.convblock2(x)\n","        x = self.convblock3(x)\n","        return x\n","\n","\n","class AttentionRefinementModule(torch.nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","        self.bn = nn.BatchNorm2d(out_channels)\n","        self.sigmoid = nn.Sigmoid()\n","        self.in_channels = in_channels\n","        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n","\n","    def forward(self, input):\n","        # global average pooling\n","        x = self.avgpool(input)\n","        assert self.in_channels == x.size(1), 'in_channels and out_channels should all be {}'.format(x.size(1))\n","        x = self.conv(x)\n","        x = self.sigmoid(self.bn(x))\n","        # x = self.sigmoid(x)\n","        # channels of input and x should be same\n","        x = torch.mul(input, x)\n","        return x\n","\n","\n","class FeatureFusionModule(torch.nn.Module):\n","    def __init__(self, num_classes, in_channels):\n","        super().__init__()\n","        # self.in_channels = input_1.channels + input_2.channels\n","        # resnet101 3328 = 256(from spatial path) + 1024(from context path) + 2048(from context path)\n","        # resnet18  1024 = 256(from spatial path) + 256(from context path) + 512(from context path)\n","        self.in_channels = in_channels\n","\n","        self.convblock = ConvBlock(in_channels=self.in_channels, out_channels=num_classes, stride=1)\n","        self.conv1 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n","        self.relu = nn.ReLU()\n","        self.conv2 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n","        self.sigmoid = nn.Sigmoid()\n","        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n","\n","    def forward(self, input_1, input_2):\n","        x = torch.cat((input_1, input_2), dim=1)\n","        assert self.in_channels == x.size(1), 'in_channels of ConvBlock should be {}'.format(x.size(1))\n","        feature = self.convblock(x)\n","        x = self.avgpool(feature)\n","\n","        x = self.relu(self.conv1(x))\n","        x = self.sigmoid(self.conv2(x))\n","        x = torch.mul(feature, x)\n","        x = torch.add(x, feature)\n","        return x\n","\n","\n","class BiSeNet(torch.nn.Module):\n","    def __init__(self, num_classes, context_path):\n","        super().__init__()\n","        # build spatial path\n","        self.saptial_path = Spatial_path()\n","\n","        # build context path\n","        self.context_path = build_contextpath(name=context_path)\n","\n","        # build attention refinement module  for resnet 101\n","        if context_path == 'resnet101':\n","            self.attention_refinement_module1 = AttentionRefinementModule(1024, 1024)\n","            self.attention_refinement_module2 = AttentionRefinementModule(2048, 2048)\n","            # supervision block\n","            self.supervision1 = nn.Conv2d(in_channels=1024, out_channels=num_classes, kernel_size=1)\n","            self.supervision2 = nn.Conv2d(in_channels=2048, out_channels=num_classes, kernel_size=1)\n","            # build feature fusion module\n","            self.feature_fusion_module = FeatureFusionModule(num_classes, 3328)\n","\n","        elif context_path == 'resnet18':\n","            # build attention refinement module  for resnet 18\n","            self.attention_refinement_module1 = AttentionRefinementModule(256, 256)\n","            self.attention_refinement_module2 = AttentionRefinementModule(512, 512)\n","            # supervision block\n","            self.supervision1 = nn.Conv2d(in_channels=256, out_channels=num_classes, kernel_size=1)\n","            self.supervision2 = nn.Conv2d(in_channels=512, out_channels=num_classes, kernel_size=1)\n","            # build feature fusion module\n","            self.feature_fusion_module = FeatureFusionModule(num_classes, 1024)\n","        else:\n","            print('Error: unspport context_path network \\n')\n","\n","        # build final convolution\n","        self.conv = nn.Conv2d(in_channels=num_classes, out_channels=num_classes, kernel_size=1)\n","\n","        self.init_weight()\n","\n","        self.mul_lr = []\n","        self.mul_lr.append(self.saptial_path)\n","        self.mul_lr.append(self.attention_refinement_module1)\n","        self.mul_lr.append(self.attention_refinement_module2)\n","        self.mul_lr.append(self.supervision1)\n","        self.mul_lr.append(self.supervision2)\n","        self.mul_lr.append(self.feature_fusion_module)\n","        self.mul_lr.append(self.conv)\n","\n","    def init_weight(self):\n","        for name, m in self.named_modules():\n","            if 'context_path' not in name:\n","                if isinstance(m, nn.Conv2d):\n","                    nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n","                elif isinstance(m, nn.BatchNorm2d):\n","                    m.eps = 1e-5\n","                    m.momentum = 0.1\n","                    nn.init.constant_(m.weight, 1)\n","                    nn.init.constant_(m.bias, 0)\n","\n","    def forward(self, input):\n","        # output of spatial path\n","        sx = self.saptial_path(input)\n","\n","        # output of context path\n","        cx1, cx2, tail = self.context_path(input)\n","        cx1 = self.attention_refinement_module1(cx1)\n","        cx2 = self.attention_refinement_module2(cx2)\n","        cx2 = torch.mul(cx2, tail)\n","        # upsampling\n","        cx1 = torch.nn.functional.interpolate(cx1, size=sx.size()[-2:], mode='bilinear')\n","        cx2 = torch.nn.functional.interpolate(cx2, size=sx.size()[-2:], mode='bilinear')\n","        cx = torch.cat((cx1, cx2), dim=1)\n","\n","        if self.training == True:\n","            cx1_sup = self.supervision1(cx1)\n","            cx2_sup = self.supervision2(cx2)\n","            cx1_sup = torch.nn.functional.interpolate(cx1_sup, size=input.size()[-2:], mode='bilinear')\n","            cx2_sup = torch.nn.functional.interpolate(cx2_sup, size=input.size()[-2:], mode='bilinear')\n","\n","        # output of feature fusion module\n","        result = self.feature_fusion_module(sx, cx)\n","\n","        # upsampling\n","        result = torch.nn.functional.interpolate(result, scale_factor=8, mode='bilinear')\n","        result = self.conv(result)\n","\n","        if self.training == True:\n","            return result, cx1_sup, cx2_sup\n","\n","        return result"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T14:18:22.405431Z","iopub.status.busy":"2024-05-29T14:18:22.405120Z","iopub.status.idle":"2024-05-29T14:18:22.419212Z","shell.execute_reply":"2024-05-29T14:18:22.418362Z","shell.execute_reply.started":"2024-05-29T14:18:22.405407Z"},"trusted":true},"outputs":[],"source":["class resnet18(torch.nn.Module):\n","    def __init__(self, pretrained=True):\n","        super().__init__()\n","        self.features = models.resnet18(pretrained=pretrained)\n","        self.conv1 = self.features.conv1\n","        self.bn1 = self.features.bn1\n","        self.relu = self.features.relu\n","        self.maxpool1 = self.features.maxpool\n","        self.layer1 = self.features.layer1\n","        self.layer2 = self.features.layer2\n","        self.layer3 = self.features.layer3\n","        self.layer4 = self.features.layer4\n","\n","    def forward(self, input):\n","        x = self.conv1(input)\n","        x = self.relu(self.bn1(x))\n","        x = self.maxpool1(x)\n","        feature1 = self.layer1(x)  # 1 / 4\n","        feature2 = self.layer2(feature1)  # 1 / 8\n","        feature3 = self.layer3(feature2)  # 1 / 16\n","        feature4 = self.layer4(feature3)  # 1 / 32\n","        # global average pooling to build tail\n","        tail = torch.mean(feature4, 3, keepdim=True)\n","        tail = torch.mean(tail, 2, keepdim=True)\n","        return feature3, feature4, tail\n","\n","\n","class resnet101(torch.nn.Module):\n","    def __init__(self, pretrained=True):\n","        super().__init__()\n","        self.features = models.resnet101(pretrained=pretrained)\n","        self.conv1 = self.features.conv1\n","        self.bn1 = self.features.bn1\n","        self.relu = self.features.relu\n","        self.maxpool1 = self.features.maxpool\n","        self.layer1 = self.features.layer1\n","        self.layer2 = self.features.layer2\n","        self.layer3 = self.features.layer3\n","        self.layer4 = self.features.layer4\n","\n","    def forward(self, input):\n","        x = self.conv1(input)\n","        x = self.relu(self.bn1(x))\n","        x = self.maxpool1(x)\n","        feature1 = self.layer1(x)  # 1 / 4\n","        feature2 = self.layer2(feature1)  # 1 / 8\n","        feature3 = self.layer3(feature2)  # 1 / 16\n","        feature4 = self.layer4(feature3)  # 1 / 32\n","        # global average pooling to build tail\n","        tail = torch.mean(feature4, 3, keepdim=True)\n","        tail = torch.mean(tail, 2, keepdim=True)\n","        return feature3, feature4, tail\n","\n","\n","def build_contextpath(name):\n","    model = {\n","        'resnet18': resnet18(pretrained=True),\n","        'resnet101': resnet101(pretrained=True)\n","    }\n","    return model[name]"]},{"cell_type":"markdown","metadata":{},"source":["# 5 - Training"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T14:18:23.301517Z","iopub.status.busy":"2024-05-29T14:18:23.301148Z","iopub.status.idle":"2024-05-29T14:18:23.310240Z","shell.execute_reply":"2024-05-29T14:18:23.309186Z","shell.execute_reply.started":"2024-05-29T14:18:23.301481Z"},"trusted":true},"outputs":[],"source":["def train(model, dataloader, criterion, optimizer, config, start_epoch):\n","    \n","    for epoch in range(start_epoch, config.epochs):\n","        running_loss = 0.0\n","        total_mIOU = 0\n","        total_images = 0\n","        \n","        for _, (inputs, targets) in enumerate(dataloader):\n","\n","            inputs, targets = inputs.cuda(), id_processing(targets).cuda()\n","\n","            outputs = model(inputs)\n","\n","            loss = criterion(outputs[0], targets)\n","\n","            # Backprpagation\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","            _, predicted = outputs[0].max(1)\n","\n","            running_mIOU = mean_iou(outputs[0].size()[1], predicted, targets)\n","            total_mIOU += running_mIOU.sum().item()\n","            total_images += len(predicted)\n","\n","            poly_lr_scheduler(optimizer, config.learning_rate, iter=epoch, max_iter=config.epochs)\n","                \n","        train_loss = running_loss / len(dataloader)\n","        mIOU = total_mIOU/total_images\n","        \n","        # Save the train metrics by using wandb\n","        train_log(train_loss, mIOU, epoch)\n","        \n","        # Save checkpoint (overwrite)\n","        save_checkpoint(config, model, optimizer, train_loss, mIOU, epoch)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T14:18:23.715050Z","iopub.status.busy":"2024-05-29T14:18:23.714404Z","iopub.status.idle":"2024-05-29T14:18:23.720674Z","shell.execute_reply":"2024-05-29T14:18:23.719671Z","shell.execute_reply.started":"2024-05-29T14:18:23.715016Z"},"trusted":true},"outputs":[],"source":["def id_processing(targets):\n","    targets = targets.cuda()\n","    \n","    # Define valid indices\n","    valid_indices = torch.tensor(list(range(19)) + [255]).to(targets.device)\n","\n","    # Replace all IDs not in valid_indices with 255\n","    processed_targets = torch.where(torch.isin(targets, valid_indices), targets, torch.tensor(255, device=targets.device))\n","    \n","    # Release GPU memory IMPORTANT\n","    del targets\n","    torch.cuda.empty_cache()\n","\n","    return processed_targets.long()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T14:18:24.130474Z","iopub.status.busy":"2024-05-29T14:18:24.129635Z","iopub.status.idle":"2024-05-29T14:18:24.137710Z","shell.execute_reply":"2024-05-29T14:18:24.136738Z","shell.execute_reply.started":"2024-05-29T14:18:24.130446Z"},"trusted":true},"outputs":[],"source":["def poly_lr_scheduler(optimizer, init_lr, iter, lr_decay_iter=1,\n","                      max_iter=300, power=0.9):\n","    \"\"\"Polynomial decay of learning rate\n","            :param init_lr is base learning rate\n","            :param iter is a current iteration\n","            :param lr_decay_iter how frequently decay occurs, default is 1\n","            :param max_iter is number of maximum iterations\n","            :param power is a polymomial power\n","\n","    \"\"\"\n","    # if iter % lr_decay_iter or iter > max_iter:\n","    # \treturn optimizer\n","\n","    lr = init_lr*(1 - iter/max_iter)**power\n","    optimizer.param_groups[0]['lr'] = lr\n","    return lr\n","\n","def mean_iou(num_classes, pred, target):\n","    mIOU = 0\n","    for i in range(len(pred)):\n","        hist = fast_hist(target[i].cpu().numpy(),pred[i].cpu().numpy(), num_classes)\n","        IOU = per_class_iou(hist)\n","        mIOU = mIOU + sum(IOU)/num_classes\n","    return mIOU\n","\n","def fast_hist(a, b, n):\n","    \"\"\"\n","    a and b are predict and mask respectively\n","    n is the number of classes\n","    \"\"\"\n","    k = (a >= 0) & (a < n) #assign True if the value is in the range between 0 and 18 (class labels)\n","    return np.bincount(n * a[k].astype(int) + b[k], minlength=n ** 2).reshape((n, n))\n","\n","def per_class_iou(hist):\n","    epsilon = 1e-5\n","    return (np.diag(hist)) / (hist.sum(1) + hist.sum(0) - np.diag(hist) + epsilon)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T14:18:25.258554Z","iopub.status.busy":"2024-05-29T14:18:25.257814Z","iopub.status.idle":"2024-05-29T14:18:25.263359Z","shell.execute_reply":"2024-05-29T14:18:25.262422Z","shell.execute_reply.started":"2024-05-29T14:18:25.258521Z"},"trusted":true},"outputs":[],"source":["def train_log(loss, mIOU, epoch):\n","    #wandb.log({\"epoch\": epoch, \"loss\": loss, \"mIOU\":mIOU})\n","    wandb.log({\"loss\": loss, \"mIOU\":mIOU}, step= epoch)\n","    print(f\"----------------------------------\")\n","    print(f\"Loss after {epoch} epochs: {loss:.3f}\")\n","    print(f\"mIOU after {epoch} epochs: {mIOU:.3f}%\")"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T14:18:26.315527Z","iopub.status.busy":"2024-05-29T14:18:26.314708Z","iopub.status.idle":"2024-05-29T14:18:26.323757Z","shell.execute_reply":"2024-05-29T14:18:26.322866Z","shell.execute_reply.started":"2024-05-29T14:18:26.315495Z"},"trusted":true},"outputs":[],"source":["def save_checkpoint(config, model, optimizer, train_loss, mIOU, epoch):\n","    checkpoint_path = os.path.join(config.checkpoint_dir, \"checkpoint.pth\")\n","    torch.save({\n","        'epoch': epoch + 1,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'loss': train_loss,\n","        'mIOU': mIOU\n","    }, checkpoint_path)\n","    print(f\"Checkpoint saved in {checkpoint_path} | Epoch: {epoch}\")\n","    \n","    \n","def load_checkpoint(config, model, optimizer):\n","    if os.path.exists(config.checkpoint_dir):\n","        checkpoint = torch.load(config.checkpoint_dir + \"/checkpoint.pth\")\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        start_epoch = checkpoint['epoch']\n","        print(f\"Checkpoint found. Resuming from epoch {start_epoch}.\")\n","        return start_epoch\n","    else:\n","        os.mkdir(config.checkpoint_dir) # divide the directory wrt the model (eg. checkpoints/DeepLabV2, checkpoints/BiSeNet)\n","        print(\"No checkpoint found. Starting from scratch.\")\n","        return 0"]},{"cell_type":"markdown","metadata":{"id":"vcIkDP8ho8wt"},"source":["# 6 - Validation"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T14:18:27.063165Z","iopub.status.busy":"2024-05-29T14:18:27.062465Z","iopub.status.idle":"2024-05-29T14:18:27.074744Z","shell.execute_reply":"2024-05-29T14:18:27.073410Z","shell.execute_reply.started":"2024-05-29T14:18:27.063134Z"},"trusted":true},"outputs":[],"source":["# Validation method\n","def val(model, dataloader):\n","    model.eval()\n","    total_mIOU = 0\n","    total_images = 0\n","    latency_list = []\n","    FPS_list = []\n","    \n","    with torch.no_grad():\n","        for _, (inputs, targets) in enumerate(dataloader):\n","            inputs, targets = inputs.cuda(), id_processing(targets).cuda()\n","            \n","            start = time.time() # Record start time\n","            outputs = model(inputs)\n","            end = time.time() # Record end time\n","\n","            # Calculate latency for this iteration\n","            latency_i = end - start\n","            latency_list.append(latency_i)\n","\n","            # Calculate FPS for this iteration\n","            FPS_i = 1 / latency_i\n","            FPS_list.append(FPS_i)\n","\n","            _, predicted = outputs.max(1)\n","            \n","            running_mIOU = mean_iou(outputs.size()[1], predicted, targets)\n","            total_mIOU += running_mIOU.sum().item()\n","            total_images += len(predicted)\n","        \n","    mIOU = total_mIOU/total_images\n","    latency = np.sum(latency_list) / len(latency_list)\n","    test_FPS = np.sum(FPS_list) / len(FPS_list)\n","    \n","    # compute flops and #param\n","    image, _ = next(iter(dataloader))\n","    height, width = image.shape[2], image.shape[3]\n","    zero_image = torch.zeros((1, 3, height, width))\n","    flops = FlopCountAnalysis(model, zero_image.cuda())\n","    print(flop_count_table(flops))\n","\n","    print(f'\\n\\nmIoU: {(mIOU*100):.3f}%, Latency: {latency:.3f}, FPS: {test_FPS:.3f}')\n","\n","    wandb.log({\"mIOU\":mIOU,\"latency\":latency,\"FPS\":test_FPS})"]},{"cell_type":"markdown","metadata":{},"source":["# 7 - Hyperparameter Sweeps using WanDB"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T17:44:08.710804Z","iopub.status.busy":"2024-05-25T17:44:08.710488Z","iopub.status.idle":"2024-05-25T17:44:08.716354Z","shell.execute_reply":"2024-05-25T17:44:08.715306Z","shell.execute_reply.started":"2024-05-25T17:44:08.710781Z"},"trusted":true},"outputs":[],"source":["sweep_config= {\n","    'name': 'DomainAdaptation-sweep',\n","    'metric': {'name': 'loss', 'goal': 'minimize'}, # the goal is maximize the accuracy\n","    'method': 'random', # test all possible combinations of the hyperparameters\n","    'parameters': {\n","        'epochs': {'values': [5]},        \n","        'learning_rate': {'values': [0.1, 0.001, 0.0001]}, # 2 parameters to optimize during the sweep\n","        'batch_size': {'values': [2, 4, 8]},\n","        'momentum': {'values': [0.9]},\n","        'weight_decay': {'values': [5e-4]}\n","    }\n","}"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T17:06:51.408913Z","iopub.status.busy":"2024-05-18T17:06:51.407840Z","iopub.status.idle":"2024-05-18T17:06:51.741483Z","shell.execute_reply":"2024-05-18T17:06:51.740501Z","shell.execute_reply.started":"2024-05-18T17:06:51.408873Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Create sweep with ID: s426arij\n","Sweep URL: https://wandb.ai/polito-tmazzarini/mldl_step2b/sweeps/s426arij\n"]}],"source":["sweep_id = wandb.sweep(sweep=sweep_config, project=\"MLDL-step3a\")"]},{"cell_type":"code","execution_count":28,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-05-18T17:06:58.186089Z","iopub.status.busy":"2024-05-18T17:06:58.185728Z","iopub.status.idle":"2024-05-18T17:06:58.563398Z","shell.execute_reply":"2024-05-18T17:06:58.562595Z","shell.execute_reply.started":"2024-05-18T17:06:58.186061Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x6jq0rxd with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch: 5\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.9\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run x6jq0rxd errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m TypeError: model_pipeline() missing 1 required positional argument: 'hyperparameters'\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n"]}],"source":["if torch.cuda.is_available():\n","    print(\"Start hyperparameter sweeps\\n\")\n","    wandb.agent(sweep_id, function=model_pipeline, count=10)\n","else:\n","    print(\"CUDA is Not available\")"]},{"cell_type":"markdown","metadata":{},"source":["# 8 - Build final model"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T14:18:30.092546Z","iopub.status.busy":"2024-05-29T14:18:30.091595Z","iopub.status.idle":"2024-05-29T14:18:30.097291Z","shell.execute_reply":"2024-05-29T14:18:30.096236Z","shell.execute_reply.started":"2024-05-29T14:18:30.092512Z"},"trusted":true},"outputs":[],"source":["# best configuration (TO CONFIGURE)\n","config = dict(\n","    epochs=50,\n","    batch_size=2,\n","    learning_rate=0.001,\n","    momentum=0.9,\n","    weight_decay=5e-4,\n","    architecture=\"BiSeNet\",\n","    checkpoint_dir=\"/kaggle/working/checkpoints\")"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T14:18:30.443339Z","iopub.status.busy":"2024-05-29T14:18:30.442950Z","iopub.status.idle":"2024-05-29T14:24:30.097570Z","shell.execute_reply":"2024-05-29T14:24:30.095966Z","shell.execute_reply.started":"2024-05-29T14:18:30.443310Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtommasomazzarini2001\u001b[0m (\u001b[33mpolito-tmazzarini\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"name":"stdout","output_type":"stream","text":["Building the model with the best configuration\n"]},{"data":{"text/html":["wandb version 0.17.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.6"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240529_141830-yw32xg8w</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/yw32xg8w' target=\"_blank\">visionary-firebrand-64</a></strong> to <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/yw32xg8w' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/yw32xg8w</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 134MB/s] \n","Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n","100%|██████████| 171M/171M [00:01<00:00, 158MB/s]  \n"]},{"name":"stdout","output_type":"stream","text":["No checkpoint found. Starting from scratch.\n"]},{"name":"stderr","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/tmp/ipykernel_34/2884409676.py\", line 12, in model_pipeline\n","    train(model, train_loader, criterion, optimizer, config, start_epoch)\n","  File \"/tmp/ipykernel_34/1525349719.py\", line 25, in train\n","    running_mIOU = mean_iou(outputs[0].size()[1], predicted, targets)\n","  File \"/tmp/ipykernel_34/2562121041.py\", line 4, in mean_iou\n","    hist = fast_hist(target[i].cpu().numpy(),pred[i].cpu().numpy(), num_classes)\n","  File \"/tmp/ipykernel_34/2562121041.py\", line 15, in fast_hist\n","    return np.bincount(n * a[k].astype(int) + b[k], minlength=n ** 2).reshape((n, n))\n","KeyboardInterrupt\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">visionary-firebrand-64</strong> at: <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/yw32xg8w' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a/runs/yw32xg8w</a><br/> View project at: <a href='https://wandb.ai/polito-tmazzarini/MLDL-step3a' target=\"_blank\">https://wandb.ai/polito-tmazzarini/MLDL-step3a</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240529_141830-yw32xg8w/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilding the model with the best configuration\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Build, train and analyze the model with the pipeline\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA is Not available\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36mmodel_pipeline\u001b[0;34m(hyperparameters)\u001b[0m\n\u001b[1;32m      9\u001b[0m model, train_loader, val_loader, criterion, optimizer, start_epoch \u001b[38;5;241m=\u001b[39m make(config)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# and use them to train the model\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# and test its final performance\u001b[39;00m\n\u001b[1;32m     15\u001b[0m val(model, val_loader)\n","Cell \u001b[0;32mIn[14], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, criterion, optimizer, config, start_epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     23\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m running_mIOU \u001b[38;5;241m=\u001b[39m \u001b[43mmean_iou\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m total_mIOU \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m running_mIOU\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     27\u001b[0m total_images \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(predicted)\n","Cell \u001b[0;32mIn[16], line 4\u001b[0m, in \u001b[0;36mmean_iou\u001b[0;34m(num_classes, pred, target)\u001b[0m\n\u001b[1;32m      2\u001b[0m mIOU \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(pred)):\n\u001b[0;32m----> 4\u001b[0m     hist \u001b[38;5;241m=\u001b[39m \u001b[43mfast_hist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     IOU \u001b[38;5;241m=\u001b[39m per_class_iou(hist)\n\u001b[1;32m      6\u001b[0m     mIOU \u001b[38;5;241m=\u001b[39m mIOU \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msum\u001b[39m(IOU)\u001b[38;5;241m/\u001b[39mnum_classes\n","Cell \u001b[0;32mIn[16], line 15\u001b[0m, in \u001b[0;36mfast_hist\u001b[0;34m(a, b, n)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03ma and b are predict and mask respectively\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03mn is the number of classes\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m k \u001b[38;5;241m=\u001b[39m (a \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (a \u001b[38;5;241m<\u001b[39m n) \u001b[38;5;66;03m#assign True if the value is in the range between 0 and 18 (class labels)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbincount\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape((n, n))\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["if torch.cuda.is_available():\n","    print(\"Building the model with the best configuration\")\n","    # Build, train and analyze the model with the pipeline\n","    model = model_pipeline(config)\n","else:\n","    print(\"CUDA is Not available\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-21T18:43:13.282783Z","iopub.status.idle":"2024-05-21T18:43:13.283135Z","shell.execute_reply":"2024-05-21T18:43:13.282961Z","shell.execute_reply.started":"2024-05-21T18:43:13.282947Z"},"trusted":true},"outputs":[],"source":["wandb.finish()"]},{"cell_type":"markdown","metadata":{},"source":["# 9 - Save model weights"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.save(model.state_dict(), 'model_weights.pth')"]},{"cell_type":"markdown","metadata":{},"source":["# 10 - Model Application"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Lad the model \n","model =  pass # we do not specify ``weights``, i.e. create untrained model\n","model.load_state_dict(torch.load('model_weights.pth'))\n","model.eval()"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T09:28:20.040795Z","iopub.status.busy":"2024-05-22T09:28:20.040095Z","iopub.status.idle":"2024-05-22T09:28:20.045272Z","shell.execute_reply":"2024-05-22T09:28:20.044240Z","shell.execute_reply.started":"2024-05-22T09:28:20.040762Z"},"trusted":true},"outputs":[],"source":["# pick une image\n","# use model and predict the mask\n","# map each class to a colored value\n","# plot the segmented image"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOekpKEeKXtTbQhIZpLmV9P","gpuType":"T4","mount_file_id":"1XNh4ReVvaCnFcvxsLVHahkWxy2OqKrm4","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4959785,"sourceId":8348545,"sourceType":"datasetVersion"},{"datasetId":4966814,"sourceId":8358034,"sourceType":"datasetVersion"},{"datasetId":5069095,"sourceId":8495419,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
