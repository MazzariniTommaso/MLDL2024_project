{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8412080,"sourceType":"datasetVersion","datasetId":5006755},{"sourceId":8412108,"sourceType":"datasetVersion","datasetId":5006778},{"sourceId":8465917,"sourceType":"datasetVersion","datasetId":5047290},{"sourceId":8495419,"sourceType":"datasetVersion","datasetId":5069095}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Discriminator\nFor the discriminator, we use an architecture similar to [30] but utilize all fully-convolutional layers to retain the spatial information. The network consists of 5 convolution layers with kernel 4 × 4 and stride of 2, where the channel number is {64, 128, 256, 512, 1}, respectively. Except for the last layer, each convolution layer is followed by a leaky ReLU [27] parameterized by 0.2. An up-sampling layer is added to the last convolution layer for re-scaling the output to the size of the input. We do not use any batch-normalization layers [16] as we jointly train the discriminator with the segmentation network using a small batch size.\n# Segmentation Network\nIt is essential to build upon a good baseline model to achieve high-quality segmentation results\n[2, 38, 40]. We adopt the DeepLab-v2 [2] framework with ResNet-101 [11] model pre-trained on ImageNet [6] as our segmentation baseline network. However, we do not use the multi-scale fusion strategy [2] due to the memory issue.\nSimilar to the recent work on semantic segmentation [2, 38], we remove the last classification layer and modify the stride\nof the last two convolution layers from 2 to 1, making the resolution of the output feature maps effectively 1/8 times the input image size. To enlarge the receptive field, we apply dilated convolution layers [38] in conv4 and conv5 layers with a stride of 2 and 4, respectively. After the last layer, we use the Atrous Spatial Pyramid Pooling (ASPP) [2] as the final classifier. Finally, we apply an up-sampling layer along with the softmax output to match the size of the input image. Based on this architecture, our segmentation model achieves 65.1% mean intersection-over-union (IoU) whentrained on the Cityscapes [4] training set and tested on the Cityscapes validation set.\n### Multi-level Adaptation Model\nWe construct the abovementioned discriminator and segmentation network as our ingle-level adaptation model. For the multi-level structure, we extract feature maps from the conv4 layer and add an ASPP module as the auxiliary classifier. Similarly, a discriminator with the same architecture is added for adversarial learning. Figure 2 shows the proposed multi-level adaptation model. In this paper, we use two levels due to the balance of its efficiency and accuracy.\n# Network Training. \nTo train the proposed single/multi-level adaptation model, we find that jointly training the segmentation network and discriminators in one stage is effective.\nIn each training batch, we first forward the source image Is to optimize the segmentation network for Lseg in (3) and generate the output Ps. For the target image It, we obtain the segmentation output Pt, and pass it along with Ps to the discriminator for optimizing Ld in (2). In addition, we compute the adversarial loss Ladv in (4) for the target prediction\nPt. For the multi-level training objective in (5), we simply repeat the same procedure for each adaptation module.\nTo train the segmentation network, we use the Stochastic Gradient Descent (SGD) optimizer with Nesterov acceleration where\nthe momentum is 0.9 and the weight decay is 10−4. The initial learning rate is set as 2.5 × 10−4 and is decreased using the polynomial decay with power of 0.9 as mentioned in [2]. For training the discriminator, we use the Adam optimizer [18] with the learning rate as 10−4 and the same polynomial decay as the segmentation network. The momentum is set as 0.9 and 0.99.\n","metadata":{}},{"cell_type":"markdown","source":"# REPO GITHUB\n[https://github.com/wasidennis/AdaptSegNet/tree/master](http://)\n\nhttps://github.com/hfslyc/AdvSemiSeg/tree/master","metadata":{}},{"cell_type":"code","source":"!pip install -U fvcore","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:25:08.409141Z","iopub.execute_input":"2024-06-07T14:25:08.409547Z","iopub.status.idle":"2024-06-07T14:25:20.785027Z","shell.execute_reply.started":"2024-06-07T14:25:08.409517Z","shell.execute_reply":"2024-06-07T14:25:20.784033Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Requirement already satisfied: fvcore in /opt/conda/lib/python3.10/site-packages (0.1.5.post20221221)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fvcore) (1.26.4)\nRequirement already satisfied: yacs>=0.1.6 in /opt/conda/lib/python3.10/site-packages (from fvcore) (0.1.8)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from fvcore) (6.0.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from fvcore) (4.66.1)\nRequirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.10/site-packages (from fvcore) (2.4.0)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from fvcore) (9.5.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from fvcore) (0.9.0)\nRequirement already satisfied: iopath>=0.1.7 in /opt/conda/lib/python3.10/site-packages (from fvcore) (0.1.10)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from iopath>=0.1.7->fvcore) (4.9.0)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.10/site-packages (from iopath>=0.1.7->fvcore) (2.8.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"# If you run the model for the first time remove all the previus checkpoints\n! rm -r checkpoints/","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:25:20.787038Z","iopub.execute_input":"2024-06-07T14:25:20.787344Z","iopub.status.idle":"2024-06-07T14:25:21.797279Z","shell.execute_reply.started":"2024-06-07T14:25:20.787316Z","shell.execute_reply":"2024-06-07T14:25:21.795934Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# IMPORT","metadata":{}},{"cell_type":"code","source":"import wandb\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport os\nimport zipfile\nimport numpy as np\nimport time\nfrom PIL import Image\n\nfrom fvcore.nn import FlopCountAnalysis, flop_count_table\n\nimport warnings\nwarnings.filterwarnings(action='ignore')","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:25:21.799230Z","iopub.execute_input":"2024-06-07T14:25:21.799599Z","iopub.status.idle":"2024-06-07T14:25:21.808728Z","shell.execute_reply.started":"2024-06-07T14:25:21.799565Z","shell.execute_reply":"2024-06-07T14:25:21.807734Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# MODEL PIPELINE","metadata":{}},{"cell_type":"code","source":"def model_pipeline(config=None):\n\n    # make the model, data, and optimization problem\n    model, source_loader, target_loader, val_loader, criterion, optimizer, start_epoch = make(config)\n\n#     val(model, val_loader)\n    # and use them to train the model\n    train(model, source_loader, target_loader, criterion, optimizer, config, start_epoch)\n\n    # and test its final performance\n    vale(model, val_loader)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:25:21.811528Z","iopub.execute_input":"2024-06-07T14:25:21.811903Z","iopub.status.idle":"2024-06-07T14:25:21.820478Z","shell.execute_reply.started":"2024-06-07T14:25:21.811870Z","shell.execute_reply":"2024-06-07T14:25:21.819596Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# DATASET","metadata":{}},{"cell_type":"code","source":"def make(config):\n    # Make the data\n    (source, target) , test = get_data(train=True), get_data(train=False)\n    source_loader = make_loader(source, batch_size=config[\"batch_size\"],train=True)\n    target_loader = make_loader(target, batch_size=config[\"batch_size\"],train=True)\n    test_loader = make_loader(test, batch_size=config[\"batch_size\"],train=False)\n\n    # Make the model (BiSeNet with ResNet-18 backbone)\n    model = build_model(model_type='BiSeNet').cuda()\n\n    # Make the loss and optimizer\n    optimizer = optim.SGD(model.parameters(), \n                          lr=config[\"learning_rate\"], \n                          momentum=config[\"momentum\"], \n                          weight_decay=config[\"weight_decay\"])\n    \n    criterion = torch.nn.CrossEntropyLoss(ignore_index=255)\n    \n    # Load the last checkpoint\n    start_epoch = load_checkpoint(config, model, optimizer)\n    \n    return model, source_loader, target_loader, test_loader, criterion, optimizer, start_epoch","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:25:21.821876Z","iopub.execute_input":"2024-06-07T14:25:21.822215Z","iopub.status.idle":"2024-06-07T14:25:21.833336Z","shell.execute_reply.started":"2024-06-07T14:25:21.822188Z","shell.execute_reply":"2024-06-07T14:25:21.832506Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Define transforms for preprocessing\nimage_transform = {\n    'cityscapes': transforms.Compose([\n        transforms.Resize((512,1024)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ]),\n    'gta': transforms.Compose([\n        transforms.Resize((720,1280)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n}\n\nlabel_transform = {\n    'cityscapes': transforms.Compose([\n        transforms.Resize((512,1024))\n    ]),\n    'gta': transforms.Compose([\n        transforms.Resize((720,1280))\n    ])\n}\n\n# GTA5 for train and CityScapes for test\ncitiyscapes_dir ='/kaggle/input/cityscapes/Cityscapes/Cityspaces'\n#gta_dir = '/kaggle/input/gta5-dataset/GTA5'\ngta_dir = '/kaggle/input/gta5-dataset-with-masks'\n\ndef get_data(train=True):\n    if train == True:\n        # train dataset\n        source_dataset = GTA5(root_dir=gta_dir, image_transform=image_transform['gta'], label_transform=label_transform['gta'])\n        target_dataset = CityScapes(root_dir=citiyscapes_dir, split='train', image_transform=image_transform['cityscapes'], label_transform=label_transform['cityscapes'])\n        return source_dataset, target_dataset\n    else:\n        # test dataset\n        dataset = CityScapes(root_dir=citiyscapes_dir, split='val', image_transform=image_transform['cityscapes'], label_transform=label_transform['cityscapes'])\n        return dataset\n\n\ndef make_loader(dataset, batch_size = 8, train=True):\n    if train == True:\n        # train dataloader\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n    else:\n        # test dataloader\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False,drop_last=True)\n    \n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:25:21.834528Z","iopub.execute_input":"2024-06-07T14:25:21.834896Z","iopub.status.idle":"2024-06-07T14:25:21.853854Z","shell.execute_reply.started":"2024-06-07T14:25:21.834867Z","shell.execute_reply":"2024-06-07T14:25:21.852906Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# TO IMPLEMENT\ndef build_model(model_type):\n    if model_type == 'BiSeNet':\n        return BiSeNet(num_classes=19, context_path=\"resnet18\")\n    elif model_type == 'DeepLabV2':\n        pretrain_model_path = '/kaggle/input/model-weight/deeplab_resnet_pretrained_imagenet.pth'\n        return get_deeplab_v2(num_classes=19, pretrain=True, pretrain_model_path=pretrain_model_path)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:25:21.855239Z","iopub.execute_input":"2024-06-07T14:25:21.855542Z","iopub.status.idle":"2024-06-07T14:25:21.869081Z","shell.execute_reply.started":"2024-06-07T14:25:21.855515Z","shell.execute_reply":"2024-06-07T14:25:21.868208Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"class CityScapes(Dataset):\n    def __init__(self, root_dir, split='train', image_transform=None, label_transform=None):\n        super(CityScapes, self).__init__()\n        \"\"\"\n        Args:\n            root_dir (string): Directory with all the images and annotations.\n            split (string): 'train' or 'val'.\n            transform (callable, optional): Optional transform to be applied on a sample.\n        \"\"\"\n\n        self.root_dir = root_dir\n        self.split = split\n        self.image_transform = image_transform\n        self.label_transform = label_transform\n\n        # Get the image and label directories\n        self.image_dir = os.path.join(root_dir, 'images', split)\n        self.label_dir = os.path.join(root_dir, 'gtFine', split)\n\n        # Get a list of all image files\n        self.image_files = []\n        for city_dir in os.listdir(self.image_dir):\n            city_image_dir = os.path.join(self.image_dir, city_dir)\n            self.image_files.extend([os.path.join(city_image_dir, f) for f in os.listdir(city_image_dir) if f.endswith('.png')])\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_name = self.image_files[idx]\n\n        # Get the corresponding label image path\n        label_name = img_name.replace('images', 'gtFine').replace('_leftImg8bit', '_gtFine_labelTrainIds')\n\n        # Load image and label\n        image = Image.open(img_name).convert('RGB')\n        label = Image.open(label_name).convert('L')\n\n        if self.image_transform:\n            image = self.image_transform(image)\n        if self.label_transform:\n            label = self.label_transform(label)\n\n        label = torch.Tensor(np.array(label))\n\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:25:21.870248Z","iopub.execute_input":"2024-06-07T14:25:21.870821Z","iopub.status.idle":"2024-06-07T14:25:21.884705Z","shell.execute_reply.started":"2024-06-07T14:25:21.870789Z","shell.execute_reply":"2024-06-07T14:25:21.883735Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"import matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as T\n\nclass GTA5(Dataset):\n    def __init__(self, root_dir, image_transform=None, label_transform=None):\n        super(GTA5, self).__init__()\n        \"\"\"\n        Args:\n            root_dir (string): Directory with all the images and annotations.\n            transform (callable, optional): Optional transform to be applied on a sample.\n        \"\"\"\n\n        self.root_dir = root_dir\n        self.image_transform = image_transform\n        self.label_transform = label_transform\n\n        # Get the image and label directories\n        self.image_dir = os.path.join(root_dir, 'images')\n        self.label_dir = os.path.join(root_dir, 'masks')\n\n        # Get a list of all image files\n        self.image_files = []\n        # Get a list of all files in the images directory\n        for file_name in os.listdir(self.image_dir):\n            file_path = os.path.join(self.image_dir, file_name)\n            if os.path.isfile(file_path):\n                self.image_files.append(file_name)\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_name, label_name = self.image_files[idx], self.image_files[idx]\n        \n        img_path = os.path.join(self.image_dir, img_name)\n        label_path = os.path.join(self.label_dir, label_name)\n\n        # Load image and label\n        image = Image.open(img_path).convert('RGB')\n        label = Image.open(label_path).convert('L')\n        \n        if self.image_transform:\n            image = self.image_transform(image)\n        if self.label_transform:\n            label = self.label_transform(label)\n\n        label = torch.Tensor(np.array(label)) \n\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:25:21.886006Z","iopub.execute_input":"2024-06-07T14:25:21.886871Z","iopub.status.idle":"2024-06-07T14:25:21.900745Z","shell.execute_reply.started":"2024-06-07T14:25:21.886841Z","shell.execute_reply":"2024-06-07T14:25:21.899789Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"# BISENET","metadata":{}},{"cell_type":"code","source":"class ConvBlock(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n                               stride=stride, padding=padding, bias=False)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU()\n\n    def forward(self, input):\n        x = self.conv1(input)\n        return self.relu(self.bn(x))\n\n\nclass Spatial_path(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.convblock1 = ConvBlock(in_channels=3, out_channels=64)\n        self.convblock2 = ConvBlock(in_channels=64, out_channels=128)\n        self.convblock3 = ConvBlock(in_channels=128, out_channels=256)\n\n    def forward(self, input):\n        x = self.convblock1(input)\n        x = self.convblock2(x)\n        x = self.convblock3(x)\n        return x\n\n\nclass AttentionRefinementModule(torch.nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.sigmoid = nn.Sigmoid()\n        self.in_channels = in_channels\n        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n\n    def forward(self, input):\n        # global average pooling\n        x = self.avgpool(input)\n        assert self.in_channels == x.size(1), 'in_channels and out_channels should all be {}'.format(x.size(1))\n        x = self.conv(x)\n        x = self.sigmoid(self.bn(x))\n        # x = self.sigmoid(x)\n        # channels of input and x should be same\n        x = torch.mul(input, x)\n        return x\n\n\nclass FeatureFusionModule(torch.nn.Module):\n    def __init__(self, num_classes, in_channels):\n        super().__init__()\n        # self.in_channels = input_1.channels + input_2.channels\n        # resnet101 3328 = 256(from spatial path) + 1024(from context path) + 2048(from context path)\n        # resnet18  1024 = 256(from spatial path) + 256(from context path) + 512(from context path)\n        self.in_channels = in_channels\n\n        self.convblock = ConvBlock(in_channels=self.in_channels, out_channels=num_classes, stride=1)\n        self.conv1 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n        self.sigmoid = nn.Sigmoid()\n        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n\n    def forward(self, input_1, input_2):\n        x = torch.cat((input_1, input_2), dim=1)\n        assert self.in_channels == x.size(1), 'in_channels of ConvBlock should be {}'.format(x.size(1))\n        feature = self.convblock(x)\n        x = self.avgpool(feature)\n\n        x = self.relu(self.conv1(x))\n        x = self.sigmoid(self.conv2(x))\n        x = torch.mul(feature, x)\n        x = torch.add(x, feature)\n        return x\n\n\nclass BiSeNet(torch.nn.Module):\n    def __init__(self, num_classes, context_path):\n        super().__init__()\n        # build spatial path\n        self.saptial_path = Spatial_path()\n\n        # build context path\n        self.context_path = build_contextpath(name=context_path)\n\n        # build attention refinement module  for resnet 101\n        if context_path == 'resnet101':\n            self.attention_refinement_module1 = AttentionRefinementModule(1024, 1024)\n            self.attention_refinement_module2 = AttentionRefinementModule(2048, 2048)\n            # supervision block\n            self.supervision1 = nn.Conv2d(in_channels=1024, out_channels=num_classes, kernel_size=1)\n            self.supervision2 = nn.Conv2d(in_channels=2048, out_channels=num_classes, kernel_size=1)\n            # build feature fusion module\n            self.feature_fusion_module = FeatureFusionModule(num_classes, 3328)\n\n        elif context_path == 'resnet18':\n            # build attention refinement module  for resnet 18\n            self.attention_refinement_module1 = AttentionRefinementModule(256, 256)\n            self.attention_refinement_module2 = AttentionRefinementModule(512, 512)\n            # supervision block\n            self.supervision1 = nn.Conv2d(in_channels=256, out_channels=num_classes, kernel_size=1)\n            self.supervision2 = nn.Conv2d(in_channels=512, out_channels=num_classes, kernel_size=1)\n            # build feature fusion module\n            self.feature_fusion_module = FeatureFusionModule(num_classes, 1024)\n        else:\n            print('Error: unspport context_path network \\n')\n\n        # build final convolution\n        self.conv = nn.Conv2d(in_channels=num_classes, out_channels=num_classes, kernel_size=1)\n\n        self.init_weight()\n\n        self.mul_lr = []\n        self.mul_lr.append(self.saptial_path)\n        self.mul_lr.append(self.attention_refinement_module1)\n        self.mul_lr.append(self.attention_refinement_module2)\n        self.mul_lr.append(self.supervision1)\n        self.mul_lr.append(self.supervision2)\n        self.mul_lr.append(self.feature_fusion_module)\n        self.mul_lr.append(self.conv)\n\n    def init_weight(self):\n        for name, m in self.named_modules():\n            if 'context_path' not in name:\n                if isinstance(m, nn.Conv2d):\n                    nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n                elif isinstance(m, nn.BatchNorm2d):\n                    m.eps = 1e-5\n                    m.momentum = 0.1\n                    nn.init.constant_(m.weight, 1)\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, input):\n        # output of spatial path\n        sx = self.saptial_path(input)\n\n        # output of context path\n        cx1, cx2, tail = self.context_path(input)\n        cx1 = self.attention_refinement_module1(cx1)\n        cx2 = self.attention_refinement_module2(cx2)\n        cx2 = torch.mul(cx2, tail)\n        # upsampling\n        cx1 = torch.nn.functional.interpolate(cx1, size=sx.size()[-2:], mode='bilinear')\n        cx2 = torch.nn.functional.interpolate(cx2, size=sx.size()[-2:], mode='bilinear')\n        cx = torch.cat((cx1, cx2), dim=1)\n\n        if self.training == True:\n            cx1_sup = self.supervision1(cx1)\n            cx2_sup = self.supervision2(cx2)\n            cx1_sup = torch.nn.functional.interpolate(cx1_sup, size=input.size()[-2:], mode='bilinear')\n            cx2_sup = torch.nn.functional.interpolate(cx2_sup, size=input.size()[-2:], mode='bilinear')\n\n        # output of feature fusion module\n        result = self.feature_fusion_module(sx, cx)\n\n        # upsampling\n        result = torch.nn.functional.interpolate(result, scale_factor=8, mode='bilinear')\n        result = self.conv(result)\n\n        if self.training == True:\n            return result, cx1_sup, cx2_sup\n\n        return result","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:25:21.904773Z","iopub.execute_input":"2024-06-07T14:25:21.905080Z","iopub.status.idle":"2024-06-07T14:25:21.948440Z","shell.execute_reply.started":"2024-06-07T14:25:21.905057Z","shell.execute_reply":"2024-06-07T14:25:21.947587Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"class resnet18(torch.nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = models.resnet18(pretrained=pretrained)\n        self.conv1 = self.features.conv1\n        self.bn1 = self.features.bn1\n        self.relu = self.features.relu\n        self.maxpool1 = self.features.maxpool\n        self.layer1 = self.features.layer1\n        self.layer2 = self.features.layer2\n        self.layer3 = self.features.layer3\n        self.layer4 = self.features.layer4\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.relu(self.bn1(x))\n        x = self.maxpool1(x)\n        feature1 = self.layer1(x)  # 1 / 4\n        feature2 = self.layer2(feature1)  # 1 / 8\n        feature3 = self.layer3(feature2)  # 1 / 16\n        feature4 = self.layer4(feature3)  # 1 / 32\n        # global average pooling to build tail\n        tail = torch.mean(feature4, 3, keepdim=True)\n        tail = torch.mean(tail, 2, keepdim=True)\n        return feature3, feature4, tail\n\n\nclass resnet101(torch.nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = models.resnet101(pretrained=pretrained)\n        self.conv1 = self.features.conv1\n        self.bn1 = self.features.bn1\n        self.relu = self.features.relu\n        self.maxpool1 = self.features.maxpool\n        self.layer1 = self.features.layer1\n        self.layer2 = self.features.layer2\n        self.layer3 = self.features.layer3\n        self.layer4 = self.features.layer4\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.relu(self.bn1(x))\n        x = self.maxpool1(x)\n        feature1 = self.layer1(x)  # 1 / 4\n        feature2 = self.layer2(feature1)  # 1 / 8\n        feature3 = self.layer3(feature2)  # 1 / 16\n        feature4 = self.layer4(feature3)  # 1 / 32\n        # global average pooling to build tail\n        tail = torch.mean(feature4, 3, keepdim=True)\n        tail = torch.mean(tail, 2, keepdim=True)\n        return feature3, feature4, tail\n\n\ndef build_contextpath(name):\n    model = {\n        'resnet18': resnet18(pretrained=True),\n        'resnet101': resnet101(pretrained=True)\n    }\n    return model[name]","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:25:21.949424Z","iopub.execute_input":"2024-06-07T14:25:21.949696Z","iopub.status.idle":"2024-06-07T14:25:21.966490Z","shell.execute_reply.started":"2024-06-07T14:25:21.949665Z","shell.execute_reply":"2024-06-07T14:25:21.965612Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":" # DISCRIMINATOR\n easy copiato da github dagli autori del paper","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass FCDiscriminator(nn.Module):\n\n\tdef __init__(self, num_classes, ndf = 64):\n\t\tsuper(FCDiscriminator, self).__init__()\n\n\t\tself.conv1 = nn.Conv2d(num_classes, ndf, kernel_size=4, stride=2, padding=1)\n\t\tself.conv2 = nn.Conv2d(ndf, ndf*2, kernel_size=4, stride=2, padding=1)\n\t\tself.conv3 = nn.Conv2d(ndf*2, ndf*4, kernel_size=4, stride=2, padding=1)\n\t\tself.conv4 = nn.Conv2d(ndf*4, ndf*8, kernel_size=4, stride=2, padding=1)\n\t\tself.classifier = nn.Conv2d(ndf*8, 1, kernel_size=4, stride=2, padding=1)\n\n\t\tself.leaky_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n\t\t#self.up_sample = nn.Upsample(scale_factor=32, mode='bilinear')\n\t\t#self.sigmoid = nn.Sigmoid()\n\n\n\tdef forward(self, x):\n\t\tx = self.conv1(x)\n\t\tx = self.leaky_relu(x)\n\t\tx = self.conv2(x)\n\t\tx = self.leaky_relu(x)\n\t\tx = self.conv3(x)\n\t\tx = self.leaky_relu(x)\n\t\tx = self.conv4(x)\n\t\tx = self.leaky_relu(x)\n\t\tx = self.classifier(x)\n\t\t#x = self.up_sample(x)\n\t\t#x = self.sigmoid(x) \n\n\t\treturn x","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:25:21.967566Z","iopub.execute_input":"2024-06-07T14:25:21.967944Z","iopub.status.idle":"2024-06-07T14:25:21.981675Z","shell.execute_reply.started":"2024-06-07T14:25:21.967893Z","shell.execute_reply":"2024-06-07T14:25:21.980826Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# TRAINING","metadata":{}},{"cell_type":"code","source":"class CrossEntropy2d(nn.Module):\n\n    def __init__(self, size_average=True, ignore_label=255):\n        super(CrossEntropy2d, self).__init__()\n        self.size_average = size_average\n        self.ignore_label = ignore_label\n\n    def forward(self, predict, target, weight=None):\n        \"\"\"\n            Args:\n                predict:(n, c, h, w)\n                target:(n, h, w)\n                weight (Tensor, optional): a manual rescaling weight given to each class.\n                                           If given, has to be a Tensor of size \"nclasses\"\n        \"\"\"\n        assert not target.requires_grad\n        assert predict.dim() == 4\n        assert target.dim() == 3\n        assert predict.size(0) == target.size(0), \"{0} vs {1} \".format(predict.size(0), target.size(0))\n        assert predict.size(2) == target.size(1), \"{0} vs {1} \".format(predict.size(2), target.size(1))\n        assert predict.size(3) == target.size(2), \"{0} vs {1} \".format(predict.size(3), target.size(3))\n        n, c, h, w = predict.size()\n        target_mask = (target >= 0) * (target != self.ignore_label)\n        target = target[target_mask]\n        if not target.data.dim():\n            return Variable(torch.zeros(1))\n        predict = predict.transpose(1, 2).transpose(2, 3).contiguous()\n        predict = predict[target_mask.view(n, h, w, 1).repeat(1, 1, 1, c)].view(-1, c)\n        loss = F.cross_entropy(predict, target, weight=weight, size_average=self.size_average)\n        return loss","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:25:21.982693Z","iopub.execute_input":"2024-06-07T14:25:21.983016Z","iopub.status.idle":"2024-06-07T14:25:21.994159Z","shell.execute_reply.started":"2024-06-07T14:25:21.982989Z","shell.execute_reply":"2024-06-07T14:25:21.993273Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(config, model, optimizer, train_loss, mIOU, epoch):\n    checkpoint_path = os.path.join(config[\"checkpoint_dir\"], \"checkpoint.pth\")\n    torch.save({\n        'epoch': epoch + 1,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': train_loss,\n        'mIOU': mIOU\n    }, checkpoint_path)\n    print(f\"Checkpoint saved in {checkpoint_path} | Epoch: {epoch}\")\n    \n    \ndef load_checkpoint(config, model, optimizer):\n    if os.path.exists(config[\"checkpoint_dir\"]):\n        checkpoint = torch.load(config[\"checkpoint_dir\"] + \"/checkpoint.pth\")\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        start_epoch = checkpoint['epoch']\n        print(f\"Checkpoint found. Resuming from epoch {start_epoch}.\")\n        return start_epoch\n    else:\n        os.mkdir(config[\"checkpoint_dir\"]) # divide the directory wrt the model (eg. checkpoints/DeepLabV2, checkpoints/BiSeNet)\n        print(\"No checkpoint found. Starting from scratch.\")\n        return 0","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:25:21.995407Z","iopub.execute_input":"2024-06-07T14:25:21.995737Z","iopub.status.idle":"2024-06-07T14:25:22.009476Z","shell.execute_reply.started":"2024-06-07T14:25:21.995708Z","shell.execute_reply":"2024-06-07T14:25:22.008629Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"input_size = [1280, 720]\ninput_size_target = [1024, 512]","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:25:22.010528Z","iopub.execute_input":"2024-06-07T14:25:22.010821Z","iopub.status.idle":"2024-06-07T14:25:22.024741Z","shell.execute_reply.started":"2024-06-07T14:25:22.010797Z","shell.execute_reply":"2024-06-07T14:25:22.023844Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"def loss_calc(pred, label):\n    \"\"\"\n    This function returns cross entropy loss for semantic segmentation\n    \"\"\"\n    # out shape batch_size x channels x h x w -> batch_size x channels x h x w\n    # label shape h x w x 1 x batch_size  -> batch_size x 1 x h x w\n    label = label.long().cuda()\n    criterion = CrossEntropy2d().cuda()\n\n    return criterion(pred, label)\n\ndef lr_poly(base_lr, iter, max_iter, power):\n    return base_lr * ((1 - float(iter) / max_iter) ** (power))\n\ndef adjust_learning_rate(optimizer, iter, max_iter, config):\n    lr = lr_poly(config[\"learning_rate\"], iter, max_iter=1000, power=0.9)\n    optimizer.param_groups[0]['lr'] = lr\n    if len(optimizer.param_groups) > 1:\n        optimizer.param_groups[1]['lr'] = lr * 10","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:41:44.772589Z","iopub.execute_input":"2024-06-07T14:41:44.772973Z","iopub.status.idle":"2024-06-07T14:41:44.781214Z","shell.execute_reply.started":"2024-06-07T14:41:44.772924Z","shell.execute_reply":"2024-06-07T14:41:44.780123Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"def id_processing(targets):\n    targets = targets.cuda()\n    \n    # Define valid indices\n    valid_indices = torch.tensor(list(range(19)) + [255]).to(targets.device)\n\n    # Replace all IDs not in valid_indices with 255\n    processed_targets = torch.where(torch.isin(targets, valid_indices), targets, torch.tensor(255, device=targets.device))\n\n    return processed_targets.long()","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:41:44.999582Z","iopub.execute_input":"2024-06-07T14:41:44.999903Z","iopub.status.idle":"2024-06-07T14:41:45.005386Z","shell.execute_reply.started":"2024-06-07T14:41:44.999876Z","shell.execute_reply":"2024-06-07T14:41:45.004549Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"num_step = 1000\ndef train(model, source_loader, target_loader, criterion, optimizer, config, start_epoch):\n    \n    model.train()\n    model.cuda()\n    \n    model_D1 = FCDiscriminator(num_classes=19)\n    model_D1.cuda()\n\n    source_loader_iter = enumerate(source_loader)\n    target_loader_iter = enumerate(target_loader)  \n    \n    optimizer = optim.SGD(model.parameters(), lr=config[\"learning_rate\"], momentum=config[\"momentum\"], weight_decay=config[\"weight_decay\"])\n    optimizer.zero_grad()\n\n    optimizer_D1 = optim.Adam(model_D1.parameters(), lr=config[\"learning_rate_D\"], betas=(0.9, 0.99))\n    optimizer_D1.zero_grad()\n\n    bce_loss = torch.nn.BCEWithLogitsLoss()\n    \n    interp = nn.Upsample(size=(input_size[1], input_size[0]), mode='bilinear')\n    interp_target = nn.Upsample(size=(input_size_target[1], input_size_target[0]), mode='bilinear')\n\n    source_label = 0\n    target_label = 1\n    \n    for iter in range(num_step):\n\n        loss_seg_value1 = 0 # QUESTA PER LE IMMAGINI PROVENIENTI DAL SOURCE DOMAIN\n        loss_adv_target_value1 = 0 # QUESTA PER QUELLE DEL TARGET DOMAIN\n        loss_D_value1 = 0  # QUESTA PER IL DISCRIMINATORE\n\n        optimizer.zero_grad()\n        adjust_learning_rate(optimizer, iter, num_step, config)\n\n        optimizer_D1.zero_grad()\n        adjust_learning_rate(optimizer_D1, iter, num_step, config)\n\n        for param in model_D1.parameters():\n            param.requires_grad = False\n\n        # train G\n        # train with source\n        _, batch = next(source_loader_iter)\n        images, labels = batch\n        images, labels = images.cuda(), id_processing(labels).cuda()\n\n        pred1 = model(images)\n        pred1 = interp(pred1[0])\n\n        loss_seg1 = loss_calc(pred1, labels)\n        loss_seg1.backward()\n        loss_seg_value1 += loss_seg1.data.cpu().numpy()\n\n        # train with target\n        _, batch = next(target_loader_iter)\n        images, _ = batch\n        images = images.cuda()\n\n        pred_target1 = model(images)\n        pred_target1 = interp_target(pred_target1[0])\n\n        D_out1 = model_D1(F.softmax(pred_target1))\n        loss_adv_target1 = bce_loss(D_out1, torch.FloatTensor(D_out1.data.size()).fill_(source_label).cuda())\n\n        loss = loss_adv_target1\n        loss.backward()\n\n        loss_adv_target_value1 += loss_adv_target1.data.cpu().numpy()\n\n        # train D\n        # train with source\n        for param in model_D1.parameters():\n            param.requires_grad = True\n\n        pred1 = pred1.detach()\n\n        D_out1 = model_D1(F.softmax(pred1))\n\n        loss_D1 = bce_loss(D_out1, torch.FloatTensor(D_out1.data.size()).fill_(source_label).cuda())\n\n        loss_D1 = loss_D1 / 2\n\n        loss_D1.backward()\n\n        loss_D_value1 += loss_D1.data.cpu().numpy()\n\n        # train with target\n        pred_target1 = pred_target1.detach()\n\n        D_out1 = model_D1(F.softmax(pred_target1))\n\n        loss_D1 = bce_loss(D_out1, torch.FloatTensor(D_out1.data.size()).fill_(target_label).cuda())\n\n        loss_D1 = loss_D1 / 2\n\n        loss_D1.backward()\n\n        loss_D_value1 += loss_D1.data.cpu().numpy()\n\n        optimizer.step()\n        optimizer_D1.step()\n\n    print('loss_seg1 = {2:.3f} loss_adv1 = {4:.3f}, loss_D1 = {4:.3f}'.format(\n    loss_seg_value1, loss_adv_target_value1, loss_D_value1))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:41:45.164241Z","iopub.execute_input":"2024-06-07T14:41:45.164530Z","iopub.status.idle":"2024-06-07T14:41:45.182558Z","shell.execute_reply.started":"2024-06-07T14:41:45.164506Z","shell.execute_reply":"2024-06-07T14:41:45.181614Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"def mean_iou(num_classes, pred, target):\n    mIOU = 0\n    for i in range(len(pred)):\n        print(len(target[i].cpu().numpy()), len(pred[i].cpu().numpy()))\n        hist = fast_hist(target[i].cpu().numpy(),pred[i], num_classes)\n        IOU = per_class_iou(hist)\n        mIOU = mIOU + sum(IOU)/num_classes\n    return mIOU\n\ndef fast_hist(a, b, n):\n    \"\"\"\n    a and b are predict and mask respectively\n    n is the number of classes\n    \"\"\"\n    k = (a >= 0) & (a < n) #assign True if the value is in the range between 0 and 18 (class labels)\n    return np.bincount(n * a[k].astype(int) + b[k], minlength=n ** 2).reshape((n, n))\n\ndef per_class_iou(hist):\n    epsilon = 1e-5\n    return (np.diag(hist)) / (hist.sum(1) + hist.sum(0) - np.diag(hist) + epsilon)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:41:45.337344Z","iopub.execute_input":"2024-06-07T14:41:45.337915Z","iopub.status.idle":"2024-06-07T14:41:45.346112Z","shell.execute_reply.started":"2024-06-07T14:41:45.337887Z","shell.execute_reply":"2024-06-07T14:41:45.345236Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"# TESTING\nSu Cityscapes, è regolare","metadata":{}},{"cell_type":"code","source":"palette = [128, 64, 128, 244, 35, 232, 70, 70, 70, 102, 102, 156, 190, 153, 153, 153, 153, 153, 250, 170, 30,\n           220, 220, 0, 107, 142, 35, 152, 251, 152, 70, 130, 180, 220, 20, 60, 255, 0, 0, 0, 0, 142, 0, 0, 70,\n           0, 60, 100, 0, 80, 100, 0, 0, 230, 119, 11, 32]\nzero_pad = 256 * 3 - len(palette)\nfor i in range(zero_pad):\n    palette.append(0)\n\n\ndef colorize_mask(mask):\n    # mask: numpy array of the mask\n    new_mask = Image.fromarray(mask.astype(np.uint8)).convert('P')\n    new_mask.putpalette(palette)\n\n    return new_mask","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:41:45.706920Z","iopub.execute_input":"2024-06-07T14:41:45.707710Z","iopub.status.idle":"2024-06-07T14:41:45.714811Z","shell.execute_reply.started":"2024-06-07T14:41:45.707682Z","shell.execute_reply":"2024-06-07T14:41:45.713800Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"def vale(model, val_loader):\n    \n    model.eval()\n    model.cuda()\n\n    interp = nn.Upsample(size=(1024, 2048), mode='bilinear')\n    \n    with torch.no_grad():\n        for _, (inputs, targets) in enumerate(val_loader):\n            image, label = inputs.cuda(), id_processing(targets).cuda()\n\n            output = model(image)\n            output = interp(output).cpu().data[0].numpy()\n            output = output.transpose(1,2,0)\n            output = np.asarray(np.argmax(output, axis=2), dtype=np.uint8)\n            \n            output_col = colorize_mask(output)\n            output = Image.fromarray(output)\n            \n            plt.figure(figsize=(10, 5))\n            plt.subplot(1, 2, 1)\n            plt.title('Original Image')\n            plt.imshow(image.cpu().numpy()[0].transpose(1, 2, 0).astype(np.uint8))\n            \n            plt.subplot(1, 2, 2)\n            plt.title('Predicted Mask')\n            plt.imshow(output_col)\n            plt.show()\n            \n","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:41:45.883495Z","iopub.execute_input":"2024-06-07T14:41:45.884228Z","iopub.status.idle":"2024-06-07T14:41:45.893263Z","shell.execute_reply.started":"2024-06-07T14:41:45.884200Z","shell.execute_reply":"2024-06-07T14:41:45.892297Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"def val(model, val_loader):\n    \n    model.eval()\n    model.cuda()\n    \n    interp = nn.Upsample(size=(1024, 2048), mode='bilinear')\n\n    with torch.no_grad():\n        for _, (inputs, targets) in enumerate(val_loader):\n            image, label = inputs.cuda(), id_processing(targets).cuda()\n\n            output = model(image)\n            print(\"FIRST\", output.shape)\n            output = interp(output).cpu().data[0].numpy()\n            print(\"SECOND\", output.shape)\n            predicted = output.max(1)\n            running_mIOU = mean_iou(len(output), predicted, targets)\n            total_mIOU += running_mIOU.sum().item()\n            total_images += len(predicted)\n        \n    mIOU = total_mIOU/total_images\n    \n    print(f'\\n\\nmIoU: {(mIOU*100):.3f}%')","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:41:46.096298Z","iopub.execute_input":"2024-06-07T14:41:46.096590Z","iopub.status.idle":"2024-06-07T14:41:46.105624Z","shell.execute_reply.started":"2024-06-07T14:41:46.096565Z","shell.execute_reply":"2024-06-07T14:41:46.104617Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"! rm -r checkpoints/\n# best configuration (TO CONFIGURE)\nconfig = dict(\n    epochs=20,\n    batch_size=2,\n    learning_rate=0.001,\n    learning_rate_D=0.001,\n    momentum=0.9,\n    weight_decay=5e-4,\n    architecture=\"BiSeNet\",\n    checkpoint_dir=\"/kaggle/working/checkpoints\" )\nif torch.cuda.is_available():\n    print(\"Building the model with the best configuration\")\n    # Build, train and analyze the model with the pipeline\n    model = model_pipeline(config)\nelse:\n    print(\"CUDA is Not available\")","metadata":{"execution":{"iopub.status.busy":"2024-06-07T14:41:46.258183Z","iopub.execute_input":"2024-06-07T14:41:46.258473Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Building the model with the best configuration\nNo checkpoint found. Starting from scratch.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}