{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8412108,"sourceType":"datasetVersion","datasetId":5006778},{"sourceId":8465917,"sourceType":"datasetVersion","datasetId":5047290},{"sourceId":8495419,"sourceType":"datasetVersion","datasetId":5069095},{"sourceId":8412080,"sourceType":"datasetVersion","datasetId":5006755},{"sourceId":8801225,"sourceType":"datasetVersion","datasetId":5292696}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Discriminator\nFor the discriminator, we use an architecture similar to [30] but utilize all fully-convolutional layers to retain the spatial information. The network consists of 5 convolution layers with kernel 4 × 4 and stride of 2, where the channel number is {64, 128, 256, 512, 1}, respectively. Except for the last layer, each convolution layer is followed by a leaky ReLU [27] parameterized by 0.2. An up-sampling layer is added to the last convolution layer for re-scaling the output to the size of the input. We do not use any batch-normalization layers [16] as we jointly train the discriminator with the segmentation network using a small batch size.\n# Segmentation Network\nIt is essential to build upon a good baseline model to achieve high-quality segmentation results\n[2, 38, 40]. We adopt the DeepLab-v2 [2] framework with ResNet-101 [11] model pre-trained on ImageNet [6] as our segmentation baseline network. However, we do not use the multi-scale fusion strategy [2] due to the memory issue.\nSimilar to the recent work on semantic segmentation [2, 38], we remove the last classification layer and modify the stride\nof the last two convolution layers from 2 to 1, making the resolution of the output feature maps effectively 1/8 times the input image size. To enlarge the receptive field, we apply dilated convolution layers [38] in conv4 and conv5 layers with a stride of 2 and 4, respectively. After the last layer, we use the Atrous Spatial Pyramid Pooling (ASPP) [2] as the final classifier. Finally, we apply an up-sampling layer along with the softmax output to match the size of the input image. Based on this architecture, our segmentation model achieves 65.1% mean intersection-over-union (IoU) whentrained on the Cityscapes [4] training set and tested on the Cityscapes validation set.\n### Multi-level Adaptation Model\nWe construct the abovementioned discriminator and segmentation network as our ingle-level adaptation model. For the multi-level structure, we extract feature maps from the conv4 layer and add an ASPP module as the auxiliary classifier. Similarly, a discriminator with the same architecture is added for adversarial learning. Figure 2 shows the proposed multi-level adaptation model. In this paper, we use two levels due to the balance of its efficiency and accuracy.\n# Network Training. \nTo train the proposed single/multi-level adaptation model, we find that jointly training the segmentation network and discriminators in one stage is effective.\nIn each training batch, we first forward the source image Is to optimize the segmentation network for Lseg in (3) and generate the output Ps. For the target image It, we obtain the segmentation output Pt, and pass it along with Ps to the discriminator for optimizing Ld in (2). In addition, we compute the adversarial loss Ladv in (4) for the target prediction\nPt. For the multi-level training objective in (5), we simply repeat the same procedure for each adaptation module.\nTo train the segmentation network, we use the Stochastic Gradient Descent (SGD) optimizer with Nesterov acceleration where\nthe momentum is 0.9 and the weight decay is 10−4. The initial learning rate is set as 2.5 × 10−4 and is decreased using the polynomial decay with power of 0.9 as mentioned in [2]. For training the discriminator, we use the Adam optimizer [18] with the learning rate as 10−4 and the same polynomial decay as the segmentation network. The momentum is set as 0.9 and 0.99.\n","metadata":{}},{"cell_type":"markdown","source":"# REPO GITHUB\n[https://github.com/wasidennis/AdaptSegNet/tree/master](http://)\n\nhttps://github.com/hfslyc/AdvSemiSeg/tree/master","metadata":{}},{"cell_type":"code","source":"!pip install -U fvcore","metadata":{"execution":{"iopub.status.busy":"2024-07-03T08:09:52.914850Z","iopub.execute_input":"2024-07-03T08:09:52.915304Z","iopub.status.idle":"2024-07-03T08:10:12.213171Z","shell.execute_reply.started":"2024-07-03T08:09:52.915274Z","shell.execute_reply":"2024-07-03T08:10:12.211986Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting fvcore\n  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m795.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fvcore) (1.26.4)\nCollecting yacs>=0.1.6 (from fvcore)\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from fvcore) (6.0.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from fvcore) (4.66.1)\nRequirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.10/site-packages (from fvcore) (2.4.0)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from fvcore) (9.5.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from fvcore) (0.9.0)\nCollecting iopath>=0.1.7 (from fvcore)\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from iopath>=0.1.7->fvcore) (4.9.0)\nCollecting portalocker (from iopath>=0.1.7->fvcore)\n  Downloading portalocker-2.10.0-py3-none-any.whl.metadata (8.5 kB)\nDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nDownloading portalocker-2.10.0-py3-none-any.whl (18 kB)\nBuilding wheels for collected packages: fvcore, iopath\n  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=9438b26620924ace61eac3047dfe74a84c007dc2b9ed2a7bd2b9bb127ef3c1b9\n  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=3ac4ca83233ff93ac47afba886b698069dea09003383e4147a65e4f4eda9dcec\n  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\nSuccessfully built fvcore iopath\nInstalling collected packages: yacs, portalocker, iopath, fvcore\nSuccessfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-2.10.0 yacs-0.1.8\n","output_type":"stream"}]},{"cell_type":"code","source":"# If you run the model for the first time remove all the previus checkpoints\n! rm -r checkpoints/","metadata":{"execution":{"iopub.status.busy":"2024-06-27T13:27:11.456819Z","iopub.execute_input":"2024-06-27T13:27:11.457128Z","iopub.status.idle":"2024-06-27T13:27:12.406552Z","shell.execute_reply.started":"2024-06-27T13:27:11.457099Z","shell.execute_reply":"2024-06-27T13:27:12.405651Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"rm: cannot remove 'checkpoints/': No such file or directory\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# IMPORT","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport os\nimport zipfile\nimport numpy as np\nimport time\nfrom PIL import Image\nimport albumentations as A\n\nfrom fvcore.nn import FlopCountAnalysis, flop_count_table\n\nimport warnings\nwarnings.filterwarnings(action='ignore')","metadata":{"execution":{"iopub.status.busy":"2024-07-03T08:10:12.215168Z","iopub.execute_input":"2024-07-03T08:10:12.215473Z","iopub.status.idle":"2024-07-03T08:10:18.127694Z","shell.execute_reply.started":"2024-07-03T08:10:12.215446Z","shell.execute_reply":"2024-07-03T08:10:18.126843Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# MODEL PIPELINE","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndef model_pipeline(config=None):\n\n    # make the model, data, and optimization problem\n    model, source_loader, target_loader, val_loader, criterion, optimizer, start_epoch = make(config)\n    \n    model = torch.nn.DataParallel(model).cuda()\n    \n    # and use them to train the model\n    train(model, source_loader, target_loader, criterion, optimizer, config, start_epoch)\n\n    # and test its final performance\n    val(model, val_loader)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-07-03T08:10:18.128718Z","iopub.execute_input":"2024-07-03T08:10:18.129093Z","iopub.status.idle":"2024-07-03T08:10:18.186970Z","shell.execute_reply.started":"2024-07-03T08:10:18.129069Z","shell.execute_reply":"2024-07-03T08:10:18.185791Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# DATASET","metadata":{}},{"cell_type":"code","source":"def make(config):\n    # Make the data\n    (source, target) , test = get_data(train=True), get_data(train=False)\n    source_loader = make_loader(source, batch_size=config[\"batch_size\"],train=True)\n    target_loader = make_loader(target, batch_size=config[\"batch_size\"],train=True)\n    test_loader = make_loader(test, batch_size=config[\"batch_size\"],train=False)\n\n    # Make the model (BiSeNet with ResNet-18 backbone)\n    model = build_model(model_type='BiSeNet').cuda()\n\n    # Make the loss and optimizer\n    optimizer = optim.SGD(model.parameters(), \n                          lr=config[\"learning_rate\"], \n                          momentum=config[\"momentum\"], \n                          weight_decay=config[\"weight_decay\"])\n    \n    criterion = torch.nn.CrossEntropyLoss(ignore_index=255)\n    \n    # Load the last checkpoint\n    start_epoch = load_checkpoint(config, model, optimizer)\n    \n    return model, source_loader, target_loader, test_loader, criterion, optimizer, start_epoch","metadata":{"execution":{"iopub.status.busy":"2024-07-03T08:10:18.189177Z","iopub.execute_input":"2024-07-03T08:10:18.189485Z","iopub.status.idle":"2024-07-03T08:10:18.197856Z","shell.execute_reply.started":"2024-07-03T08:10:18.189455Z","shell.execute_reply":"2024-07-03T08:10:18.196987Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Define transforms for preprocessing\ntransform_cityscapes = A.Compose([\n    A.Resize(height=512, width=1024),\n])\ntransform_gta5 = A.Compose([\n    A.Resize(height=720, width=1280)\n])\n\n# GTA5 for train and CityScapes for test\ncitiyscapes_dir ='/kaggle/input/cityscapes/Cityscapes/Cityspaces'\n#gta_dir = '/kaggle/input/gta5-dataset/GTA5'\ngta_dir = '/kaggle/input/gta5-with-mask/GTA5_with_mask/'\n\ndef get_data(train=True):\n    if train == True:\n        # train dataset\n        source_dataset = GTA5(root_dir=gta_dir, transform=transform_gta5)\n        target_dataset = CityScapes(root_dir=citiyscapes_dir, split='train',transform=transform_cityscapes)\n        return source_dataset, target_dataset\n    else:\n        # test dataset\n        dataset = CityScapes(root_dir=citiyscapes_dir, split='val', transform=transform_cityscapes)\n        return dataset\n\n\ndef make_loader(dataset, batch_size = 8, train=True):\n    if train == True:\n        # train dataloader\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n    else:\n        # test dataloader\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False,drop_last=True)\n    \n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2024-07-03T08:10:18.198981Z","iopub.execute_input":"2024-07-03T08:10:18.199301Z","iopub.status.idle":"2024-07-03T08:10:18.209341Z","shell.execute_reply.started":"2024-07-03T08:10:18.199270Z","shell.execute_reply":"2024-07-03T08:10:18.208568Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# TO IMPLEMENT\ndef build_model(model_type):\n    if model_type == 'BiSeNet':\n        return BiSeNet(num_classes=19, context_path=\"resnet18\")\n    elif model_type == 'DeepLabV2':\n        pretrain_model_path = '/kaggle/input/model-weight/deeplab_resnet_pretrained_imagenet.pth'\n        return get_deeplab_v2(num_classes=19, pretrain=True, pretrain_model_path=pretrain_model_path)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T08:10:18.210356Z","iopub.execute_input":"2024-07-03T08:10:18.210694Z","iopub.status.idle":"2024-07-03T08:10:18.220841Z","shell.execute_reply.started":"2024-07-03T08:10:18.210664Z","shell.execute_reply":"2024-07-03T08:10:18.220070Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport torch\nfrom PIL import Image\nimport numpy as np\nimport os\nfrom typing import Optional, Tuple\nfrom albumentations import Compose\n\nclass CityScapes(Dataset):\n    \n    \"\"\"\n    _summary_\n    \"\"\"\n    def __init__(self, \n                 root_dir:str, \n                 split:str = 'train', \n                 transform: Optional[Compose] = None):\n        super(CityScapes, self).__init__()\n        \n        \"\"\"\n        \n        _summary\n        \n        Args:\n            root_dir (string): Directory with all the images and annotations.\n            split (string): 'train' or 'val'.\n            image_transform (callable, optional): Optional transform to be applied on a sample image.\n            label_transform (callable, optional): Optional transform to be applied on a sample label.\n        \"\"\"\n        self.root_dir = root_dir\n        self.split = split\n        self.transform = transform\n        \n        # Load the data\n        self.data = []\n        path = os.path.join(self.root_dir, 'images', split)\n        for city in os.listdir(path):\n            images = os.path.join(path, city)\n            for image in os.listdir(images):\n                image = os.path.join(images, image)\n                label = image.replace('images', 'gtFine').replace('_leftImg8bit','_gtFine_labelTrainIds')\n                self.data.append((image, label))\n\n    def __len__(self)->int:\n        \n        \"\"\"\n        \n        _summary\n        \n        Returns:\n            int: _description_\n        \"\"\"\n        \n        return len(self.data)\n\n    def __getitem__(self, idx:int)-> Tuple[torch.Tensor, torch.Tensor]:\n        \n        \"\"\"\n        \n        _summary\n        \n        Args:\n            idx (int): _description_\n\n        Returns:\n            tuple[torch.Tensor, torch.Tensor]: _description_\n        \"\"\"\n        \n        image_path, label_path = self.data[idx]\n\n        # Load image and label\n        image = Image.open(image_path).convert('RGB')\n        label = Image.open(label_path).convert('L')\n        image, label = np.array(image), np.array(label)\n        \n        if self.transform:\n            transformed = self.transform(image=image, mask=label)\n            image, label = transformed['image'], transformed['mask']\n\n        image = torch.from_numpy(image).permute(2, 0, 1).float()/255\n        label = torch.from_numpy(label).long()\n        \n        return image, label","metadata":{"execution":{"iopub.status.busy":"2024-07-03T08:10:18.221894Z","iopub.execute_input":"2024-07-03T08:10:18.222146Z","iopub.status.idle":"2024-07-03T08:10:18.237473Z","shell.execute_reply.started":"2024-07-03T08:10:18.222125Z","shell.execute_reply":"2024-07-03T08:10:18.236725Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def get_color_to_id() -> dict:\n    \"\"\"\n    Returns a dictionary mapping RGB color tuples to their corresponding class IDs.\n\n    Returns:\n        dict: A dictionary where keys are RGB color tuples and values are class IDs.\n    \"\"\"\n    id_to_color = get_id_to_color()\n    color_to_id = {color: id for id, color in id_to_color.items()}\n    return color_to_id\n\ndef get_id_to_color() -> dict:\n    \"\"\"\n    Returns a dictionary mapping class IDs to their corresponding colors.\n\n    Returns:\n        dict: A dictionary where keys are class IDs and values are RGB color tuples.\n    \"\"\"\n    return {\n        0: (128, 64, 128),    # road\n        1: (244, 35, 232),    # sidewalk\n        2: (70, 70, 70),      # building\n        3: (102, 102, 156),   # wall\n        4: (190, 153, 153),   # fence\n        5: (153, 153, 153),   # pole\n        6: (250, 170, 30),    # light\n        7: (220, 220, 0),     # sign\n        8: (107, 142, 35),    # vegetation\n        9: (152, 251, 152),   # terrain\n        10: (70, 130, 180),   # sky\n        11: (220, 20, 60),    # person\n        12: (255, 0, 0),      # rider\n        13: (0, 0, 142),      # car\n        14: (0, 0, 70),       # truck\n        15: (0, 60, 100),     # bus\n        16: (0, 80, 100),     # train\n        17: (0, 0, 230),      # motorcycle\n        18: (119, 11, 32),    # bicycle\n    }","metadata":{"execution":{"iopub.status.busy":"2024-07-03T08:10:18.238364Z","iopub.execute_input":"2024-07-03T08:10:18.238690Z","iopub.status.idle":"2024-07-03T08:10:18.250424Z","shell.execute_reply.started":"2024-07-03T08:10:18.238667Z","shell.execute_reply":"2024-07-03T08:10:18.249578Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport torch\nfrom PIL import Image\nimport numpy as np\nimport os\nfrom typing import Optional, Tuple\n# from utils import get_color_to_id\n\n\nclass GTA5(Dataset):\n    \n    \"\"\"\n    _summary_    \n    \"\"\"\n    \n    def __init__(self, \n                 root_dir:str,\n                 compute_mask:bool=False,\n                 transform: Optional[Compose] = None):\n        super(GTA5, self).__init__()\n        \n        \"\"\"\n        Args:\n            root_dir (string): Directory with all the images and annotations.\n            transform (callable, optional): Optional transform to be applied on a sample.\n        \"\"\"\n\n        self.root_dir = root_dir\n        self.compute_mask = compute_mask\n        self.transform = transform\n        if self.compute_mask:\n            self.color_to_id = get_color_to_id()\n        \n        # Load the data\n        self.data = []\n        image_dir = os.path.join(self.root_dir, 'images')\n        \n        if self.compute_mask:\n            label_dir = os.path.join(self.root_dir, 'labels')\n        else:\n            label_dir = os.path.join(self.root_dir, 'masks')\n            \n        for filename in os.listdir(image_dir):\n            image = os.path.join(image_dir, filename)\n            label = os.path.join(label_dir, filename)\n            self.data.append((image, label))\n        \n    def __len__(self)->int:\n        \n        \"\"\"_summary_\n\n        Returns:\n            int: _description_\n        \"\"\"\n        \n        return len(self.data)\n\n    def __getitem__(self, idx:int)-> Tuple[torch.Tensor,torch.Tensor]:\n        \n        \"\"\"_summary_\n\n        Args:\n            idx (int): _description_\n\n        Returns:\n            Tuple[torch.Tensor,torch.Tensor]: _description_\n        \"\"\"\n        \n        image_path, label_path = self.data[idx]\n\n        # Load images and labels or masks\n        image = Image.open(image_path).convert('RGB')\n        \n        if self.compute_mask:\n            label = self._rgb_to_label(Image.open(label_path).convert('RGB'))\n        else:\n            label = Image.open(label_path).convert('L')\n            \n        image, label = np.array(image), np.array(label)\n        \n        if self.transform:\n            transformed = self.transform(image=image, mask=label)\n            image, label = transformed['image'], transformed['mask']\n\n        image = torch.from_numpy(image).permute(2, 0, 1).float()/255\n        label = torch.from_numpy(label).long()\n        return image, label\n    \n    def _rgb_to_label(self, image:Image.Image)->np.ndarray:\n        \"\"\"_summary_\n\n        Args:\n            image (Image.Image): _description_\n\n        Returns:\n            np.ndarray: _description_\n        \"\"\"\n        \n        gray_image = Image.new('L', image.size)\n        rgb_pixels = image.load()\n        gray_pixels = gray_image.load()\n        \n        for i in range(image.width):\n            for j in range(image.height):\n                rgb = rgb_pixels[i,j]\n                gray_pixels[i,j] = self.color_to_id.get(rgb,255)\n                \n        return gray_image","metadata":{"execution":{"iopub.status.busy":"2024-07-03T08:10:18.251789Z","iopub.execute_input":"2024-07-03T08:10:18.252150Z","iopub.status.idle":"2024-07-03T08:10:18.269131Z","shell.execute_reply.started":"2024-07-03T08:10:18.252122Z","shell.execute_reply":"2024-07-03T08:10:18.268283Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# BISENET","metadata":{}},{"cell_type":"code","source":"class ConvBlock(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n                               stride=stride, padding=padding, bias=False)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU()\n\n    def forward(self, input):\n        x = self.conv1(input)\n        return self.relu(self.bn(x))\n\n\nclass Spatial_path(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.convblock1 = ConvBlock(in_channels=3, out_channels=64)\n        self.convblock2 = ConvBlock(in_channels=64, out_channels=128)\n        self.convblock3 = ConvBlock(in_channels=128, out_channels=256)\n\n    def forward(self, input):\n        x = self.convblock1(input)\n        x = self.convblock2(x)\n        x = self.convblock3(x)\n        return x\n\n\nclass AttentionRefinementModule(torch.nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.sigmoid = nn.Sigmoid()\n        self.in_channels = in_channels\n        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n\n    def forward(self, input):\n        # global average pooling\n        x = self.avgpool(input)\n        assert self.in_channels == x.size(1), 'in_channels and out_channels should all be {}'.format(x.size(1))\n        x = self.conv(x)\n        x = self.sigmoid(self.bn(x))\n        # x = self.sigmoid(x)\n        # channels of input and x should be same\n        x = torch.mul(input, x)\n        return x\n\n\nclass FeatureFusionModule(torch.nn.Module):\n    def __init__(self, num_classes, in_channels):\n        super().__init__()\n        # self.in_channels = input_1.channels + input_2.channels\n        # resnet101 3328 = 256(from spatial path) + 1024(from context path) + 2048(from context path)\n        # resnet18  1024 = 256(from spatial path) + 256(from context path) + 512(from context path)\n        self.in_channels = in_channels\n\n        self.convblock = ConvBlock(in_channels=self.in_channels, out_channels=num_classes, stride=1)\n        self.conv1 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n        self.sigmoid = nn.Sigmoid()\n        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n\n    def forward(self, input_1, input_2):\n        x = torch.cat((input_1, input_2), dim=1)\n        assert self.in_channels == x.size(1), 'in_channels of ConvBlock should be {}'.format(x.size(1))\n        feature = self.convblock(x)\n        x = self.avgpool(feature)\n\n        x = self.relu(self.conv1(x))\n        x = self.sigmoid(self.conv2(x))\n        x = torch.mul(feature, x)\n        x = torch.add(x, feature)\n        return x\n\n\nclass BiSeNet(torch.nn.Module):\n    def __init__(self, num_classes, context_path):\n        super().__init__()\n        # build spatial path\n        self.saptial_path = Spatial_path()\n\n        # build context path\n        self.context_path = build_contextpath(name=context_path)\n\n        # build attention refinement module  for resnet 101\n        if context_path == 'resnet101':\n            self.attention_refinement_module1 = AttentionRefinementModule(1024, 1024)\n            self.attention_refinement_module2 = AttentionRefinementModule(2048, 2048)\n            # supervision block\n            self.supervision1 = nn.Conv2d(in_channels=1024, out_channels=num_classes, kernel_size=1)\n            self.supervision2 = nn.Conv2d(in_channels=2048, out_channels=num_classes, kernel_size=1)\n            # build feature fusion module\n            self.feature_fusion_module = FeatureFusionModule(num_classes, 3328)\n\n        elif context_path == 'resnet18':\n            # build attention refinement module  for resnet 18\n            self.attention_refinement_module1 = AttentionRefinementModule(256, 256)\n            self.attention_refinement_module2 = AttentionRefinementModule(512, 512)\n            # supervision block\n            self.supervision1 = nn.Conv2d(in_channels=256, out_channels=num_classes, kernel_size=1)\n            self.supervision2 = nn.Conv2d(in_channels=512, out_channels=num_classes, kernel_size=1)\n            # build feature fusion module\n            self.feature_fusion_module = FeatureFusionModule(num_classes, 1024)\n        else:\n            print('Error: unspport context_path network \\n')\n\n        # build final convolution\n        self.conv = nn.Conv2d(in_channels=num_classes, out_channels=num_classes, kernel_size=1)\n\n        self.init_weight()\n\n        self.mul_lr = []\n        self.mul_lr.append(self.saptial_path)\n        self.mul_lr.append(self.attention_refinement_module1)\n        self.mul_lr.append(self.attention_refinement_module2)\n        self.mul_lr.append(self.supervision1)\n        self.mul_lr.append(self.supervision2)\n        self.mul_lr.append(self.feature_fusion_module)\n        self.mul_lr.append(self.conv)\n\n    def init_weight(self):\n        for name, m in self.named_modules():\n            if 'context_path' not in name:\n                if isinstance(m, nn.Conv2d):\n                    nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n                elif isinstance(m, nn.BatchNorm2d):\n                    m.eps = 1e-5\n                    m.momentum = 0.1\n                    nn.init.constant_(m.weight, 1)\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, input):\n        # output of spatial path\n        sx = self.saptial_path(input)\n\n        # output of context path\n        cx1, cx2, tail = self.context_path(input)\n        cx1 = self.attention_refinement_module1(cx1)\n        cx2 = self.attention_refinement_module2(cx2)\n        cx2 = torch.mul(cx2, tail)\n        # upsampling\n        cx1 = torch.nn.functional.interpolate(cx1, size=sx.size()[-2:], mode='bilinear')\n        cx2 = torch.nn.functional.interpolate(cx2, size=sx.size()[-2:], mode='bilinear')\n        cx = torch.cat((cx1, cx2), dim=1)\n\n        if self.training == True:\n            cx1_sup = self.supervision1(cx1)\n            cx2_sup = self.supervision2(cx2)\n            cx1_sup = torch.nn.functional.interpolate(cx1_sup, size=input.size()[-2:], mode='bilinear')\n            cx2_sup = torch.nn.functional.interpolate(cx2_sup, size=input.size()[-2:], mode='bilinear')\n\n        # output of feature fusion module\n        result = self.feature_fusion_module(sx, cx)\n\n        # upsampling\n        result = torch.nn.functional.interpolate(result, scale_factor=8, mode='bilinear')\n        result = self.conv(result)\n\n        if self.training == True:\n            return result, cx1_sup, cx2_sup\n\n        return result","metadata":{"execution":{"iopub.status.busy":"2024-07-03T08:10:18.272508Z","iopub.execute_input":"2024-07-03T08:10:18.272796Z","iopub.status.idle":"2024-07-03T08:10:18.304802Z","shell.execute_reply.started":"2024-07-03T08:10:18.272767Z","shell.execute_reply":"2024-07-03T08:10:18.303870Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class resnet18(torch.nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = models.resnet18(pretrained=pretrained)\n        self.conv1 = self.features.conv1\n        self.bn1 = self.features.bn1\n        self.relu = self.features.relu\n        self.maxpool1 = self.features.maxpool\n        self.layer1 = self.features.layer1\n        self.layer2 = self.features.layer2\n        self.layer3 = self.features.layer3\n        self.layer4 = self.features.layer4\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.relu(self.bn1(x))\n        x = self.maxpool1(x)\n        feature1 = self.layer1(x)  # 1 / 4\n        feature2 = self.layer2(feature1)  # 1 / 8\n        feature3 = self.layer3(feature2)  # 1 / 16\n        feature4 = self.layer4(feature3)  # 1 / 32\n        # global average pooling to build tail\n        tail = torch.mean(feature4, 3, keepdim=True)\n        tail = torch.mean(tail, 2, keepdim=True)\n        return feature3, feature4, tail\n\n\nclass resnet101(torch.nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = models.resnet101(pretrained=pretrained)\n        self.conv1 = self.features.conv1\n        self.bn1 = self.features.bn1\n        self.relu = self.features.relu\n        self.maxpool1 = self.features.maxpool\n        self.layer1 = self.features.layer1\n        self.layer2 = self.features.layer2\n        self.layer3 = self.features.layer3\n        self.layer4 = self.features.layer4\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.relu(self.bn1(x))\n        x = self.maxpool1(x)\n        feature1 = self.layer1(x)  # 1 / 4\n        feature2 = self.layer2(feature1)  # 1 / 8\n        feature3 = self.layer3(feature2)  # 1 / 16\n        feature4 = self.layer4(feature3)  # 1 / 32\n        # global average pooling to build tail\n        tail = torch.mean(feature4, 3, keepdim=True)\n        tail = torch.mean(tail, 2, keepdim=True)\n        return feature3, feature4, tail\n\n\ndef build_contextpath(name):\n    model = {\n        'resnet18': resnet18(pretrained=True),\n        'resnet101': resnet101(pretrained=True)\n    }\n    return model[name]","metadata":{"execution":{"iopub.status.busy":"2024-07-03T08:10:18.305875Z","iopub.execute_input":"2024-07-03T08:10:18.306174Z","iopub.status.idle":"2024-07-03T08:10:18.320561Z","shell.execute_reply.started":"2024-07-03T08:10:18.306152Z","shell.execute_reply":"2024-07-03T08:10:18.319726Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":" # DISCRIMINATOR\n easy copiato da github dagli autori del paper","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass FCDiscriminator(nn.Module):\n\n\tdef __init__(self, num_classes, ndf = 64):\n\t\tsuper(FCDiscriminator, self).__init__()\n\n\t\tself.conv1 = nn.Conv2d(num_classes, ndf, kernel_size=4, stride=2, padding=1)\n\t\tself.conv2 = nn.Conv2d(ndf, ndf*2, kernel_size=4, stride=2, padding=1)\n\t\tself.conv3 = nn.Conv2d(ndf*2, ndf*4, kernel_size=4, stride=2, padding=1)\n\t\tself.conv4 = nn.Conv2d(ndf*4, ndf*8, kernel_size=4, stride=2, padding=1)\n\t\tself.classifier = nn.Conv2d(ndf*8, 1, kernel_size=4, stride=2, padding=1)\n\n\t\tself.leaky_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n\t\t#self.up_sample = nn.Upsample(scale_factor=32, mode='bilinear')\n\t\t#self.sigmoid = nn.Sigmoid()\n\n\n\tdef forward(self, x):\n\t\tx = self.conv1(x)\n\t\tx = self.leaky_relu(x)\n\t\tx = self.conv2(x)\n\t\tx = self.leaky_relu(x)\n\t\tx = self.conv3(x)\n\t\tx = self.leaky_relu(x)\n\t\tx = self.conv4(x)\n\t\tx = self.leaky_relu(x)\n\t\tx = self.classifier(x)\n\t\t#x = self.up_sample(x)\n\t\t#x = self.sigmoid(x) \n\n\t\treturn x","metadata":{"execution":{"iopub.status.busy":"2024-07-03T08:10:18.321666Z","iopub.execute_input":"2024-07-03T08:10:18.321919Z","iopub.status.idle":"2024-07-03T08:10:18.333767Z","shell.execute_reply.started":"2024-07-03T08:10:18.321898Z","shell.execute_reply":"2024-07-03T08:10:18.332944Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# TRAINING","metadata":{}},{"cell_type":"code","source":"class CrossEntropy2d(nn.Module):\n\n    def __init__(self, size_average=True, ignore_label=255):\n        super(CrossEntropy2d, self).__init__()\n        self.size_average = size_average\n        self.ignore_label = ignore_label\n\n    def forward(self, predict, target, weight=None):\n        \"\"\"\n            Args:\n                predict:(n, c, h, w)\n                target:(n, h, w)\n                weight (Tensor, optional): a manual rescaling weight given to each class.\n                                           If given, has to be a Tensor of size \"nclasses\"\n        \"\"\"\n        assert not target.requires_grad\n        assert predict.dim() == 4\n        assert target.dim() == 3\n        assert predict.size(0) == target.size(0), \"{0} vs {1} \".format(predict.size(0), target.size(0))\n        assert predict.size(2) == target.size(1), \"{0} vs {1} \".format(predict.size(2), target.size(1))\n        assert predict.size(3) == target.size(2), \"{0} vs {1} \".format(predict.size(3), target.size(3))\n        n, c, h, w = predict.size()\n        target_mask = (target >= 0) * (target != self.ignore_label)\n        target = target[target_mask]\n        if not target.data.dim():\n            return Variable(torch.zeros(1))\n        predict = predict.transpose(1, 2).transpose(2, 3).contiguous()\n        predict = predict[target_mask.view(n, h, w, 1).repeat(1, 1, 1, c)].view(-1, c)\n        loss = F.cross_entropy(predict, target, weight=weight, size_average=self.size_average)\n        return loss","metadata":{"execution":{"iopub.status.busy":"2024-07-03T08:10:18.334778Z","iopub.execute_input":"2024-07-03T08:10:18.335083Z","iopub.status.idle":"2024-07-03T08:10:18.347132Z","shell.execute_reply.started":"2024-07-03T08:10:18.335054Z","shell.execute_reply":"2024-07-03T08:10:18.346269Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(config, model, optimizer, train_loss, mIOU, epoch):\n    checkpoint_path = os.path.join(config[\"checkpoint_dir\"], \"checkpoint.pth\")\n    torch.save({\n        'epoch': epoch + 1,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': train_loss,\n        'mIOU': mIOU\n    }, checkpoint_path)\n    print(f\"Checkpoint saved in {checkpoint_path} | Epoch: {epoch}\")\n    \n    \ndef load_checkpoint(config, model, optimizer):\n    if os.path.exists(config[\"checkpoint_dir\"]):\n        checkpoint = torch.load(config[\"checkpoint_dir\"] + \"/checkpoint.pth\")\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        start_epoch = checkpoint['epoch']\n        print(f\"Checkpoint found. Resuming from epoch {start_epoch}.\")\n        return start_epoch\n    else:\n        os.mkdir(config[\"checkpoint_dir\"]) # divide the directory wrt the model (eg. checkpoints/DeepLabV2, checkpoints/BiSeNet)\n        print(\"No checkpoint found. Starting from scratch.\")\n        return 0","metadata":{"execution":{"iopub.status.busy":"2024-07-03T08:10:18.348118Z","iopub.execute_input":"2024-07-03T08:10:18.348363Z","iopub.status.idle":"2024-07-03T08:10:18.361128Z","shell.execute_reply.started":"2024-07-03T08:10:18.348342Z","shell.execute_reply":"2024-07-03T08:10:18.360339Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"input_size = [1280, 720]\ninput_size_target = [1024, 512]","metadata":{"execution":{"iopub.status.busy":"2024-07-03T08:10:18.362094Z","iopub.execute_input":"2024-07-03T08:10:18.362391Z","iopub.status.idle":"2024-07-03T08:10:18.372359Z","shell.execute_reply.started":"2024-07-03T08:10:18.362368Z","shell.execute_reply":"2024-07-03T08:10:18.371544Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def loss_calc(pred, label):\n    \"\"\"\n    This function returns cross entropy loss for semantic segmentation\n    \"\"\"\n    # out shape batch_size x channels x h x w -> batch_size x channels x h x w\n    # label shape h x w x 1 x batch_size  -> batch_size x 1 x h x w\n    label = label.long().cuda()\n    criterion = CrossEntropy2d().cuda()\n\n    return criterion(pred, label)\n\ndef lr_poly(base_lr, iter, max_iter, power):\n    return base_lr * ((1 - float(iter) / max_iter) ** (power))\n\ndef adjust_learning_rate(optimizer, iter, max_iter, config):\n    lr = lr_poly(config[\"learning_rate\"], iter, max_iter=1000, power=0.9)\n    optimizer.param_groups[0]['lr'] = lr\n    if len(optimizer.param_groups) > 1:\n        optimizer.param_groups[1]['lr'] = lr * 10","metadata":{"execution":{"iopub.status.busy":"2024-07-03T08:10:18.373406Z","iopub.execute_input":"2024-07-03T08:10:18.373664Z","iopub.status.idle":"2024-07-03T08:10:18.381208Z","shell.execute_reply.started":"2024-07-03T08:10:18.373643Z","shell.execute_reply":"2024-07-03T08:10:18.380510Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def id_processing(targets):\n    targets = targets.cuda()\n    \n    # Define valid indices\n    valid_indices = torch.tensor(list(range(19)) + [255]).to(targets.device)\n\n    # Replace all IDs not in valid_indices with 255\n    processed_targets = torch.where(torch.isin(targets, valid_indices), targets, torch.tensor(255, device=targets.device))\n\n    return processed_targets.long()","metadata":{"execution":{"iopub.status.busy":"2024-07-03T08:10:18.382182Z","iopub.execute_input":"2024-07-03T08:10:18.382457Z","iopub.status.idle":"2024-07-03T08:10:18.390461Z","shell.execute_reply.started":"2024-07-03T08:10:18.382434Z","shell.execute_reply":"2024-07-03T08:10:18.389703Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def poly_lr_scheduler(optimizer, init_lr, iter, max_iter, lr_decay_iter=1, power=0.9):\n    \"\"\"Polynomial decay of learning rate\n            :param init_lr is base learning rate\n            :param iter is a current iteration\n            :param lr_decay_iter how frequently decay occurs, default is 1\n            :param max_iter is number of maximum iterations\n            :param power is a polymomial power\n\n    \"\"\"\n    #if iter % lr_decay_iter or iter > max_iter:\n        #return optimizer\n\n    lr = init_lr*(1 - iter/max_iter)**power\n    optimizer.param_groups[0]['lr'] = lr\n    return lr\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T08:10:19.468941Z","iopub.execute_input":"2024-07-03T08:10:19.469758Z","iopub.status.idle":"2024-07-03T08:10:19.474892Z","shell.execute_reply.started":"2024-07-03T08:10:19.469727Z","shell.execute_reply":"2024-07-03T08:10:19.473913Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from itertools import cycle\nfrom tqdm import tqdm\n\ndef train(model, source_loader, target_loader, criterion, optimizer, config, start_epoch):\n    lambda_adv = 0.001\n    model_D = FCDiscriminator(num_classes=19)\n    model.cuda()\n    model_D.cuda() \n    \n    optimizer = optim.SGD(model.parameters(), lr=config[\"learning_rate\"], momentum=config[\"momentum\"], weight_decay=config[\"weight_decay\"])\n    optimizer_D = optim.Adam(model_D.parameters(), lr=config[\"learning_rate_D\"], betas=(0.9, 0.99))\n    \n    g_initial_lr = optimizer.param_groups[0]['lr']\n    d_initial_lr = optimizer_D.param_groups[0]['lr']\n\n    bce_loss = torch.nn.BCEWithLogitsLoss()\n    criterion = torch.nn.CrossEntropyLoss(ignore_index=255)\n    \n    interp = nn.Upsample(size=(input_size[1], input_size[0]), mode='bilinear')\n    interp_target = nn.Upsample(size=(input_size_target[1], input_size_target[0]), mode='bilinear')\n\n    source_label = 0\n    target_label = 1\n    \n    for epoch in range(config[\"epochs\"]):\n        print(\"Epoche {}/{}\".format(epoch + 1, config[\"epochs\"]))\n        model.train()\n        model_D.train() \n        \n        target_loader_cycle = cycle(target_loader)\n        train_loop = tqdm(zip(source_loader, target_loader_cycle), total=len(source_loader), leave=False)\n        train_loop.set_description(f'Epoch {epoch+1}/{config[\"epochs\"]} (Train)')\n        loss_seg_value = 0\n        loss_adv_target_value = 0\n        loss_d_value = 0\n        for (source_data, source_labels), (target_data, _) in train_loop:\n            source_data, source_labels = source_data.to(device), source_labels.to(device)\n            target_data = target_data.to(device)\n            \n            optimizer.zero_grad()\n            optimizer_D.zero_grad()\n\n            #TRAIN G\n            \n            #Train with source\n            for param in model_D.parameters():\n                param.requires_grad = False\n                \n            pred, _, _ = model(source_data)\n            pred = interp(pred)\n\n            loss_seg = criterion(pred, source_labels)\n            loss_seg.backward()\n            loss_seg_value = loss_seg.data.cpu().numpy()\n            train_loop.set_postfix(loss=loss_seg.item())\n    \n            #Train with target\n            pred_target, _, _ = model(target_data)\n            pred_target = interp_target(pred_target)\n            \n            d_out = model_D(F.softmax(pred_target))\n\n            loss_adv_target = bce_loss(d_out, torch.FloatTensor(d_out.data.size()).fill_(source_label).cuda())\n            loss_d = lambda_adv * loss_adv_target\n            loss_d.backward()\n            loss_adv_target_value = loss_adv_target.data.cpu().numpy()\n            train_loop.set_postfix(loss=loss_d.item())\n            #TRAIN D\n            \n            #Train with source\n            for param in model_D.parameters():\n                param.requires_grad = True\n                \n            pred = pred.detach()\n            d_out = model_D(F.softmax(pred))\n            \n            loss_d = bce_loss(d_out, torch.FloatTensor(d_out.data.size()).fill_(source_label).cuda())\n            loss_d.backward()\n            train_loop.set_postfix(loss=loss_d.item())\n\n            #Train with target\n            pred_target = pred_target.detach()\n            \n            d_out = model_D(F.softmax(pred_target))\n            \n            loss_d = bce_loss(d_out, torch.full_like(d_out, target_label))\n            loss_d.backward()\n            loss_d_value = loss_d.data.cpu().numpy()\n            train_loop.set_postfix(loss_seg=loss_seg.item(), loss_adv=loss_adv_target.item(), loss_d=loss_d.item())\n            \n            poly_lr_scheduler(optimizer, init_lr=g_initial_lr, iter=epoch, max_iter=config[\"epochs\"])\n            poly_lr_scheduler(optimizer_D, init_lr=d_initial_lr, iter=epoch, max_iter=config[\"epochs\"])\n            \n            optimizer.step()\n            optimizer_D.step()\n        \n        print(optimizer.param_groups[0]['lr'])\n        print('loss_seg = {0:.3f} loss_adv = {1:.3f}, loss_d = {2:.3f}'.format(\n        loss_seg_value, loss_adv_target_value, loss_d_value))\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T08:21:40.853115Z","iopub.execute_input":"2024-07-03T08:21:40.853861Z","iopub.status.idle":"2024-07-03T08:21:40.874947Z","shell.execute_reply.started":"2024-07-03T08:21:40.853823Z","shell.execute_reply":"2024-07-03T08:21:40.874056Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# TESTING\nSu Cityscapes, è regolare","metadata":{}},{"cell_type":"code","source":"def mean_iou(num_classes, pred, target):\n    mIOU = 0\n    for i in range(len(pred)):\n        hist = fast_hist(target[i].cpu().numpy(), pred[i].cpu().numpy(), num_classes)\n        IOU = per_class_iou(hist)\n        mIOU = mIOU + sum(IOU)/num_classes\n    return mIOU\n\ndef fast_hist(a, b, n):\n    \"\"\"\n    a and b are predict and mask respectively\n    n is the number of classes\n    \"\"\"\n    k = (a >= 0) & (a < n) #assign True if the value is in the range between 0 and 18 (class labels)\n    return np.bincount(n * a[k].astype(int) + b[k], minlength=n ** 2).reshape((n, n))\n\ndef per_class_iou(hist):\n    epsilon = 1e-5\n    return (np.diag(hist)) / (hist.sum(1) + hist.sum(0) - np.diag(hist) + epsilon)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T08:21:41.281381Z","iopub.execute_input":"2024-07-03T08:21:41.282031Z","iopub.status.idle":"2024-07-03T08:21:41.289632Z","shell.execute_reply.started":"2024-07-03T08:21:41.282003Z","shell.execute_reply":"2024-07-03T08:21:41.288744Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def val(model, val_loader):\n    \n    model.eval()\n    model.cuda()\n    \n    interp = nn.Upsample(size=(512, 1024), mode='bilinear')\n    total_mIOU = 0\n    total_images = 0\n\n    with torch.no_grad():\n        for _, (inputs, targets) in enumerate(val_loader):\n            image, label = inputs.cuda(), id_processing(targets).cuda()\n\n            output = model(image)\n            _, predicted = output.max(1)\n            running_mIOU = mean_iou(output.size()[1], predicted, targets)\n            total_mIOU += running_mIOU.sum().item()\n            total_images += len(predicted)\n        \n    mIOU = total_mIOU/total_images\n    \n    print(f'\\n\\nmIoU: {(mIOU*100):.3f}%')","metadata":{"execution":{"iopub.status.busy":"2024-07-03T08:21:41.441855Z","iopub.execute_input":"2024-07-03T08:21:41.442463Z","iopub.status.idle":"2024-07-03T08:21:41.449470Z","shell.execute_reply.started":"2024-07-03T08:21:41.442436Z","shell.execute_reply":"2024-07-03T08:21:41.448569Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"! rm -r checkpoints/\n# best configuration (TO CONFIGURE)\nconfig = dict(\n    epochs=20,\n    batch_size=4,\n    learning_rate=1e-4,\n    learning_rate_D=1e-2,\n    momentum=0.9,\n    weight_decay=1e-4,\n    architecture=\"BiSeNet\",\n    checkpoint_dir=\"/kaggle/working/checkpoints\" )\nif torch.cuda.is_available():\n    print(\"Building the model with the best configuration\")\n    # Build, train and analyze the model with the pipeline\n    model = model_pipeline(config)\nelse:\n    print(\"CUDA is Not available\")","metadata":{"execution":{"iopub.status.busy":"2024-07-03T08:21:41.747004Z","iopub.execute_input":"2024-07-03T08:21:41.747570Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Building the model with the best configuration\nNo checkpoint found. Starting from scratch.\nEpoche 1/20\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/20 (Train):   1%|          | 6/625 [00:17<21:53,  2.12s/it, loss=0]                                        ","output_type":"stream"}]},{"cell_type":"code","source":"! rm -r checkpoints/","metadata":{"execution":{"iopub.status.busy":"2024-06-27T15:39:26.353581Z","iopub.status.idle":"2024-06-27T15:39:26.353898Z","shell.execute_reply.started":"2024-06-27T15:39:26.353742Z","shell.execute_reply":"2024-06-27T15:39:26.353755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best configuration (TO CONFIGURE)\nconfig = dict(\n    epochs=5,\n    batch_size=6,\n    learning_rate=2.5e-4,\n    learning_rate_D=1e-4,\n    momentum=0.9,\n    weight_decay=1e-4,\n    architecture=\"BiSeNet\",\n    checkpoint_dir=\"/kaggle/working/checkpoints\" )\nif torch.cuda.is_available():\n    print(\"Building the model with the best configuration\")\n    # Build, train and analyze the model with the pipeline\n    model = model_pipeline(config)\nelse:\n    print(\"CUDA is Not available\")","metadata":{"execution":{"iopub.status.busy":"2024-06-27T13:27:13.902020Z","iopub.execute_input":"2024-06-27T13:27:13.902359Z","iopub.status.idle":"2024-06-27T15:17:18.559272Z","shell.execute_reply.started":"2024-06-27T13:27:13.902329Z","shell.execute_reply":"2024-06-27T15:17:18.553612Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Building the model with the best configuration\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 144MB/s] \nDownloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n100%|██████████| 171M/171M [00:01<00:00, 156MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"No checkpoint found. Starting from scratch.\nEpoche 1/5\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"0.00025\nloss_seg = 0.748 loss_adv = 2.362, loss_d = 0.322\nEpoche 2/5\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"0.00020451303651271462\nloss_seg = 0.527 loss_adv = 2.305, loss_d = 0.418\nEpoche 3/5\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"0.00015786146687233883\nloss_seg = 0.559 loss_adv = 2.373, loss_d = 0.394\nEpoche 4/5\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"0.00010959582263852174\nloss_seg = 0.638 loss_adv = 2.560, loss_d = 0.277\nEpoche 5/5\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"5.873094715440094e-05\nloss_seg = 0.448 loss_adv = 2.644, loss_d = 0.369\n\n\nmIoU: 14.595%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}